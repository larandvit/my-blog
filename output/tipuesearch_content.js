var tipuesearch = {"pages":[{"title":"About blog","text":"Starting this blog, I want to accomplish some goals. The most important goal is to share my knowledge and experience with others. It's my contribution back to the community of technology gurus who helped me be where I am now. I haven't done anything special. I've just learnt information from different sources, validated, refined, added my research, and published back. I appreciate everybody who makes information open and available to public. We are growing together helping each other. The second goal is to create my personal repository as a quick way to refresh my memory in case of forgetting something. In many cases, we are doing a lot of staff but we can't keep it in our heads. When we need it again, we try to reinvent the wheel because we can't recall what it was done before. The last goal is to express myself. This is when I can be creative without any limits. It's my way to go and I'm happy to be successful. If you want to contact me, please drop me a line here . You can find me on git as well. Best wishes, Vitaly Saversky","tags":"pages","url":"https://techjogging.com/pages/about-blog.html","loc":"https://techjogging.com/pages/about-blog.html"},{"title":"Access MinIO S3 Storage in PrestoDB Cluster","text":"PrestoDB is aimed to access a variety of data sources by means of connectors. Hive connector is used to access files stored in Hadoop Distributed File System (HDFS) or S3 compatible storages. Metadata can be accessible via Hive metastore. Another option to access metadata is PrestoDB. It simplifies the PrestoDB infrastructure eliminating Hive metastore. Also, in case of Hive metastore, internal tables are stored in HDFS and as a result, it requests installation of Hadoop. PrestoDB can handle both metadata and internal tables. S3 compatible storages are very good alternatives to store big data. They are lightweight, easy to set up, and support. Many of those storages are open source. When building an enterprise level system, it is important to set up and tune up PrestoDB to work with a coordinator and one or more workers. The setup is different from single node one. MinIO S3 compatible storage along with hiveless metadata configuration is used in the sample below. Internal tables are stored in a shared folder. Hive connector property file is created in /etc/presto/catalog folder or it can be deployed by presto-admin tool or other tools. The name might be minio.properties . It has to have .properties extension name. A set of mandatory parameters are. connector . name = hive - hadoop2 hive . s3 . path - style - access = true hive . metastore = file hive . metastore . catalog . dir = file : /// mnt / presto / data / minio hive . s3 . endpoint = http : // minio . sample . com : 9000 hive . s3 . aws - access - key = YourAccessKey hive . s3 . aws - secret - key = YourSercetKey hive . temporary - staging - directory - path = file : /// mnt / presto / data / tmp hive . s3 . socket - timeout = 1 m hive.metastore.catalog.dir - the folder is shared between all nodes: a coordinator and workers. Internal tables are stored in the folder. hive.temporary-staging-directory-path - the folder is shared between all nodes: a coordinator and workers. The location of temporary staging folder that is used for write operations. Each user has a separate sub folder with the name pattern: presto-UserName . If the parameter is missing, INSERT INTO or CREATE TABLE AS statements will write only a portion of data into destination tables and sporadically, the error message will come up. Error moving data files from file:/tmp/presto-root/6b5efc64-177e-409f-b34c-aeddbc942a92/20200612_155605_00395_stnes_45252f19-7244-46ec-86f0-88da4c300c3d to final location file:/mnt/presto/data/minio/schema_name/table_name/20200612_155605_00395_stnes_45252f19-7244-46ec-86f0-88da4c300c3d hive.s3.socket-timeout - default value is 5 seconds and if MinIO is busy, you get the error. Unable to execute HTTP request: Read time out. The sample to test access to MinIO data. CREATE SCHEMA minio . sample_schema ; CREATE TABLE sample_table ( combined_columns VARCHAR ) WITH ( external_location = 's3a://your_minio_bucket_name/' , format = 'TEXTFILE' , skip_header_line_count = 1 );","tags":"PrestoDB","url":"https://techjogging.com/access-minio-s3-storage-prestodb-cluster.html","loc":"https://techjogging.com/access-minio-s3-storage-prestodb-cluster.html"},{"title":"Create Ticket Cache File for Kerberos Authentication in Linux","text":"Kerberos credentials can be stored in Kerberos ticket cache. They are valid for relatively short period of time. The period can be a session or a specified timeframe. A Kerberos ticket cache contains a service and a client principal names, lifetime indicators, flags, and the credential itself. Kerberos 5 client is aimed to generate a ticket cache file. The article is based on CentOS / RHEL distribution. 1. Validate that Kerberos 5 client is installed Kerberos 5 client is installed as default. There are two components. yum list installed | grep 'krb5-workstation\\|krb5-libs' Output krb5-libs.x86_64 1.15.1-46.el7 @base krb5-workstation.x86_64 1.15.1-46.el7 @base Kerberos 5 client installation sudo yum install krb5-workstation krb5-libs 2. Create a folder to store ticket cache file mkdir ~/kerberos 3. Add KRB5CCNAME variable The variable defines the location of a Kerberos ticket cache file. Open .bashrc file. nano ~/.bashrc Add the variable export command. export KRB5CCNAME = /home/username/kerberos/krb5cc_username Reboot your computer to make it effective. Validate KRB5CCNAME variable. export | grep KRB5CCNAME 4. Create ticket cache file kinit -c /home/username/kerberos/krb5cc_username username@SAMPLE.COM -l 10h -c means the location of the ticket cache -l states lifetime of the ticket cache 4. Validate ticket cache file klist -c /home/username/kerberos/krb5cc_username 5. Configuration file krb5.conf is a configuration file to tune up Kerberos ticket cache creation. The default location is /etc but KRB5_CONFIG environmental variable can overwrite the location of the configuration file. Our interest is mainly 2 sections: [libdefaults] and [realms] . [libdefaults] default_realm = SAMPLE.COM ticket_lifetime = 24h renew_lifetime = 7d forwardable = true [realms] SAMPLE.COM = { kdc = server1.sample.com }","tags":"Kerberos","url":"https://techjogging.com/create-ticket-cache-kerberos-authentication-linux.html","loc":"https://techjogging.com/create-ticket-cache-kerberos-authentication-linux.html"},{"title":"Add Google Analytics Pageviews in Static Web Site","text":"Google Analytics requests authentication to access Google Analytics data/reports. Implementation of Google Analytics can be done in both places client and server. Server APIs support a wide variety of languages and the authentication process is transparent for users. Javascript is a language to access Google Analytics APIs in client implementation. Also, to view Google Analytics in javascript, users have to be authenticated. The process of authentication includes a Google form to enter user credentials. It works perfectly for a set of scenarios but it doesn't work if we want to see pageviews in static web sites. To solve an issue with authentication, we can use a Google Analytics token. A token allows bypass entering of credentials. It will add an extra step to generate a token and place it in a location accessible by your static web site. Google Analytics token has limited life; it is only 1 hour, so we need to renew our token every hour. It seems complicated but in fact not. It can be used a tool to generate tokens and push it in a static web site. A sample is located in github repository. Static web site implementation is Pelican blog . 1. Create Google Analytics service account and extract json key file Go to Google Analytics Platform APIs and Services Credentials to create a Service account Create Key 2. Develop Generate Google Analytics token tool The tool will use oauth2client library to run from_json_keyfile_name function from ServiceAccountCredentials service object. After getting a token, the token file will be updated on the static web site. from oauth2client.service_account import ServiceAccountCredentials # The scope for the OAuth2 request. SCOPE = 'https://www.googleapis.com/auth/analytics.readonly' # Defines a method to get an access token from the ServiceAccount object. def access_token ( key_file_path ): return ServiceAccountCredentials . from_json_keyfile_name ( key_file_path , SCOPE ) . get_access_token () if __name__ == \"__main__\" : key_file_path = 'tech-jogging-blog-98stj21aac52.json' token_file_path = 'report_access.js' with open ( token_file_path , \"w\" ) as f_out : token = access_token ( key_file_path ) . access_token token_code = 'var ANALYTICS_TOKEN = \\' {} \\' ;' . format ( token ) f_out . write ( token_code ) Generated token javascript file looks like. It contains a variable with token value. var ANALYTICS_TOKEN = 'ya29.c.Ko4BzQdIsUMOFvVHh5O90tnJZvW3but0Ym-C9C1NhKEt3ihBKEk6AOQFp-Mm-MUzZiyLJsfNSD90vqeUm078fSeXl0NXUhWZKvY79BJhg33UB_crRmwDY3Xn98KaPTgi22y4_QFdRA0l3GiQeISkQcnEmb0P1Y_eCquWR-qtDWVy-IBDZRJph2j6otc64oxoqQ' ; 3. Develop javascript code to access Google Analytics The code runs when Google Analytics APIs is loaded, then it is used the generated token to authorize access to Google Analytics, after a pageviews query is sent, and finally the received pageview value is assigned to an HTML control. api . analytics . ready ( function () { /** * Authorize the user with an access token. */ gapi . analytics . auth . authorize ({ 'serverAuth' : { 'access_token' : ANALYTICS_TOKEN } }); var pagePathFilter = 'ga:pagePath==' + window . location . pathname ; var report = new gapi . analytics . report . Data ({ query : { ids : 'ga:209816969' , 'start-date' : 'today' , 'end-date' : 'today' , metrics : 'ga:pageviews' , filters : pagePathFilter } }); report . on ( 'success' , function ( response ) { document . getElementById ( 'query-output' ). value = response . totalsForAllResults [ 'ga:pageviews' ]; }); report . execute (); }); 4. Create a HTML file to show pageviews <!DOCTYPE html> < html > < head > < meta charset = \"utf-8\" > < title > Google Analytics Pageviews Sample </ title > < script > ( function ( w , d , s , g , js , fs ){ g = w . gapi || ( w . gapi = {}); g . analytics = { q : [], ready : function ( f ){ this . q . push ( f );}}; js = d . createElement ( s ); fs = d . getElementsByTagName ( s )[ 0 ]; js . src = 'https://apis.google.com/js/platform.js' ; fs . parentNode . insertBefore ( js , fs ); js . onload = function (){ g . load ( 'analytics' );}; }( window , document , 'script' )); </ script > </ head > < body > < h1 > Google Analytics </ h1 > < h3 > Pageviews Counter </ h3 > < textarea cols = \"10\" rows = \"1\" id = \"query-output\" ></ textarea > < script src = 'report_access.js' ></ script > < script src = 'pageviews.js' ></ script > </ body > </ html > 5. Schedule Generate Google Analytics token tool to be run every hour It can be done in many different ways, for example, Linux - cron, Windows - Task Scheduler, Synology DSM - Task Scheduler.","tags":"Google Analytics","url":"https://techjogging.com/add-google-analytics-pageviews-static-web-site.html","loc":"https://techjogging.com/add-google-analytics-pageviews-static-web-site.html"},{"title":"Create Keytab for Kerberos Authentication in Linux","text":"Keytab stands for key table. It is a file which stores one or more Kerberos principals with corresponding encrypted keys. Encrypted keys are generated based on user passwords. It allows to secure storing of passwords and authenticate users without entering of passwords. The current version of the Kerberos protocol is 5. The article is sampled in CentOS / RHEL distribution. 1. Validate that Kerberos 5 client is installed Kerberos 5 client is installed as default. There are two components. yum list installed | grep 'krb5-workstation\\|krb5-libs' Output krb5-libs.x86_64 1.15.1-46.el7 @base krb5-workstation.x86_64 1.15.1-46.el7 @base Kerberos 5 client installation sudo yum install krb5-workstation krb5-libs 2. Create a folder to store keytab file mkdir ~/kerberos 3. Create keytab file The tool to generate keytab file is interactive one and you need to type in the commands. ktutil ktutil: addent -password -p username@SAMPLE.COM -k 1 -e RC4-HMAC Password for username@SAMPLE.COM: ktutil: wkt /home/username/kerberos/username.keytab ktutil: l slot KVNO Principal ---- ---- --------------------------------------------------------------------- 1 1 username@SAMPLE.COM ktutil: exit 4. Validate keytab file klist -e -k -t ~/kerberos/username.keytab","tags":"Kerberos","url":"https://techjogging.com/create-keytab-file-kerberos-authentication-linux.html","loc":"https://techjogging.com/create-keytab-file-kerberos-authentication-linux.html"},{"title":"Configure Talend Job to Connect to SQL Server with Windows Integrated Authentication","text":"The main way to access MS SQL Server in Talend is SQL Server authentication when user name and password must be supplied. SQL Server credentials are compromised as password is in plain text. In some cases, this is only a way to connect to SQL Server. There is another option to use Windows Integrated authentication. It's safe way because Talend jobs don't contain either user name or password. Credentials are provided by Windows Operation System or another service in background. Windows Integrated authentication is common in Windows environment as it's used Active Directory. 1. Download jTDS SQL Server and Sybase JDBC driver 2. Extract ntlmauth.dll library from the driver package. As Talend is 64 bit now, it has to be 64 bit library as well. jtds-1.3.1-dist │ └───x64 └───SSO ntlmauth.dll 3. Place the extracted library to Talend root folder 4. Set Up tMSSqlInput Talend Component There are 2 options. The first one is to set up tMSSqlInput component directly and another one uses tMSSqlInput component in conjunction with database connection. Option #1. tMSSqlInput component settings. Leave Username and password empty with double quotes. The advanced settings has to include IntegratedSecurity=true in double quotes. Option #2 Create a new database connection. Leave Username and password blank. Add IntegratedSecurity=true to Additional parameters. MSSqlInput component settings with the database connection. 5. Testing Create 2 jobs. Each job contains 2 components: (1) tMSSqlInput and (2) tJavaRow. Change settings of tMSSqlInput component as per Set Up tMSSqlInput Talend Component accordingly. Change settings of tJavaRow component as below.","tags":"Talend","url":"https://techjogging.com/configure-talend-job-connect-sqlserver-windows-integrated-authentication.html","loc":"https://techjogging.com/configure-talend-job-connect-sqlserver-windows-integrated-authentication.html"},{"title":"Installing and Using PIP on Synology DSM","text":"Python3 can be easily installed on Synology DSM through Synology Installation Center but pip installation is skipped. One of the methods to install pip is to bootstrap the pip installer into an existing Python installation. The ensurepip package is aimed for it. It's available starting from Python version 3.4. As all pip components are a part of Python package, the internet connection is not required to install pip. The article is based on Synology DSM 6.2.2-24922 Update 4 and Python version is 3.5.1. Python 3.5.1 bin folder is /volume1/@appstore/py3k/usr/local/bin . Python 3.5.1 lib is located in /volume1/@appstore/py3k/usr/local/lib/python3.5 folder. 1. Validate Python3 installation and version. python3 -V 2. Install pip Run pip installation with admin privilege. A running Synology user has to belong to the administrator group. There are 2 options to proceed: (1) run commands as sudo or (2) switch to root with sudo -i . sudo python3 -m ensurepip 3. Upgrade pip to the latest version. sudo python3 -m pip install --upgrade pip 4. Validate pip installation and version. python3 -m pip -V 5. Install a package, for example, requests. sudo python3 -m pip install requests Resources Bootstrapping the pip installer","tags":"Synology DSM","url":"https://techjogging.com/installing-and-using-pip-on-synology-dsm.html","loc":"https://techjogging.com/installing-and-using-pip-on-synology-dsm.html"},{"title":"Create Ticket Cache File for Kerberos Authentication in Windows","text":"Kerberos ticket cache is one of the options to utilize Kerberos authentication in Windows. Another option is to use Kerberos keytab file . Kerberos ticket cache can be transparently consumed by many tools, whereas Kerberos keytab requests additional setup to plug in to tools. Kerberos ticket cache file default location and name are C:\\Users\\windowsuser\\krb5cc_windowsuser and mostly tools recognizes it. There are some tools and techniques to generate a ticket cache file. 1. Kinit Java tool Make sure that Java JRE or SDK or open source equivalent, for example, OpenJDK is installed. Run kinit tool located in C:\\Program Files\\Java\\jre[version]\\bin folder. The folder name depends on JRE or SDK or 32 or 64 bit edition. It's assumed java 8 is installed in C:\\Program Files\\Java\\jre1.8.0_192 folder. If Kerberos ticket cache is created for a user currently logged in to a Windows computer \"C:\\Program Files\\Java\\jre1.8.0_192\\kinit\" Output Password for windowsuser@SAMPLE.COM: New ticket is stored in cache file C:\\Users\\windowsuser\\krb5cc_windowsuser If Kerberos ticket cache is created for a different user from currently logged in to a Windows computer \"C:\\Program Files\\Java\\jre1.8.0_192\\kinit\" windowsuser@SAMPLE.COM Output Password for windowsuser@SAMPLE.COM: New ticket is stored in cache file C:\\Users\\windowsuser\\krb5cc_windowsuser Utilize Kerberos keytab file with kerberized Windows service account provided by your administrator. \"C:\\Program Files\\Java\\jre1.8.0_192\\kinit\" servicewindowsaccount@SAMPLE.COM -k -t C: \\k eytabfolder \\k eytabname.keytab Output New ticket is stored in cache file C:\\Users\\windowsuser\\krb5cc_windowsuser The created cache file can be validated with klist command \"C:\\Program Files\\Java\\jre1.8.0_192\\klist\" Output Credentials cache: C:\\Users\\windowsuser\\krb5cc_windowsuser Default principal: windowsuser@SAMPLE.COM, 1 entry found. [1] Service Principal: krbtgt/SAMPLE.COM@SAMPLE.COM Valid starting: Mar 26, 2020 21:35:00 Expires: Mar 27, 2020 07:35:00 2.MIT Kerberos software MIT Kerberos can be loaded from MIT Kerberos Distribution Page . It includes command line and GUI tools. Because of coming from Unix environment, it doesn't understand the default location and the location should be explicitly stated. If Kerberos ticket cache is created for a user currently logged in to a Windows computer \"C:\\Program Files\\MIT\\Kerberos\\bin\\kinit\" -c C: \\U sers \\w indowsuser \\k rb5cc_windowsuser No output. If Kerberos ticket cache is created for a different user from currently logged in to a Windows computer \"C:\\Program Files\\MIT\\Kerberos\\bin\\kinit\" -c C: \\U sers \\w indowsuser \\k rb5cc_windowsuser windowsuser@SAMPLE.COM No output. Utilize Kerberos keytab file with kerberized Windows service account provided by your administrator. \"C:\\Program Files\\MIT\\Kerberos\\bin\\kinit\" -k -t C: \\k eytabfolder \\k eytabname.keytab -c C: \\U sers \\w indowsuser \\k rb5cc_windowsuser servicewindowsaccount@SAMPLE.COM No output. The created cache file can be validated with klist command \"C:\\Program Files\\MIT\\Kerberos\\bin\\klist\" -c C: \\U sers \\w indowsuser \\k rb5cc_windowsuser Output Ticket cache: FILE:C:\\Users\\windowsuser\\krb5cc_windowsuser Default principal: windowsuser@SAMPLE.COM Valid starting Expires Service principal 05/09/20 22:39:22 05/10/20 08:39:22 krbtgt/krbtgt/SAMPLE.COM@SAMPLE.COM renew until 05/10/20 22:39:22 It can be applied some options to customize ticket cache, for example, -r renewable_life . MIT Kerberos Ticket Manager is GUI tool. It can be run from Windows Start menu or from desktop or C:\\Program Files\\MIT\\Kerberos\\bin\\MIT Kerberos.exe . Set up 'KRB5CCNAME' environment variable Open System Properties entering sysdm.cpl in Windows Start Go to Advanced tab and click Environment Variables... Add a new System Variable . Name: KRB5CCNAME and value: C:\\Users\\windowsuser\\krb5cc_windowsuser . Reboot computer to make it in effect. Run MIT Kerberos Ticket Manager Click Get Ticket and enter Principal and Password . Also, you can customize ticket properties. Validate ticket location in Credential Cache column or C:\\Users\\windowsuser\\krb5cc_windowsuser file.","tags":"Kerberos","url":"https://techjogging.com/create-ticket-cache-file-for-kerberos-authentication-in-windows.html","loc":"https://techjogging.com/create-ticket-cache-file-for-kerberos-authentication-in-windows.html"},{"title":"Multi-Character Field Delimiter in Apache Hive Table","text":"Multi-character field delimiter is not implemented in LazySimpleSerDe and OpenCSVSerde text file SerDe classes. There are two options to use multi-character field delimiter in Hive. The first option is MultiDelimitSerDe class specially developed to handle multi-character field delimiters. The second one is to use RegexSerDe class as a workaround. Those two options don't work as expected because of limitations. MultiDelimitSerDe implementation MultiDelimitSerDe SerDe is considered as experimental one until Hive release 4.0.0. It's included in hive-contrib-<version>.jar library and you have to add the library to the class path. If hive-contrib-<version>.jar library is not included in the class path, the functionality is limited to run only SELECT * FROM table_name; queries. The limitation is caused by map/reduce jobs which don't have access to the library. The issue should be fixed in Hive 4.0.0 when MultiDelimitSerDe class is supposed to be included in org.apache.hadoop.hive.serde2 library. Currently, MultiDelimitSerDe class is a part of org.apache.hadoop.hive.contrib.serde2 library. One more limitation is that skip header lines functionality ( TBLPROPERTIES (\"skip.header.line.count\"=\"1\") ) doesn't work. Sample of experimental version with ~| field delimiter. The code is run on Hive 1.1.0. CREATE EXTERNAL TABLE ` sample_table ` ( ` column1 ` string , ` column2 ` int , ` column3 ` decimal ( 10 , 2 ), ` column4 ` timestamp ) ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.MultiDelimitSerDe' WITH SERDEPROPERTIES ( \"field.delim\" = \"~|\" ); LOCATION '/folder/folder2' Sample of final version with ~| field delimiter. The code can't be validated. CREATE EXTERNAL TABLE ` sample_table ` ( ` column1 ` string , ` column2 ` int , ` column3 ` decimal ( 10 , 2 ), ` column4 ` timestamp ) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.MultiDelimitSerDe' WITH SERDEPROPERTIES ( \"field.delim\" = \"~|\" ); LOCATION '/folder/folder2' RegexSerDe implementation RegexSerDe SerDe limitation is to support only string data type in Hive tables. Also, It's expected performance overhead. Sample with ~| field delimiter. CREATE EXTERNAL TABLE ` sample_table ` ( ` column1 ` string , ` column2 ` string , ` column3 ` string , ` column4 ` string ) ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.RegexSerDe' WITH SERDEPROPERTIES ( \"input.regex\" = \"(.*)[~][|](.*)[~][|](.*)[~][|](.*)\" ) LOCATION '/folder/folder2' TBLPROPERTIES ( \"skip.header.line.count\" = \"1\" ); Resources MultiDelimitSerDe Use multiple-characters as field delimiter Include MultiDelimitSerDe in HIveServer2 By Default Hive contrib jar should not be in lib","tags":"Hive","url":"https://techjogging.com/multi-character-field-delimiter-in-apache-hive-table.html","loc":"https://techjogging.com/multi-character-field-delimiter-in-apache-hive-table.html"},{"title":"Field Delimiter in Apache Hive Table","text":"Not too much official documentation can be found on how to define a field delimiter in a create or an alter Apache Hive statement. This setting is requested for delimited text files placed as source of Hive tables. When a field delimiter is not assigned properly, Hive can't split data into columns, and as a result, the first column will contain all data and the rest of columns will have NULL values. Also, it's critical to know a default field delimiter if field delimiter setting is missed in a create statement. There are 2 major SerDe (Serializer/Deserializer) classes for text data. SerDe defines input/output (IO) interface which handles: (1) read data from a Hive table and (2) write it back out to HDFS. org.apache.hadoop.hive.serde2 is the Hive SerDe library including TEXTFILE formats. org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe . The default field delimiter value is '\\001' . org.apache.hadoop.hive.serde2.OpenCSVSerde The default field delimiter value is ',' . LazySimpleSerDe is more efficient in terms of performance. OpenCSVSerde has a limitation to handle only string data type in Hive tables. The default format is LazySimpleSerDe . The main issue with field delimiter is that Java char data type is used as an argument to assign a field delimiter. It can hold only 2 bytes. Java char data type can understand both ASCII and Unicode characters but it can handle Unicode characters which belong to ASCII table. Characters of the first part of ASCII table with codes from 0 to 127 are only accepted as field delimiters. If you need to use the extended ASCII character from 128 to 255 codes, it should be used other SerDe classes, for example, org.apache.hadoop.hive.contrib.serde2.RegexSerDe . The rules to assign a filed delimiter are. Any visible ASCII character can be assigned directly, for example, '1' , 'a' , or '!' . It can be used special predefined characters, for example. '\\t' , '\\r' , and '\\n' . If a character belongs to ASCII set and invisible, it can be used octal or Unicode notations. Octal starts from back slash and contains 3 digits, for example, '\\001' . Character 'a' is '\\040' . Hex has '\\u' prefix and includes 4 digits. It represents a Unicode code but you have to use decimal ASCII code, for example, '\\u0010' definition is converted to '\\000a' Hive table field delimiter. Another sample is visible ASCII character 'a' , '\\u0032' field delimiter definition is converted to '\\0020' in Hive table. Those commands can be used to retrieve field delimiter for a table from Hive meta data. Show statement. SHOW CREATE TABLE sample_table_name ; Describe statement #1. DESCRIBE FORMATTED sample_table_name ; Describe statement #2. DESCRIBE EXTENDED sample_table_name ; Field delimiter can be assigned or changed in those Hive statements. CREATE statement with LazySimpleSerDe interface. CREATE TABLE sample ( column1 string , column2 string , column3 string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\n' STORED AS TEXTFILE LOCATION '/folder1/folder2' CREATE statement with OpenCSVSerde interface. CREATE TABLE sample ( column1 string , column2 string , column3 string ) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' WITH SERDEPROPERTIES ( 'separatorChar' = ',' ) STORED AS TEXTFILE LOCATION '/folder1/folder2' ALTER statement with LazySimpleSerDe interface. ALTER TABLE multi_char_field_delim SET SERDEPROPERTIES ( 'field.delim' = ',' ) ALTER statement with OpenCSVSerde interface. ALTER TABLE multi_char_field_delim SET SERDEPROPERTIES ( 'separatorChar' = ',' ) or ALTER TABLE multi_char_field_delim SET SERDEPROPERTIES ( 'field.delim' = ',' )","tags":"Hive","url":"https://techjogging.com/field-delimiter-in-apache-hive-table.html","loc":"https://techjogging.com/field-delimiter-in-apache-hive-table.html"},{"title":"Create OAuth Credentials for Google Analytics APIs","text":"One of the ways to implement Google Analytics in your tools or Web sites is to utilize Google Analytics APIs. This way is very flexible as Google Analytics APIs can be used with wide range of the programming languages. Moreover, APIs are mature product which on the market for many years. The latest v4 contains rich set of functionalities which was forged from earliest versions. It's back compatible with previous v3. The first step to start using Google Analytics APIs includes creating of an OAuth credentials. There are many options to proceed with it but we follow a route to generate credentials for a Web site with javascript implementation. It's applicable to other scenarios as well. The article is based on API v4 and includes a sample of code to test your setup. The sample is extracted from Google JavaScript quickstart for web applications . The original code was modified to show error messages in case of encountering any issues and logout functionality. 1. Create a Google account All Google tools request a Google account. If you already have a Google email, it can be used as your Google account otherwise follow link . 2. Open Google Developer Console Google Developer Console is used to create an OAuth credentials. To open the tool, sign in with your Google account, and then agree to Terms of Service. 3. Create a new API project Type in your project name. If you create a new project for your personal usage, leave Location with No organization default value. Your project name is the current one. 4. Enable Google Analytics APIs Click ENABLE APIS AND SERVICES button. Search for Google Analytics Reporting APIs from the list of available APIs. Confirm your intention. 5. Configure Consent Select Credentials menu item from the dashboard. Initiate configuration of consent clicking CONFIGURE CONSENT SCREEN . If you left Location with No organization value, you have only External User Type option enabled. Enter information about your Web site. Final screen of the configuration. 6. Create an OAuth credentials Go back to Credentials screen and click CREATE CREDENTIALS button, and then select OAuth client ID. Select Web application type, type in your application name, and enter your website. If you test it, you can enter http://localhost.com. Confirmation screen. Your OAuth credentials created. OAuth credentials information can be retrieved if you click on the name of OAuth 2.0 Client ID entry. 7. Grant Account Explorer access to your Google Account Open Account Explorer . Allow Account Explorer access your Google Account. Select your Account, Property, and View. View field contain VIEW_ID requested by Google Analytics API. 8. API v4 Sample The sample shows a number of sessions for the last 8 days. Create an HTML file on your Web server replacing (1) client_id with yours in <meta name=\"google-signin-client_id\" content=\"109743573222-tu7960r1m6kam5acmigfumlqebf016cf.apps.googleusercontent.com\"> line and (2) VIEW_ID in var VIEW_ID = '209816969'; line, and then open it in browser. It will ask for your Google account credentials associated with your Google Analytics. <!DOCTYPE html> < html > < head > < meta charset = \"utf-8\" > < title > Analytics Reporting API V4 Sample </ title > < meta name = \"google-signin-client_id\" content = \"109743573222-tu7960r1m6kam5acmigfumlqebf016cf.apps.googleusercontent.com\" > < meta name = \"google-signin-scope\" content = \"https://www.googleapis.com/auth/analytics.readonly\" > </ head > < body > < h1 > Analytics Reporting API V4 Sample </ h1 > <!-- The Sign-in button. This will run `queryReports()` on success. --> < p class = \"g-signin2\" data-onsuccess = \"queryReports\" ></ p > <!-- The Sign-out button. --> < button onclick = \"signOut()\" > Logout </ button > <!-- The API response will be printed here. --> < h3 > API response </ h3 > < textarea cols = \"80\" rows = \"10\" id = \"query-output\" ></ textarea > <!-- The errors will be printed here. --> < h3 > Error </ h3 > < textarea cols = \"80\" rows = \"5\" id = \"query-error\" ></ textarea > < script > // Replace with your view ID. var VIEW_ID = '209816969' ; // Query the API and print the results to the page. function queryReports () { try { document . getElementById ( 'query-error' ). value = '' ; gapi . client . request ({ path : '/v4/reports:batchGet' , root : 'https://analyticsreporting.googleapis.com/' , method : 'POST' , body : { reportRequests : [ { viewId : VIEW_ID , dateRanges : [ { startDate : '7daysAgo' , endDate : 'today-1' } ], metrics : [ { expression : 'ga:sessions' } ] } ] } }). then ( displayResults , displayErrors ); } catch ( err ) { document . getElementById ( 'query-error' ). value = err ; } } function displayResults ( response ) { var formattedJson = JSON . stringify ( response . result , null , 2 ); document . getElementById ( 'query-output' ). value = formattedJson ; } function displayErrors ( reason ) { document . getElementById ( 'query-error' ). value = reason . result . error . message ; } function signOut () { gapi . auth2 . getAuthInstance (). disconnect () document . getElementById ( 'query-output' ). value = '' ; document . getElementById ( 'query-error' ). value = '' ; } </ script > <!-- Load the JavaScript API client and Sign-in library. --> < script src = \"https://apis.google.com/js/client:platform.js\" ></ script > </ body > </ html > Response is { \"reports\" : [ { \"columnHeader\" : { \"metricHeader\" : { \"metricHeaderEntries\" : [ { \"name\" : \"ga:sessions\" , \"type\" : \"INTEGER\" } ] } }, \"data\" : { \"rows\" : [ { \"metrics\" : [ { \"values\" : [ \"43\" ] } ] } ], \"totals\" : [ { \"values\" : [ \"43\" ] } ], \"rowCount\" : 1 , \"minimums\" : [ { \"values\" : [ \"43\" ] } ], \"maximums\" : [ { \"values\" : [ \"43\" ] } ] } } ] }","tags":"Google Analytics","url":"https://techjogging.com/create-oauth-credentials-for-google-analytics-apis.html","loc":"https://techjogging.com/create-oauth-credentials-for-google-analytics-apis.html"},{"title":"Fast Record Count in Table or Tables in Database in SQL Server","text":"As data size is growing each year, count record in a table takes more and more time if you use old fashion methods, for example, SELECT COUNT(*) FROM table . It's the most reliable method to get record count but if you deal with multi-billion record table, it might take hours to get your result. Also, you need to consider that COUNT(*) operator is resource consuming as MS SQL Server has to scan every record in worst scenario. Moreover, COUNT(*) calculation locks a table which impacts on overall performance. Meta data is a good source of information without workloading our SQL Server and getting results in no time. One drawback is that we might get \"dirty\" numbers because meta data is not aware of transactions which might occur during retrieving of record count. If we look at \"dirty\" numbers from another point of view, we can accept it. Let's imagine that we need to check our progress on inserting record in a table, we don't need exact number, we need to get a number to evaluate our current progress. Another case is when we have completed data transformation on a big table, we need to validate our result with record count. We know exactly that we don't do any data modification on the table and we are safe to use meta data. To get record count fast, we need to consider a case when meta data might be locked and getting record count quickly will be problematic. It can be solved by reading uncommitted (\"dirty\") data with NOLOCK or READUNCOMMITTED hint. An exception is when TABLOCK hint is used on your table. It will lock access to the table completely. 1. Count Records in a Table SELECT SCHEMA_NAME ( schema_id ) + '.' + t . name AS [ Table Name ], FORMAT ( SUM ( p .[ rows ]), '#,#' ) AS [ Row Count ] FROM sys . tables t WITH ( NOLOCK ) JOIN sys . partitions p WITH ( NOLOCK ) ON t .[ object_id ] = p .[ object_id ] AND p . index_id IN ( 0 , 1 ) WHERE t . name = '$(Table name)' GROUP BY t .[ schema_id ] , t .[ name ]; Replace $(Table name) place holder with your table name. 2. Count Record in User Tables in a Database DECLARE @ SqlText NVARCHAR ( MAX ); SELECT @ SqlText = COALESCE ( @ SqlText + CHAR ( 13 ) + 'UNION ALL' + CHAR ( 13 ), '' ) + 'SELECT SCHEMA_NAME(schema_id) + ''.'' + t.name AS [Table Name],' + 'FORMAT(SUM(p.[rows]), ''#,#'') AS [Row Count] ' + 'FROM sys.tables t WITH (NOLOCK) JOIN sys.partitions p WITH (NOLOCK) ON t.[object_id] = p.[object_id] ' + 'AND p.index_id IN (0, 1) ' + 'WHERE t.name=''' + l .[ name ] + ''' ' + 'AND t.[schema_id]=' + CAST ( l .[ schema_id ] AS VARCHAR ( 10 )) + ' ' + 'GROUP BY [schema_id]' + ',t.[name]' FROM sys . tables l WITH ( NOLOCK ) WHERE [ type ] = 'U' ORDER BY SCHEMA_NAME ([ schema_id ]) ,[ name ]; EXEC sp_executesql @ SqlText","tags":"MS SQL Server","url":"https://techjogging.com/fast-record-count-in-table-or-tables-in-database-in-sql-server.html","loc":"https://techjogging.com/fast-record-count-in-table-or-tables-in-database-in-sql-server.html"},{"title":"Redirect 404 Error to Specified URL in Synology DSM","text":"Synology DiskStation Manager (DSM) restricts flexibility of Web Station customization. Access to many settings in Apache or Nginx Web servers are hidden or not available. There are some open doors to accomplish your customization without breaking Synology DSM. If you want to address 404 error with your way, it's still possible. The sample below is based on Synology DSM 6.2.2. and it shows 404 error page, and then load a Web page of your choice. You can customize the logic eliminating showing of 404 error page at all and go straight to your page. 1. Create a file with missing name. Don't use any extensions in file name. It has to be the bare name. The file has to be place in root of your Web site. 2. Add HTML content in the missing file. The code is extracted from Synology DSM 404 error page. <!DOCTYPE html> < html > < head > < meta charset = \"utf-8\" > < meta http-equiv = \"refresh\" content = \"2; url = https://techjogging.com\" /> < style > html { height : 100 % } body { margin : 0 auto ; min-height : 600 px ; min-width : 800 px ; height : 100 % }. top { height : 100 px ; height : calc ( 40 % - 140 px )}. bottom { height : 150 px ; height : calc ( 60 % - 210 px )}. center { height : 350 px ; text-align : center ; vertical-align : middle ; font- family : Verdana }. circle { margin : auto ; width : 260 px ; height : 260 px ; border-radius : 50 % ; background : #c0c6cc }. circle_text { line-height : 260 px ; font-size : 100 px ; color : #ffffff ; font-weight : bold }. text { line-height : 40 px ; font-size : 26 px ; color : #505a64 } </ style > </ head > < body > < div class = \"top\" ></ div > < div class = \"center\" > < div class = \"circle\" > < div class = \"circle_text\" > 404 </ div > </ div > < div > < p class = \"text\" id = \"a\" > The page you are looking for cannot be found </ p > </ div > < div class = \"bottom\" ></ div > </ body > </ html > 3. Replace parameters. < meta http-equiv = \"refresh\" content = \"2; url = https://techjogging.com\" /> Delay before redirecting to your URL. Your URL.","tags":"Synology DSM","url":"https://techjogging.com/redirect-404-error-to-specified-url-in-synology-dsm.html","loc":"https://techjogging.com/redirect-404-error-to-specified-url-in-synology-dsm.html"},{"title":"Redirect WWW to HTTPS in Synology DSM Nginx","text":"WWW website prefix has long history and it was used to classify information what was exposed to users of websites. Another usage of WWW might be for load balancing when web traffic is redirected to a cluster of web servers. Also, some sources tell that WWW was introduced accidently as a mistake but everybody considered it as the new standard and started using it. WWW is not required to be used in URLs. In any case, we need to address this prefix to keep back compatibility with the old rules. It means that www.sample.com, https://www.sample.com, and https://www.sample.com URLs should be valid and converted to https://www.sample.com one to make a Web connection secured. 1. Add WWW CNAME record to DSN. The sample is based on noip.com DSN service. 2. Create SSL Certificate in Synology DiskStation Manager (DSM) The easiest way to create a SSL certificate is Synology DSM which supports Let's Encrypt natively. Make sure to add WWW to Subject Alternative Name . 3. Enable SSH service. 4. Install Web Station. 5. Make Nginx Web Server as Default. 6. Modify Moustache Template Sample is based on DSM 6.2.2 operation system and the original moustache template is. server { listen 80 default_server{{#reuseport}} reuseport{{/reuseport}}; listen [::]:80 default_server{{#reuseport}} reuseport{{/reuseport}}; gzip on; {{> /usr/syno/share/nginx/WWW_Main}} location ~ &#94;/$ { rewrite / http://$host:{{DSM.port}}/ redirect; } } server { listen 443 default_server ssl{{#reuseport}} reuseport{{/reuseport}}; listen [::]:443 default_server ssl{{#reuseport}} reuseport{{/reuseport}}; {{#DSM.https.compression}} gzip on; {{/DSM.https.compression}} {{> /usr/syno/share/nginx/WWW_Main}} location ~ &#94;/$ { rewrite / https://$host:{{DSM.ssl.port}}/ redirect; } } Use ssh client to access your Synology NAS with a user which has administrative permission. It can be PuTTY tool in Windows or terminal with ssh command in Unix. Switch to root user sudo su - Back up the current moustache template cp /usr/syno/share/nginx/WWWService.mustache /usr/syno/share/nginx/WWWService.mustache.bak Open the moustache template for editing vi /usr/syno/share/nginx/WWWService.mustache Replace 4 lines in port 80 section {{> /usr/syno/share/nginx/WWW_Main}} location ~ &#94;/$ { rewrite / http://$host:{{DSM.port}}/ redirect; } with those 2 lines server_name _; return 301 https://$host$request_uri; The final content should be server { listen 80 default_server{{#reuseport}} reuseport{{/reuseport}}; listen [::]:80 default_server{{#reuseport}} reuseport{{/reuseport}}; gzip on; server_name _; return 301 https://$host$request_uri; } server { listen 443 default_server ssl{{#reuseport}} reuseport{{/reuseport}}; listen [::]:443 default_server ssl{{#reuseport}} reuseport{{/reuseport}}; {{#DSM.https.compression}} gzip on; {{/DSM.https.compression}} {{> /usr/syno/share/nginx/WWW_Main}} location ~ &#94;/$ { rewrite / https://$host:{{DSM.ssl.port}}/ redirect; } } Restart the Nginx web server to apply the changes synoservicecfg --restart nginx The last important step is to refresh your browser. When you open your web site with http , it's still showing as http and don't redirect to https . Just click Ctrl-F5 .","tags":"Synology DSM","url":"https://techjogging.com/redirect-www-to-https-in-synology-nas-nginx.html","loc":"https://techjogging.com/redirect-www-to-https-in-synology-nas-nginx.html"},{"title":"Connect DBeaver to MS SQL Server with JAAS Using Kerberos Keytab in Windows","text":"Kerberos authentication can be established by applying Kerberos ticket cache or keytab file. Kerberos ticket cache method has a disadvantage, ticket cache should be renewed on regular basis. If it is not automatic process, it will request our attention. Another option is to use Kerberos keytab file. First of all, it has to be regenerated only in case of changing password for kerberized account. It's beneficial when a service account is involved. In that scenario, an administrator might maintain keytab file and distribute it to users. Also, keytab file be created by users. 1. Create Kerberos keytab File Keytab file can be created following Create keytab File for Kerberos Authentication in Windows article. 2. Create JAAS Configuration File JAAS file stores Kerberos setup used by Microsoft JDBC Driver for SQL Server driver. Krb5LoginModule module authenticates users using Kerberos protocols. SQLJDBCDriver { com.sun.security.auth.module.Krb5LoginModule required doNotPrompt = true useKeyTab = true keyTab = \"keytabname.keytab\" useTicketCache = false renewTGT = false principal = \"windowsserviceaccount@SAMPLE.COM\"; }; Make your changes in the sample: Replace keyTab=\"keytabname.keytab\" setting with your keytab file name. Keytab file name without path means that the file is stored in root folder of your DBeaver installation, for example, C:\\dbeaver-ce-7.0.0-win32.win32.x86_64 . Make sure that you use double back slash (\\) or forward slash (/) in file path, for example, C:\\\\kerberos\\\\keytabname.keytab or C:/kerberos/keytabname.keytab Replace principal=\"windowsserviceaccount@SAMPLE.COM\" setting with your service account name which kerberized in keytab file and your default realm. 3. DBeaver Kerberos Setup Add JAAS file name and location to dbeaver.ini DBeaver configuration file -Djava.security.auth.login.config = jaas.conf The file name is jaas.conf and the location is a root folder of DBeaver installation, for example, C:\\dbeaver-ce-7.0.0-win32.win32.x86_64 . Optionally, it can be added a command to relax the usual restriction of requiring a GSS mechanism . Your network security configuration might make it as mandatory. -Djavax.security.auth.useSubjectCredsOnly = false -Djava.security.auth.login.config = jaas.conf Optionally, krb5.ini Kerberos configuration file can be added. The file can be obtained from your administrator. -Djavax.security.auth.useSubjectCredsOnly = false -Djava.security.krb5.conf = krb5.ini -Djava.security.auth.login.config = jaas.conf The file location is a root folder of DBeaver installation, for example, C:\\dbeaver-ce-7.0.0-win32.win32.x86_64 . This is a sample of the final dbeaver.ini file for DBeaver Community edition version 7.0.0. -startup plugins/org.eclipse.equinox.launcher_1.5.600.v20191014-2022.jar --launcher.library plugins/org.eclipse.equinox.launcher.win32.win32.x86_64_1.1.1100.v20190907-0426 -vmargs -XX:+IgnoreUnrecognizedVMOptions --add-modules = ALL-SYSTEM -Xms64m -Xmx1024m -Djavax.security.auth.useSubjectCredsOnly = false -Djava.security.krb5.conf = krb5.ini -Djava.security.auth.login.config = jaas.conf 4. Add New MS SQL Connection Open New Database Connection wizard from Database menu and select MS SQL Server driver. On the next step, replace Host with your server name and choose Kerberos from Authentication list. Click Finish button to complete wizard. Next screen will request to download drivers. Just click Download button. Be patient, it takes time to retrieve SQL server meta data. 5. Upgrade MS SQL Drivers It's an optional step if you are able to connect to MS SQL Server. Download Microsoft JDBC Driver for SQL Server Unzip driver and store in C:\\Users\\windowsuser\\AppData\\Roaming\\DBeaverData\\drivers DBeaver settings folder, for example, C:\\Users\\windowsuser\\AppData\\Roaming\\DBeaverData\\drivers\\sqljdbc_8.2 Edit created connection Click Edit Driver Settings button Delete all files in Libraries tab Click Add File button and select downloaded files a) JDBC driver corresponding your java version, for example, java 1.8 - mssql-jdbc-8.2.2.jre8.jar. b) Authentication library. It should be 64 bit - mssql-jdbc_auth-8.2.2.x64.dll. Click OK button twice to complete setup. Restart DBeaver to apply new drivers.","tags":"DBeaver","url":"https://techjogging.com/connect-dbeaver-to-ms-sql-server-with-jaas-using-kerberos-keytab-file-in-windows.html","loc":"https://techjogging.com/connect-dbeaver-to-ms-sql-server-with-jaas-using-kerberos-keytab-file-in-windows.html"},{"title":"Connect DBeaver to MS SQL Server with Kerberos Ticket Cache in Windows","text":"The easiest way to connect DBeaver to MS SQL Server with Kerberos authentication is Kerberos ticket cache. It requests only 2 steps. The first step is to create Kerberos ticket cache and the second one is to add a new connection to DBeaver with default settings. There are variety of tool in Windows to create Kerberos ticket cache. Also, you can apply different techniques to generate ticket cache. 1. Create Kerberos Ticket Cache File See Create Ticket Cache File for Kerberos Authentication in Windows article. 2. Add New MS SQL Connection Open New Database Connection wizard from Database menu and select MS SQL Server driver. On the next step, replace Host with your server name and choose Kerberos from Authentication list. Click Finish button to complete wizard. Next screen will request to download drivers. Just click Download button. Be patient, it takes time to retrieve SQL server meta data. 3. Upgrade MS SQL Drivers It's an optional step if you are not able to connect to MS SQL Server. Download Microsoft JDBC Driver for SQL Server Unzip driver and store in C:\\Users\\windowsuser\\AppData\\Roaming\\DBeaverData\\drivers DBeaver settings folder, for example, C:\\Users\\windowsuser\\AppData\\Roaming\\DBeaverData\\drivers\\sqljdbc_8.2 Edit created connection Click Edit Driver Settings button Delete all files in Libraries tab Click Add File button and select downloaded files a) JDBC driver corresponding your java version, for example, java 1.8 - mssql-jdbc-8.2.2.jre8.jar. b) Authentication library. It should be 64 bit - mssql-jdbc_auth-8.2.2.x64.dll. Click OK button twice to complete setup. Restart DBeaver to apply new drivers.","tags":"DBeaver","url":"https://techjogging.com/connect-dbeaver-to-ms-sql-server-with-kerberos-ticket-cache-in-windows.html","loc":"https://techjogging.com/connect-dbeaver-to-ms-sql-server-with-kerberos-ticket-cache-in-windows.html"},{"title":"Enable gzip Compression in Synology DSM in Nginx","text":"Enabling gzip compression for your website can be done in Synology DiskStation Manager (DSM). HTTP traffic is already compressed with gzip as default. HTTPS protocol needs to be enabled explicitly. Synology DSM is doing it in an easy step without rebooting Synology NAS. Also, Synology DSM includes text/javascript and text/css MIME types additional to text/html one. 1. Validate Compression 1.1 Firefox Web browser Open your website in Firefox browser Click Shift-F12 to open Developer Tools panels. Go to Network panel Click Reload button in the panel or Ctrl-F5 buttons to initiate your website data load. Filter resources which you are interested in. Those resources are: HTML , CSS , and JS , for example, HTML website root page. Click on a resource to see compression and other resource properties, for example, HTML resource. Clear cache. Gzip compression is set up. 1.2 Chrome Web browser The steps to get information are very similar to Firefox. To open Chrome DevTools, press Command+Option+C (Mac) or Ctrl+Shift+C (Windows, Linux, Chrome OS). 2. Enable gzip Compression in DSM Open Control Panel in Synology DiskStation Manager. Go to Security settings. Select Advanced tab and flag Enable HTTP Compression . Apply the setting pressing Save button. 3. Include Different MIME Types Compression is applied to HTML MIME type as default. Also, compression is applied to CSS and JS in Synology DSM. If compression is missing for CSS and JS resources, it can be set up. The sample is based on DSM 6.2.2 operation system. Use ssh client to access your Synology NAS with a user which has administrative permission. It can be PuTTY tool in Windows or terminal with ssh command in Unix. Switch to root user. sudo su - Back up the current moustache template. cp /usr/syno/share/nginx/WWWService.mustache /usr/syno/share/nginx/WWWService.mustache.bak Open the moustache template for editing. vi /usr/syno/share/nginx/WWWService.mustache Add a line after gzip on; command to both HTTP and HTTPS server sections. gzip_types text/javascript text/css; The final content should be. server { listen 80 default_server{{#reuseport}} reuseport{{/reuseport}}; listen [::]:80 default_server{{#reuseport}} reuseport{{/reuseport}}; gzip on; gzip_types text/javascript text/css; {{> /usr/syno/share/nginx/WWW_Main}} location ~ &#94;/$ { rewrite / http://$host:{{DSM.port}}/ redirect; } } server { listen 443 default_server ssl{{#reuseport}} reuseport{{/reuseport}}; listen [::]:443 default_server ssl{{#reuseport}} reuseport{{/reuseport}}; {{#DSM.https.compression}} gzip on; gzip_types text/javascript text/css; {{/DSM.https.compression}} {{> /usr/syno/share/nginx/WWW_Main}} location ~ &#94;/$ { rewrite / https://$host:{{DSM.ssl.port}}/ redirect; } } Restart the Nginx web server to apply the changes. synoservicecfg --restart nginx","tags":"Synology DSM","url":"https://techjogging.com/enable-gzip-compression-in-synology-nas-in-nginx.html","loc":"https://techjogging.com/enable-gzip-compression-in-synology-nas-in-nginx.html"},{"title":"Create Keytab for Kerberos Authentication in Windows","text":"There are two ways to utilize Kerberos authentication: Kerberos ticket cache and Kerberos keytab. Windows has a limited set of tools to create a keytab file. There are a couple of tools for this purpose. One tool is the Windows Server built-in utility ktpass . It can be only run on a Windows Server. Another tool is ktab which can be used on any Windows computer. ktab tool is a part of Java installation. 1. ktpass There are some restrict requirements to run the tool. It must be run on either a member server or a domain controller of the Active Directory domain. Windows Server operating system such as Windows Server 2008, 2012, or 2016 are supported. When running ktpass.exe, Windows Command Prompt must be run with Run as administrator option. ktpass -princ [ Windows user name ] @ [ Realm name ] -pass [ Password ] -crypto [ Encryption type ] -ptype [ Principle type ] -kvno [ Key version number ] -out [ Keytab file path ] [Windows user name] - mywindowsname. [Real name] - SAMPLE.COM. [Password] - mywindowsname user password. [Encryption type] - RC4-HMAC-NT. See RFC 3961, section 8 . [Principle type] - KRB5_NT_PRINCIPAL which is Kerberos protocol 5. [Key version number] - 0. [Keytab file path] - c:\\kerberos\\keytabname.keytab. 2. ktab It requests to install Java JRE or SDK or open source equivalent, for example, OpenJDK. The tool has a limited set of options. It can't be defined encryption and principle types. It will be used Kerberos protocol 5 and it will be created multiple encryption types. ktab -a [ Windows user name ] @ [ Realm name ] [ Password ] -n [ Key version number ] -k [ Keytab file path ] List all encryption types stored in a keytab file ktab -l -e -k [ Keytab file path ] If multiple encryption types are not accepted in authentication process, it can be left one encryption type and the rest can be deleted. ktab -d [ Windows user name ] @ [ Realm name ] -f -e [ Number of encryption type ] -k [ Keytab file path ] [Number of encryption type] - 16. As per RFC 3961, section 8. 3. Usage Samples 3.1. DBeaver connection to Hive with Kerberos Authentication It can be created multiple encryption types in a keytab file. Create a keytab file ktab -a mywindowsname@SAMPLE.COM mypassword -n 0 -k c: \\k erberos \\m ywindowsname.keytab Done! Service key for mywindowsname@SAMPLE.COM is saved in c: \\k erberos \\m ywindowsname.keytab List content of the keytab file ktab -l -e -k c: \\k erberos \\m ywindowsname.keytab Keytab name: c: \\k erberos \\m ywindowsname.keytab KVNO Principal ---- --------------------------------------------------------------- 0 mywindowsname@SAMPLE.COM ( 18 :AES256 CTS mode with HMAC SHA1-96 ) 0 mywindowsname@SAMPLE.COM ( 17 :AES128 CTS mode with HMAC SHA1-96 ) 0 mywindowsname@SAMPLE.COM ( 16 :DES3 CBC mode with SHA1-KD ) 0 mywindowsname@SAMPLE.COM ( 23 :RC4 with HMAC ) 3.2. Talend tHDFSConnection Component with Kerberos Authentication It should be one encryption type in a keytab file, for example, 23. Create a keytab file ktab -a mywindowsname@SAMPLE.COM mypassword -n 0 -k c: \\k erberos \\m ywindowsname.keytab Done! Service key for mywindowsname@SAMPLE.COM is saved in c: \\k erberos \\m ywindowsname.keytab List content of the keytab file ktab -l -e -k c: \\k erberos \\m ywindowsname.keytab Keytab name: c: \\k erberos \\m ywindowsname.keytab KVNO Principal ---- --------------------------------------------------------------- 0 mywindowsname@SAMPLE.COM ( 18 :AES256 CTS mode with HMAC SHA1-96 ) 0 mywindowsname@SAMPLE.COM ( 17 :AES128 CTS mode with HMAC SHA1-96 ) 0 mywindowsname@SAMPLE.COM ( 16 :DES3 CBC mode with SHA1-KD ) 0 mywindowsname@SAMPLE.COM ( 23 :RC4 with HMAC ) Delete unused encryption types ## 16-18 ktab -d mywindowsname@SAMPLE.COM -f -e 16 -k c: \\k erberos \\m ywindowsname.keytab Done! 1 entries removed. ktab -d mywindowsname@SAMPLE.COM -f -e 17 -k c: \\k erberos \\m ywindowsname.keytab Done! 1 entries removed. ktab -d mywindowsname@SAMPLE.COM -f -e 18 -k c: \\k erberos \\m ywindowsname.keytab Done! 1 entries removed. List content of the keytab file again ktab -l -e -k c: \\k erberos \\m ywindowsname.keytab Keytab name: c: \\k erberos \\m ywindowsname.keytab KVNO Principal ---- --------------------------------------------------------------- 0 mywindowsname@SAMPLE.COM ( 23 :RC4 with HMAC ) 3.3. Windows It depends on Windows account settings how many encryption types and what types can be used. Windows account properties dialog contains the next options for Kerberos authentication. 4. Encryption types As per RFC 3961, section 8 . Encryption Type Code Section or Comment des-cbc-crc 1 6.2.3 des-cbc-md4 2 6.2.2 des-cbc-md5 3 6.2.1 [reserved] 4 des3-cbc-md5 5 [reserved] 6 des3-cbc-sha1 7 dsaWithSHA1-CmsOID 9 (pkinit) md5WithRSAEncryption-CmsOID 10 (pkinit) sha1WithRSAEncryption-CmsOID 11 (pkinit) rc2CBC-EnvOID 12 (pkinit) rsaEncryption-EnvOID 13 (pkinit from PKCS#1 v1.5) rsaES-OAEP-ENV-OID 14 (pkinit from PKCS#1 v2.0) des-ede3-cbc-Env-OID 15 (pkinit) des3-cbc-sha1-kd 16 6.3 aes128-cts-hmac-sha1-96 17 [KRB5-AES] aes256-cts-hmac-sha1-96 18 [KRB5-AES] rc4-hmac 23 (Microsoft) rc4-hmac-exp 24 (Microsoft) subkey-keymaterial 65 (opaque; PacketCable)","tags":"Kerberos","url":"https://techjogging.com/create-keytab-file-for-kerberos-authentication-in-windows.html","loc":"https://techjogging.com/create-keytab-file-for-kerberos-authentication-in-windows.html"},{"title":"Build DBeaver Installation Package to Access Hive with JAAS Using Kerberos Authentication in Windows","text":"When your organization has decided to start using DBeaver, you need to plan how to deploy and setup DBeaver for users. It might not be an issue if you are going to utilize databases/drivers which included in the standard configuration. But let's imagine case when you need to connect to Hive with Kerberos authentication. This task request advanced skills and time for troubleshooting in case of encountering any issues. Setup The following sample based on Cloudera Hive JDBC driver v. 2.6.5.1007 and DBeaver EE v. 6.3.0 64 bit or DBeaver CE v. 7.0.0 64 bit edition. Download Windows 64 bit (zip) DBeaver installation and unzip it, for example, to C: drive. Your root folder is C:\\dbeaver-ce-7.0.0-win32.win32.x86_64 . Download and unzip Cloudera Hive JDBC driver. Place it in C:\\dbeaver-ce-7.0.0-win32.win32.x86_64\\DBeaverData\\drivers folder. C:\\dbeaver-ce-7.0.0-win32.win32.x86_64\\DBeaverData └───drivers └───hive_jdbc_2.6.5.1007 ├───ClouderaHiveJDBC41-2.6.5.1007 │ HiveJDBC41.jar │ └───docs Cloudera-JDBC-Driver-for-Apache-Hive-Install-Guide.pdf Cloudera-JDBC-Driver-for-Apache-Hive-Release-Notes.txt third-party-licenses.txt Copy or modify the following configuration files in C:\\dbeaver-ce-7.0.0-win32.win32.x86_64 DBeaver root folder. You can figure out how make them from Connect DBeaver to Cloudera Hive with JAAS Configuration using Kerberos Authentication in Windows article. 1) dbeaver.ini 2) jaas.conf 3) krb5.ini 4) organization-service-account.keytab Create dbeaver.vbs file and placed in C:\\dbeaver-ce-7.0.0-win32.win32.x86_64 DBeaver root folder as well. This file is a DBeaver launcher and our goal is to redirect DBeaver workspace from Windows User Home folder to DBeaver root one. We could create Windows batch file but it would show Command Prompt window during DBeaver start. Set WshShell = CreateObject ( \"WScript.Shell\" ) WshShell . Run \"dbeaver.exe -data ./DBeaverData/workspace6\" , 0 Set WshShell = Nothing Run DBeaver with dbeaver.vbs launcher and create a Hive driver and a new Hive connection as per Connect DBeaver to Cloudera Hive with JAAS Configuration using Kerberos Authentication in Windows article. After this step, it has to be created workspace folders by DBeaver. C:\\dbeaver-ce-7.0.0-win32.win32.x86_64\\DBeaverData └───workspace6 ├───.metadata │ ├───.plugins │ │ ├───org.eclipse.core.resources │ │ │ ├───.history │ │ │ ├───.projects │ │ │ │ └───General │ │ │ ├───.root │ │ │ │ └───.indexes │ │ │ └───.safetable │ │ ├───org.eclipse.core.runtime │ │ │ └───.settings │ │ ├───org.eclipse.e4.ui.workbench.swt │ │ ├───org.eclipse.e4.workbench │ │ ├───org.eclipse.ui.workbench │ │ ├───org.jkiss.dbeaver.core │ │ │ └───security │ │ ├───org.jkiss.dbeaver.model │ │ └───org.jkiss.dbeaver.ui │ └───qmdb └───General ├───.dbeaver └───Scripts The final step is to zip C:\\dbeaver-ce-7.0.0-win32.win32.x86_64 DBeaver root folder. Your zipped file name is dbeaver-ce-7.0.0-win32.win32.x86_64.zip . Deployment and Setup Users get dbeaver-ce-7.0.0-win32.win32.x86_64.zip file and they need to unzip it in a desired folder. If DBeaver has been set up in a network folder, it will be one limitation. You need to use mapped driver to run DBeaver, for example, Y:\\myuser\\desktop\\dbeaver-ce-7.0.0-win32.win32.x86_64 . It will not work if you run from \\\\networkserver\\UserHome\\myuser\\desktop\\dbeaver-ce-7.0.0-win32.win32.x86_64 folder. This limitation is caused by a reference to HiveJDBC41.jar library. You can run it from a network folder with UNC path but you need to make \\\\networkserver\\UserHome\\myuser\\desktop\\dbeaver-ce-7.0.0-win32.win32.x86_64 folder as your DBeaver root one and complete steps ## 1-6 in that folder.","tags":"DBeaver","url":"https://techjogging.com/build-dbeaver-installation-package-to-access-hive-with-jaas-configuration-using-kerberos-authentication-in-windows.html","loc":"https://techjogging.com/build-dbeaver-installation-package-to-access-hive-with-jaas-configuration-using-kerberos-authentication-in-windows.html"},{"title":"Connect DBeaver to Cloudera Hive with JAAS using Kerberos Authentication in Windows","text":"DBeaver allows connecting to a wide range of databases including Cloudera Hive. Hive driver is part of DBeaver installation but it uses basic authentication with user name and password. Kerberos authentication is another option to connect to Hive. It can be accomplished by adding a new driver to DBeaver. The DBeaver driver is based on Cloudera JDBC Driver for Hive and JAAS configuration file. Legend Before going forward, let's get agreed with the initial information used in configuration files. You can easily replace it with configurations applicable to your cases. It will be a kind of a template. Windows user name: mywindowsuser. Hive name host: hivehost. Kerberos realm if your organization supports multiple regions: REGION.SAMPLE.COM. Hive port: 10000. Prerequisites Kerberos configuration file: krb5.conf. It can be obtained from your Kerberos administrator or from the /etc/krb5.conf folder on the machine that is hosting the Hive Server 2 instance. One of the files below. It depends on which method is chosen. a) Kerberos credential cache: krb5cc_mywindowsuser. This file contains your Windows kerberized credentials. Using this file will request to renew it. b) Kerberos keytab file: mywindowsuser.keytab. This file stores your Windows kerberized credentials. It doesn't request renewal. Setup The following sample based on DBeaver EE v. 6.3.0 64 bit and Cloudera Hive JDBC driver v. 2.6.5.1007 . Download and unzip Cloudera Hive JDBC driver. Place the unzipped folder to the permanent location. DBeaver reads it from that location every time when the driver is used. It's advisable to read the driver documentation from ./hive_jdbc_2.6.5.1007/docs folder. Registering the Driver Class section tells the class name used in DBeaver driver setup and Using Kerberos section explains each parameter utilized in the driver setup. Append those 4 lines to dbeaver.ini file. The location of the ini file can be different. If you have installed DBeaver, it will be in C:\\Program Files\\DBeaverEE folder. If you use zipped installation, it will be a root folder where it has been unzipped. -Djavax.security.auth.useSubjectCredsOnly = false -Dsun.security.krb5.debug = true -Djava.security.krb5.conf = C:/Users/mywindowsuser/DBeaver/krb5.ini -Djava.security.auth.login.config = C:/Users/mywindowsuser/DBeaver/jaas.conf It's very critical to use correct path separator. It has to be \"/\" forward slash character or you have to use \"\\\\\" double back slash characters. After successful completion of the setup, you need to remove the line. -Dsun.security.krb5.debug = true The final beaver.ini file is -startup plugins/org.eclipse.equinox.launcher_1.5.600.v20191014-2022.jar --launcher.library plugins/org.eclipse.equinox.launcher.win32.win32.x86_64_1.1.1100.v20190907-0426 -vmargs -XX:+IgnoreUnrecognizedVMOptions --add-modules = ALL-SYSTEM -Xms128m -Xmx2048m -Djavax.security.auth.useSubjectCredsOnly = false -Dsun.security.krb5.debug = true -Djava.security.krb5.conf = C:/Users/mywindowsuser/DBeaver/krb5.ini -Djava.security.auth.login.config = C:/Users/mywindowsuser/DBeaver/jaas.conf Move krb5.conf file with new krb5.ini name to C:/Users/mywindowsuser/DBeaver folder. You don't need to do any changes to the file. Create JAAS configuration file Option #1. Kerberos credential cache. Client { com.sun.security.auth.module.Krb5LoginModule required debug = true doNotPrompt = true useKeyTab = true keyTab = \"C:/Users/mywindowsuser/krb5cc_mywindowsuser\" useTicketCache = true renewTGT = true principal = \"mywindowsuser@REGION.SAMPLE.COM\"; }; Option #2. Kerberos keytab file. Client { com.sun.security.auth.module.Krb5LoginModule required debug = true doNotPrompt = true useKeyTab = true keyTab = \"mywindowsuser.keytab\" useTicketCache = false renewTGT = false principal = \"mywindowsuser@REGION.SAMPLE.COM\"; }; After successful completion of the setup, you need to remove the line. debug = true Create Kerberos authentication file Option #1. Kerberos credential cache. See Create Ticket Cache File for Kerberos Authentication in Windows article. Option #2. Kerberos keytab file. See Create keytab File for Kerberos Authentication in Windows article. Create a new driver in DBeaver Option #1. Kerberos credential cache. Driver Name: Hive-Cloudera Class Name: com.cloudera.hive.jdbc41.HS2Driver URL Template: jdbc:hive2://hivehost:10000/{database};AuthMech=1;KrbRealm=REGION.SAMPLE.COM;KrbServiceName=hive;KrbHostFQDN=hivehost.region.sample.com;KrbAuthType=2; Default Port: 10000 Category: Hadoop Add jar file to the driver: ./hive_jdbc_2.6.5.1007/ClouderaHiveJDBC41-2.6.5.1007/HiveJDBC41.jar Option #2. Kerberos keytab file. Driver Name: Hive-Cloudera Class Name: com.cloudera.hive.jdbc41.HS2Driver URL Template: jdbc:hive2://hivehost:10000/{database};AuthMech=1;KrbRealm=REGION.SAMPLE.COM;KrbServiceName=hive;KrbHostFQDN=hivehost.region.sample.com;KrbAuthType=1; Default Port: 10000 Category: Hadoop Add jar file to the driver: ./hive_jdbc_2.6.5.1007/ClouderaHiveJDBC41-2.6.5.1007/HiveJDBC41.jar Create a new connection in DBeaver Utilize the driver created on step #6. The name is Hive-Cloudera . Add default to Database/Schema field. Flag Save password locally . Test your new connection clicking Test Connection button. Troubleshooting Check the debug log in C:\\Users\\mywindowsuser\\AppData\\Roaming\\DBeaverData\\workspace6\\.metadata\\dbeaver-debug.log folder.","tags":"DBeaver","url":"https://techjogging.com/connect-dbeaver-to-cloudera-hive-with-jaas-configuration-using-kerberos-authentication-in-windows.html","loc":"https://techjogging.com/connect-dbeaver-to-cloudera-hive-with-jaas-configuration-using-kerberos-authentication-in-windows.html"},{"title":"Cron Scheduler with Docker Container in CentOS/RHEL 7","text":"Using cron with the official CentOS Docker image requests activating systemd, keeping container running, and opening a Docker container in privileged mode. CentOS Docker Hub image includes the description of the setup which should be done to activate systemd and keep a container going. The missing is information how to install and set up cron. This article provides a summary of steps and a functioning sample of Dockerfile. Some customization is needed to implement your cron project. Dockerfile The Dockerfile can be used as a template to design your file. After line #15, you can add your commands to install any packages. You need to replace line #20 with your time zone. Finally, line #22 shows how to add a scheduled job to crontab file. ROM centos:7 ENV container docker RUN ( cd /lib/systemd/system/sysinit.target.wants/ ; for i in * ; do [ $i == \\ systemd-tmpfiles-setup.service ] || rm -f $i ; done ) ; \\ rm -f /lib/systemd/system/multi-user.target.wants/* ; \\ rm -f /etc/systemd/system/*.wants/* ; \\ rm -f /lib/systemd/system/local-fs.target.wants/* ; \\ rm -f /lib/systemd/system/sockets.target.wants/*udev* ; \\ rm -f /lib/systemd/system/sockets.target.wants/*initctl* ; \\ rm -f /lib/systemd/system/basic.target.wants/* ; \\ rm -f /lib/systemd/system/anaconda.target.wants/* ; VOLUME [ \"/sys/fs/cgroup\" ] RUN yum install -y cronie && yum clean all RUN rm -rf /etc/localtime RUN ln -s /usr/share/zoneinfo/America/Toronto /etc/localtime RUN crontab -l | { cat ; echo \"25 04 * * sun,mon,tue python3 /app/do_maintenance.py\" ; } | crontab - CMD [ \"/usr/sbin/init\" ] Dockerfile logical parts Activating systemd. RUN ( cd /lib/systemd/system/sysinit.target.wants/ ; for i in * ; do [ $i == \\ systemd-tmpfiles-setup.service ] || rm -f $i ; done ) ; \\ rm -f /lib/systemd/system/multi-user.target.wants/* ; \\ rm -f /etc/systemd/system/*.wants/* ; \\ rm -f /lib/systemd/system/local-fs.target.wants/* ; \\ rm -f /lib/systemd/system/sockets.target.wants/*udev* ; \\ rm -f /lib/systemd/system/sockets.target.wants/*initctl* ; \\ rm -f /lib/systemd/system/basic.target.wants/* ; \\ rm -f /lib/systemd/system/anaconda.target.wants/* ; VOLUME [ \"/sys/fs/cgroup\" ] Install cron. RUN yum install -y cronie && yum clean all Set up your time zone. RUN rm -rf /etc/localtime RUN ln -s /usr/share/zoneinfo/America/Toronto /etc/localtime Add a job in crontab file. RUN crontab -l | { cat ; echo \"25 04 * * sun,mon,tue python3 /app/do_maintenance.py\" ; } | crontab - Keep container running. CMD [ \"/usr/sbin/init\" ] Build conatiner The image name is c7-cron and it's designated as local one. docker build --rm -t local/c7-cron . Run container To create and start a container, use the command. docker run --privileged --name = parking –v /sys/fs/cgroup:/sys/fs/cgroup:ro -d local/c7-cron To access the container in a terminal, run the command. docker exec -it parking /bin/bash","tags":"Docker","url":"https://techjogging.com/cron-scheduler-with-docker-container-in-centosrhel-7.html","loc":"https://techjogging.com/cron-scheduler-with-docker-container-in-centosrhel-7.html"},{"title":"Resize Disk in Oracle VM VirtualBox","text":"Virtualization is flexible in terms of using resources. A virtual machine can be built with minimal assigned resources and later, they can be added when it's needed. Oracle VM VirtualBox allows allocation more space to an existing disk. This procedure is common for all virtualized operation systems. It's similar to replacing your old hard drive with new one which is larger size. Run Oracle VM VirtualBox Manager and open settings of a virtual machine. It can be observed that the disk of the virtual machine is run out of space. Open Virtual Media Manager Select disk to add more space After applying the change, open settings of the virtual machine again to validate it Next steps depend on your operation system. You need to run your virtual machine and consume added space. Follow article to set it up for CentOS/RHEL.","tags":"Virtualization","url":"https://techjogging.com/resize-disk-in-oracle-vm-virtualbox.html","loc":"https://techjogging.com/resize-disk-in-oracle-vm-virtualbox.html"},{"title":"Expand Logical Volume in CentOS/RHEL 7","text":"As Linux systems have root file systems on a logical volume, it can be used Logical Volume Management (LVM) to resize the volume. The exercise of logical volume expanding is completed in case of adding an extra disk to a physical system or having a pool of storage in a virtual environment. Run fdisk or gdisk partition tool. gdisk is used if the partition layout is GPT otherwise fdisk has to be used. gdisk will make your system unbootable if you don't have GPT partition. Both tools work identically when a new partiton is created. How to figure out if GPT partition is present in your system? Assiming that sda is device with available space, run gdisk /dev/sda command. This screen shows that your partition is GPT. $ gdisk /dev/sda GPT fdisk (gdisk) version 0.8.10 Partition table scan: MBR: protective BSD: not present APM: not present GPT: present Found valid GPT with protective MBR; using GPT. In case of MBR partition your screen is. $ gdisk /dev/sda GPT fdisk (gdisk) version 0.8.10 Partition table scan: MBR: MBR only BSD: not present APM: not present GPT: not present *************************************************************** Found invalid GPT and valid MBR; converting MBR to GPT format in memory. THIS OPERATION IS POTENTIALLY DESTRUCTIVE! Exit by typing 'q' if you don't want to convert your MBR partitions to GPT format! *************************************************************** Create a new logical volume partition. Enter 8E00 partition code. Command (? for help): n Partition number (6-128, default 6): First sector (31457280-52428766, default = 31457280) or {+-}size{KMGTP}: Last sector (31457280-52428766, default = 52428766) or {+-}size{KMGTP}: Current type is 'Linux filesystem' Hex code or GUID (L to show codes, Enter = 8300): 8E00 Changed type of partition to 'Linux LVM' Apply changes. Command (? for help): w Final checks complete. About to write GPT data. THIS WILL OVERWRITE EXISTING PARTITIONS!! Do you want to proceed? (Y/N): Y OK; writing new GUID partition table (GPT) to /dev/sda. Warning: The kernel is still using the old partition table. The new table will be used at the next reboot. The operation has completed successfully. Notify the operation system about changes in the partition tables. $ partprobe Validate the new created partition. It can be used either fdisk or gdisk partition tool $ fdisk -l /dev/sda WARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion. Disk /dev/sda: 26.8 GB, 26843545600 bytes, 52428800 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: gpt Disk identifier: 71DD2E79-BD1C-4713-9880-22664C87E57B # Start End Size Type Name 1 2048 2099199 1G Linux filesyste Linux filesystem 2 2099200 16777215 7G Linux LVM Linux LVM 3 16777216 20971519 2G Linux LVM Linux LVM 4 20971520 31457279 5G Linux LVM Linux LVM 5 34 2047 1007K BIOS boot BIOS boot partition 6 31457280 52428766 10G Linux LVM Linux LVM Find out what logical groups/volumes are available. $ lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert root centos -wi-ao---- <13.19g swap centos -wi-ao---- 820.00m Our intertest is to add space to the root file system. The logical volume path is centos/root . In case of RHEL, it might be rhel/root . Create a physical volume. $ pvcreate /dev/sda6 WARNING: ext4 signature detected on /dev/sda6 at offset 1080. Wipe it? [y/n]: y Wiping ext4 signature on /dev/sda6. Physical volume \"/dev/sda6\" successfully created. Extend centos volume group. $ vgextend centos /dev/sda6 Volume group \"centos\" successfully extended Figure out exact free space in PE. The field name is Free PE / Size and the value in the sample is 2559 $ vgdisplay --- Volume group --- VG Name centos System ID Format lvm2 Metadata Areas 4 Metadata Sequence No 8 VG Access read/write VG Status resizable MAX LV 0 Cur LV 2 Open LV 2 Max PV 0 Cur PV 4 Act PV 4 VG Size 23.98 GiB PE Size 4.00 MiB Total PE 6140 Alloc PE / Size 3581 / <13.99 GiB Free PE / Size 2559 / <10.00 GiB VG UUID ZPaYGz-7hbZ-2H6y-RS9W-x13x-2K81-pXCsA3 Extend centos/root logical volume $ lvextend -l+2559 centos/root Size of logical volume centos/root changed from <13.19 GiB (3376 extents) to 23.18 GiB (5935 extents). Logical volume centos/root successfully resized. XFS file system may be grown while mounted using the xfs_growfs command. $ xfs_growfs /dev/centos/root meta-data=/dev/mapper/centos-root isize=512 agcount=9, agsize=406016 blks = sectsz=512 attr=2, projid32bit=1 = crc=1 finobt=0 spinodes=0 data = bsize=4096 blocks=3457024, imaxpct=25 = sunit=0 swidth=0 blks naming =version 2 bsize=4096 ascii-ci=0 ftype=1 log =internal bsize=4096 blocks=2560, version=2 = sectsz=512 sunit=0 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 data blocks changed from 3457024 to 6077440","tags":"Linux","url":"https://techjogging.com/expand-logical-volume-in-centosrhel-7.html","loc":"https://techjogging.com/expand-logical-volume-in-centosrhel-7.html"},{"title":"Convert MBR Partition into GPT in CentOS/RHEL 7","text":"Master Boot Record (MBR) partitioned disks are replaced with newer GUID Partition Table (GPT) standard but MBR is still used widely as a default format. GPT layout for the partition tables has a lot of benefits comparing with MBR one. Along with supporting significantly larger size of disks, it introduces faster and more stable booting. GPT requests to support Unified Extensible Firmware Interface (UEFI) boot. Switch to root user. $ sudo su - Run GPT partition tool. Assuming that sda disk is bootable and will be converted into GPT. $ gdisk /dev/sda GPT fdisk (gdisk) version 0.8.10 Partition table scan: MBR: MBR only BSD: not present APM: not present GPT: not present *************************************************************** Found invalid GPT and valid MBR; converting MBR to GPT format in memory. THIS OPERATION IS POTENTIALLY DESTRUCTIVE! Exit by typing 'q' if you don't want to convert your MBR partitions to GPT format! *************************************************************** Make sure that there is enough space before the first partition to support a boot partition. 2048 value for the first sector confirms that GPT can be applied to MBR. Command (? for help): p Disk /dev/sda: 52428800 sectors, 25.0 GiB Logical sector size: 512 bytes Disk identifier (GUID): 71DD2E79-BD1C-4713-9880-22664C87E57B Partition table holds up to 128 entries First usable sector is 34, last usable sector is 52428766 Partitions will be aligned on 2048-sector boundaries Total free space is 20973501 sectors (10.0 GiB) Number Start (sector) End (sector) Size Code Name 1 2048 2099199 1024.0 MiB 8300 Linux filesystem 2 2099200 16777215 7.0 GiB 8E00 Linux LVM 3 16777216 20971519 2.0 GiB 8E00 Linux LVM 4 20971520 31457279 5.0 GiB 8E00 Linux LVM Create a new bootable partition. As the first sector, enter 34 and the last sector is 2047 . Partition code is ef02 . Command (? for help): n Partition number (5-128, default 5): First sector (34-52428766, default = 31457280) or {+-}size{KMGTP}: 34 Last sector (34-2047, default = 2047) or {+-}size{KMGTP}: Current type is 'Linux filesystem' Hex code or GUID (L to show codes, Enter = 8300): ef02 Changed type of partition to 'BIOS boot partition' Save changes. Command (? for help): w Final checks complete. About to write GPT data. THIS WILL OVERWRITE EXISTING PARTITIONS!! Do you want to proceed? (Y/N): Y OK; writing new GUID partition table (GPT) to /dev/sda. Warning: The kernel is still using the old partition table. The new table will be used at the next reboot. The operation has completed successfully. Notify the operation system about changes. It eliminates rebooting of the system. $ partprobe Install GRUB on the new bootable partition. $ grub2-install /dev/sda Installing for i386-pc platform. Installation finished. No error reported. Validate the conversion. $ gdisk /dev/sda GPT fdisk (gdisk) version 0.8.10 Partition table scan: MBR: protective BSD: not present APM: not present GPT: present Found valid GPT with protective MBR; using GPT.","tags":"Linux","url":"https://techjogging.com/convert-mbr-partition-into-gpt-in-centosrhel-7.html","loc":"https://techjogging.com/convert-mbr-partition-into-gpt-in-centosrhel-7.html"},{"title":"Redirect HTTP to HTTPS in Synology DSM Nginx","text":"Synology DiskStation Manager (DSM) doesn't include GUI based functionality to set up a redirect HTTP web traffic to secured HTTPS version of your web site. The default web server in DSM 6 is Nginx and the configuration of the web server should be adjusted. It can be accomplished making manual changes to the Nginx web server moustache template. Prerequisites SSL certificate is added to Synology NAS. SSH service is enabled. Web Station is installed. Web server is Nginx. Environment Document is based on DSM 6.2.2 operation system Original moustache template server { listen 80 default_server{{#reuseport}} reuseport{{/reuseport}}; listen [::]:80 default_server{{#reuseport}} reuseport{{/reuseport}}; gzip on; {{> /usr/syno/share/nginx/WWW_Main}} location ~ &#94;/$ { rewrite / http://$host:{{DSM.port}}/ redirect; } } server { listen 443 default_server ssl{{#reuseport}} reuseport{{/reuseport}}; listen [::]:443 default_server ssl{{#reuseport}} reuseport{{/reuseport}}; {{#DSM.https.compression}} gzip on; {{/DSM.https.compression}} {{> /usr/syno/share/nginx/WWW_Main}} location ~ &#94;/$ { rewrite / https://$host:{{DSM.ssl.port}}/ redirect; } } Setup Use ssh client to access your Synology NAS with a user which has administrative permission. It can be PuTTY tool in Windows or terminal with ssh command in Unix. Switch to root user sudo su - Back up the current moustache template cp /usr/syno/share/nginx/WWWService.mustache /usr/syno/share/nginx/WWWService.mustache.bak Open the moustache template for editing vi /usr/syno/share/nginx/WWWService.mustache Replace 4 lines in port 80 section {{> /usr/syno/share/nginx/WWW_Main}} location ~ &#94;/$ { rewrite / http://$host:{{DSM.port}}/ redirect; } with those 2 lines server_name _; return 301 https://$host$request_uri; The final content should be server { listen 80 default_server{{#reuseport}} reuseport{{/reuseport}}; listen [::]:80 default_server{{#reuseport}} reuseport{{/reuseport}}; gzip on; server_name _; return 301 https://$host$request_uri; } server { listen 443 default_server ssl{{#reuseport}} reuseport{{/reuseport}}; listen [::]:443 default_server ssl{{#reuseport}} reuseport{{/reuseport}}; {{#DSM.https.compression}} gzip on; {{/DSM.https.compression}} {{> /usr/syno/share/nginx/WWW_Main}} location ~ &#94;/$ { rewrite / https://$host:{{DSM.ssl.port}}/ redirect; } } Restart the Nginx web server to apply the changes synoservicecfg --restart nginx The last important step is to refresh your browser. When you open your web site with http , it's still showing as http and don't redirect to https . Just click Ctrl-F5 .","tags":"Synology DSM","url":"https://techjogging.com/redirect-http-to-https-in-synology-nas-nginx.html","loc":"https://techjogging.com/redirect-http-to-https-in-synology-nas-nginx.html"},{"title":"Extract Number in Microsoft Access/Excel","text":"Microsoft Access and Excel don't include any string functions to extract a number from a string. It should be created a custom function to complete this task. Regular Expressions are a good option to deal with manipulation of text data. Microsoft Access and Excel are lacking of support of regular expressions but they allow to utilize third party libraries. A library of our interest is Microsoft VBScript Regular Expressions 5.5 one. Open your Microsoft Access database or Excel spreadsheet, then go to VBA Editor pressing Alt-F11 combination. Microsoft Access Microsoft Excel Create a new module calling context menu on the root node of the project tree. Microsoft Access Microsoft Excel Copy and paste the function below to the new created module. Function ExtractNumber ( textValue ) Dim re As Object Set re = CreateObject ( \"vbscript.RegExp\" ) re . Pattern = \"[&#94;\\d]\" re . Global = True ExtractNumber = re . Replace ( textValue , \"\" ) End Function This code uses late binding to the library. This method is not preferable but it reduces number of steps to implement the solution. Close VBA Editor. Create a table and a query in Microsoft Access and a column of values in Microsoft Excel to test the function. Microsoft Access Microsoft Excel","tags":"Microsoft Access","url":"https://techjogging.com/extract-number-in-microsoft-accessexcel.html","loc":"https://techjogging.com/extract-number-in-microsoft-accessexcel.html"},{"title":"Set Up Scanner in CentOS 7","text":"As a scanner software can be used Paperwork . It's open source software which is available in both Windows and Unix. CentOS 7 includes Paperwork scanner software as a part of the distribution repository. Make sure that your scanner driver installed in CentOS. This is a sample of Brother printer/scanner driver installation instructions from the official website. Install Scanner Access Now Easy (SANE) application programming interface that provides standardized access to any raster image scanner hardware. sudo yum install sane-backends Enable SANE connection required for scanning. sudo sh -c \"echo 127.0.0.1 >> /etc/sane.d/saned.conf\" Set up SANE service to start automatically and run the service. sudo systemctl enable saned.socket sudo systemctl start saned.socket Open Application Installer from System Tools menu and search for paperwork in search bar. Install Paperwork. After completion it will be available in Office menu. Run Paperwork and open Setting from menu Validate that Device , Default source , and Resolution populated properly as per your scanner.","tags":"Linux","url":"https://techjogging.com/set-up-scanner-in-centos-7.html","loc":"https://techjogging.com/set-up-scanner-in-centos-7.html"},{"title":"Clean Up USB Flash Drive","text":"When a USB flash drive is used in Linux or as an ISO image is recorded to it, USB drive might be is not usable in Windows. diskpart Windows tool can be used to clean up and re-partition USB flash drive. The tool can be run from Windows Command Prompt typing diskpart The first step is to identify an index of our USB drive. Run LIST DISK command Based on size, our flash drive index is 2 and the next step is to select our drive. Run SELECT DISK Now it's time to remove all content from the drive including any partitions. Run CLEAN command After making our flash drive empty, a new primary partition is created. Run CREATE PRIMARY PARTITION command Next step is to format flash drive. A list of file systems is FAT, FAT32, NTFS, exFAT. quick option allows to complete it very fast. Run FORMAT command Final step is to assign a letter to access the flash drive in Windows. Run ASSIGN command Close diskpart tool. Run EXIT","tags":"Windows Tools","url":"https://techjogging.com/clean-up-usb-flash-drive.html","loc":"https://techjogging.com/clean-up-usb-flash-drive.html"}]};