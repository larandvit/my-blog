var tipuesearch = {"pages":[{"title":"About blog","text":"Starting this blog, I want to accomplish some goals. The most important goal is to share my knowledge and experience with others. It's my contribution back to the community of technology gurus who helped me be where I am now. I haven't done anything special. I've just learnt information from different sources, validated, refined, added my research, and published back. I appreciate everybody who makes information open and available to public. We are growing together helping each other. The second goal is to create my personal repository as a quick way to refresh my memory in case of forgetting something. In many cases, we are doing a lot of staff but we can't keep it in our heads. When we need it again, we try to reinvent the wheel because we can't recall what it was done before. The last goal is to express myself. This is when I can be creative without any limits. It's nice to write my articles to bring ideas and innovations to the public. The feelings of excitement is encouraging for future adventures. If you want to contact me, please drop me a line here . You can find me on git as well. Best wishes, Vitaly Saversky","tags":"pages","url":"https://techjogging.com/pages/about-blog.html","loc":"https://techjogging.com/pages/about-blog.html"},{"title":"Access SQL Server with Trusted Connection in Linux with Python Using Kerberos Keytab","text":"When supplying credentials in plain text in Python applications and tools are a concern of the security policy in your company, Kerberos keytab might be a relief. Kerberos keytab hides sensitive information and it serves as a password to authenticate access to different resources. In other words, a keytab is a password replacement. The most common use case is a service account when an application utilizes a special account. All users who run an application are authenticated with the same service account. It is convenient way to access, for example, data located on SQL Server. The sample code is developed in CentOS 7 with Python 3 and pyodbc library. Prerequisites Install Kerberos client. sudo yum install krb5-workstation krb5-libs Install ODBC on non MS Windows platforms. sudo yum install unixODBC-devel Install Microsoft ODBC driver for SQL Server. The sample uses ODBC driver version 13 but the latest version is 17. sudo rpm -i https://packages.microsoft.com/rhel/7/prod/msodbcsql-13.1.9.2-1.x86_64.rpm If you decide for the latest ODBC driver, get one from Download ODBC Driver for SQL Server . Install Python pyodbc library pip install pyodbc Create a Kerberos keytab Follow steps in Create Keytab for Kerberos Authentication in Linux or Create Keytab for Kerberos Authentication in Windows . Sample import pyodbc import os def createKerberosTicket ( user_full_name , keytab_path ): error_code = os . system ( f 'kinit {user_full_name} -k -t {keytab_path}' ) if ( error_code != 0 ): message = 'kinit error: {}' . format ( error_code ) print ( message ) return error_code def sqlserver_connection (): return pyodbc . connect ( \"Driver={ODBC Driver 13 for SQL Server};Server=\" + 'sqlserver.sample.com' + \\ \";Database=\" + 'master' + \\ \";Trusted_Connection=yes\" ) if __name__ == '__main__' : user_name = 'sampleuser' keytab_path = f '/home/sampleuser/{user_name}.keytab' domain_name = 'SAMPLE.COM' user_full_name = f '{user_name}@{domain_name}' createKerberosTicket ( user_full_name , keytab_path ) cur = sqlserver_connection () . cursor () sql_text = 'SELECT @@VERSION' cur . execute ( sql_text ) row = cur . fetchone () print ( row [ 0 ]) Troubleshooting Error message pyodbc.OperationalError: ('HYT00', '[HYT00] [unixODBC][Microsoft][ODBC Driver 13 for SQL Server]Login timeout expired (0) (SQLDriverConnect)') One of the reasons is your SQL Server name. Make sure that it is used Fully Qualified Domain Name (FQDN), for example, sqlserver.sample.com . Check Kerberos ticket(s) created by an application. Open terminal and run commands. klist Or valid ticket(s). klist -s Missing Kerberos ticket cache file variable. It might be requested to create KRB5CCNAME variable with location and name of Kerberos ticket cache file. See more details in Create Ticket Cache File for Kerberos Authentication in Linux article. Missing krb5.conf Kerberos configuration file. The file contains default realm and Kerberos ticket settings. See more details in Create Ticket Cache File for Kerberos Authentication in Linux article. Resources pyodbc kinit - MIT Kerberos Documentation klist - MIT Kerberos Documentation Microsoft ODBC Driver for SQL Server Create Keytab for Kerberos Authentication in Linux Create Keytab for Kerberos Authentication in Windows Create Ticket Cache File for Kerberos Authentication in Linux","tags":"Python","url":"https://techjogging.com/access-sqlserver-trusted-connection-linux-python-kerberos-keytab.html","loc":"https://techjogging.com/access-sqlserver-trusted-connection-linux-python-kerberos-keytab.html"},{"title":"Access SQL Server with Trusted Connection in Linux with Python Using Kerberos Ticket","text":"Integration of Windows and Linux environments is common in many companies. One of the scenarios is when Windows Active Directory serves for user authentication and Linux servers or containers are used for running applications. Also, MS SQL Server is utilized for storing data. There are some ways to connect to MS SQL Server in that case. The first method is the traditional one to access to a SQL Server data with a SQL Server account. It is an easy way which does not request any special efforts. The second way is more complicated with using a kerberized Active Directory account and Windows trusted connection to SQL Server. We need to create a Kerberos keytab which contains Windows service account credentials and generate a Kerberos ticket based on the Kerberos keytab. This method might not work if we want to know who access our SQL Server data as each user is going to connect to SQL Server with the same Windows service account. Let's try to supply our applications with the current user Windows credentials for accessing of SQL Server data. Our issue is that Python data access libraries are fed with SQL Server accounts or access data with Windows trusted connection. Windows trusted connection works perfectly in Windows but Linux environment behaves differently. To achieve access to SQL Server with user Windows credentials, we follow the steps. Receive Windows user name and password. Create a Kerberos ticket. Access data with Windows trusted connection. You need to think how to pass user credentials in your application pipeline safely without compromising it. It might be many moving parts with possibility of revealing those credentials. The sample code is developed in CentOS 7 with Python 3 and pyodbc library. Prerequisites Install Kerberos client. sudo yum install krb5-workstation krb5-libs Install ODBC on non MS Windows platforms. sudo yum install unixODBC-devel Install Microsoft ODBC driver for SQL Server. The sample uses ODBC driver version 13 but the latest version is 17. sudo rpm -i https://packages.microsoft.com/rhel/7/prod/msodbcsql-13.1.9.2-1.x86_64.rpm If you decide for the latest ODBC driver, get one from Download ODBC Driver for SQL Server . Install Python pyodbc library pip install pyodbc Sample import pyodbc import subprocess import getpass def create_kerberos_ticket ( user_name , domain_name , user_password ): ssh = subprocess . Popen ([ \"kinit\" , f ' {user_name} @ {domain_name} ' ], stdin = subprocess . PIPE , stdout = subprocess . PIPE , stderr = subprocess . PIPE , universal_newlines = True , bufsize = 0 ) ssh . stdin . write ( f \" {user_password} \\n \" ) ssh . stdin . write ( \"exit \\n \" ) ssh . stdin . close () def sqlserver_connection (): return pyodbc . connect ( 'Driver={ODBC Driver 13 for SQL Server}' + \\ ';Server=' + 'sqlserver.sample.com' + \\ ';Database=' + 'sample_database' + \\ ';Trusted_Connection=yes' ) if __name__ == '__main__' : user_name = 'sampleuser' user_password = getpass . getpass ( f 'Enter {user_name} Windows password: ' ) domain_name = 'SAMPLE.COM' create_kerberos_ticket ( user_name , domain_name , user_password ) cur = sqlserver_connection () . cursor () sql_text = 'SELECT @@VERSION' cur . execute ( sql_text ) row = cur . fetchone () print ( row ) Troubleshooting Error message pyodbc.Error: ('HY000', '[HY000] [unixODBC][Microsoft][ODBC Driver 13 for SQL Server]SSPI Provider: No Kerberos credentials available (default cache: FILE:/tmp/krb5cc_1000) (851968) (SQLDriverConnect)') One of the reasons is incorrect password. In that case, a Kerberos ticket is not created. Check Kerberos ticket(s) created by an application. Open terminal and run commands. klist Or valid ticket(s). klist -s Missing Kerberos ticket cache file variable. It might be requested to create KRB5CCNAME variable with location and name of Kerberos ticket cache file. See more details in Create Ticket Cache File for Kerberos Authentication in Linux article. Missing krb5.conf Kerberos configuration file. The file contains default realm and Kerberos ticket settings. See more details in Create Ticket Cache File for Kerberos Authentication in Linux article. Resources pyodbc kinit - MIT Kerberos Documentation klist - MIT Kerberos Documentation How to Execute Shell Commands with Python Microsoft ODBC Driver for SQL Server Create Ticket Cache File for Kerberos Authentication in Linux","tags":"Python","url":"https://techjogging.com/access-sqlserver-windows-trusted-connection-linux-python.html","loc":"https://techjogging.com/access-sqlserver-windows-trusted-connection-linux-python.html"},{"title":"Troubleshooting Access to HTTP/HTTPS Resources in Docker","text":"As containerization is trending now, you might encounter some issues with setting up software to access HTTP/HTTPS resource in Docker. For example, if you are installing a Hive Standalone Metastore for Trino in Docker, Hive metasrore might need access to MinIO storage and PostgreSQL database. The access to MinIO can be done with HTTP or secured HTTP protocols. The same technic is applicable to other software used in Docker. 1. Make sure that container is running docker container ls 2. Connect to container If a container is running already docker exec -it [ container name ] bash if a container is not running yet docker run -it [ container name ] bash 3. Access to host from Docker ping sample.org Output PING sample.org (64.99.80.121) 56(84) bytes of data. 64 bytes from realnames.com (64.99.80.121): icmp_seq=1 ttl=243 time=71.4 ms 64 bytes from realnames.com (64.99.80.121): icmp_seq=2 ttl=243 time=79.8 ms 64 bytes from realnames.com (64.99.80.121): icmp_seq=3 ttl=243 time=61.7 ms 4. Run diagnostic in Docker Issue to connect to the resource curl -v sample.org:883 Or curl --verbose sample.org:883 Output * About to connect() to sample.org port 883 (#0) * Trying 64.99.80.121... * Connection refused * Failed connect to sample.org:883; Connection refused * Closing connection 0 curl: (7) Failed connect to sample.org:883; Connection refused Connection is successful for HTTP curl -v sample.org *Default port is 80. Output * About to connect() to sample.org port 80 (#0) * Trying 64.99.80.121... * Connected to sample.org (64.99.80.121) port 80 (#0) > GET / HTTP/1.1 > User-Agent: curl/7.29.0 > Host: sample.org > Accept: */* > < HTTP/1.1 200 OK < Server: nginx/1.6.2 < Date: Wed, 28 Jul 2021 02:24:48 GMT < Content-Type: text/html; charset=utf-8 < Transfer-Encoding: chunked < Connection: keep-alive < Vary: Accept-Encoding < X-Frame-Options: SAMEORIGIN < X-XSS-Protection: 1; mode=block < X-Content-Type-Options: nosniff < ETag: W/\"3fa3474c0ba2674deb1c00e2999534d1\" < Cache-Control: max-age=0, private, must-revalidate < X-Request-Id: 51daaf75baa3f45054c5516923b8e12c < X-Runtime: 0.019993 < P3P: CP=\"IDC DSP COR ADM DEVi TAIi PSA PSD IVAi IVDi CONi HIS OUR IND CNT\" < <!DOCTYPE html> <html> <head> ... </body> </html> * Connection #0 to host sample.org left intact Connection is successful for HTTPS curl -v https://s3.amazonaws.com Output * About to connect() to s3.amazonaws.com port 443 (#0) * Trying 52.217.38.94... * Connected to s3.amazonaws.com (52.217.38.94) port 443 (#0) * Initializing NSS with certpath: sql:/etc/pki/nssdb * CAfile: /etc/pki/tls/certs/ca-bundle.crt CApath: none * SSL connection using TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 * Server certificate: * subject: CN=s3.amazonaws.com,O=\"Amazon.com, Inc.\",L=Seattle,ST=Washington,C=US * start date: Jun 23 00:00:00 2021 GMT * expire date: Jul 24 23:59:59 2022 GMT * common name: s3.amazonaws.com * issuer: CN=DigiCert Baltimore CA-2 G2,OU=www.digicert.com,O=DigiCert Inc,C=US > GET / HTTP/1.1 > User-Agent: curl/7.29.0 > Host: s3.amazonaws.com > Accept: */* > < HTTP/1.1 307 Temporary Redirect < x-amz-id-2: fRm7Lvqpofrkn0IElv2QHsE+4J34eVigkg13gSB9oiBUbVwVWm9APdQ5pudCaAPn/DbK+xXH9/s= < x-amz-request-id: B3340YVNDXF5C17X < Date: Wed, 28 Jul 2021 02:32:41 GMT < Location: https://aws.amazon.com/s3/ < Server: AmazonS3 < Content-Length: 0 < * Connection #0 to host s3.amazonaws.com left intact Connection is successful in case of not entering requested login credentials curl -v https://storage.googleapis.com Output * About to connect() to storage.googleapis.com port 443 (#0) * Trying 172.217.164.208... * Connected to storage.googleapis.com (172.217.164.208) port 443 (#0) * Initializing NSS with certpath: sql:/etc/pki/nssdb * CAfile: /etc/pki/tls/certs/ca-bundle.crt CApath: none * SSL connection using TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 * Server certificate: * subject: CN=*.storage.googleapis.com,O=Google LLC,L=Mountain View,ST=California,C=US * start date: Jun 28 03:04:25 2021 GMT * expire date: Sep 20 03:04:24 2021 GMT * common name: *.storage.googleapis.com * issuer: CN=GTS CA 1O1,O=Google Trust Services,C=US > GET / HTTP/1.1 > User-Agent: curl/7.29.0 > Host: storage.googleapis.com > Accept: */* > < HTTP/1.1 400 Bad Request < X-GUploader-UploadID: ADPycdvqEm57jFatvBvI0Up9BvFXb0C9rZkiTP4NAX4izSnzcu-Van59elCfXihl9_BxQ-9RS9Iic4hW6-AOUzxtV5gHCkNgtA < Content-Type: application/xml; charset=UTF-8 < Content-Length: 181 < Date: Wed, 28 Jul 2021 02:38:17 GMT < Expires: Wed, 28 Jul 2021 02:38:17 GMT < Cache-Control: private, max-age=0 < Server: UploadServer < Alt-Svc: h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000,h3-T051=\":443\"; ma=2592000,h3-Q050=\":443\"; ma=2592000,h3-Q046=\":443\"; ma=2592000,h3-Q043=\":443\"; ma=2592000,quic=\":443\"; ma=2592000; v=\"46,43\" < * Connection #0 to host storage.googleapis.com left intact <?xml version='1.0' encoding='UTF-8'?><Error><Code>MissingSecurityHeader</Code><Message>Your request was missing a required header.</Message><Details>Authorization</Details></Error> Alternative diagnostic tool wget sample.org Resources curl.1 the man page Docker documentation","tags":"Docker","url":"https://techjogging.com/troubleshooting-access-https-resources-docker.html","loc":"https://techjogging.com/troubleshooting-access-https-resources-docker.html"},{"title":"Apply Trino (Presto) Configuration Properties in DBeaver Connection","text":"Trino configuration properties are used to tune Trino or change its behavior when required. Mostly times, those properties are applied in Trino configuration files. It is required a restarting of a Trino cluster to make it in effect in case if customization is needed. There is another flexible option without restarting a Trino cluster and applying those changes on-fly in a Trino DBeaver connection when a query is submitted for an execution. It can be done as a part of the Trino jdbc driver setup in DBeaver. The article uses Starburts open-source distribution with DBeaver 21.1.0 CE installed in Windows. 1. Open Trino connection in DBeaver 2. Add sessionProperties jdbc driver parameter 3. Apply Trino properties One property Multiple properties separated by semicolon (;) Resources Trino JDBC driver Trino Properties reference","tags":"Trino(Presto)","url":"https://techjogging.com/apply-trino-configuration-properties-dbeaver-connection.html","loc":"https://techjogging.com/apply-trino-configuration-properties-dbeaver-connection.html"},{"title":"How to Use DBeaver CE with Git","text":"In spite of some limitations in DBeaver Community Edition (CE) to work with Git, it is not an obstacle in making smooth integration of DBeaver with Git. The issue is DBeaver perspective which does not allow completely viewing of the current status of a project and project files under Git source control. One of the solutions is to utilize Resource perspective. The perspective is fully integrated with Git. The combination of both DBeaver and Resource perspectives helps make DBeaver CE as a good tool. The article uses DBeaver 21.1.0 CE installed in Windows with GitHub. Those articles can help get started with Git describing installation of Git extension and adding GitHub repository to DBeaver. Install DBeaver CE Git Extension Add GitHub Repository to DBeaver CE Secured with SSH Protocol 1. DBeaver does not show all information for GitHub project Current branch name. Status of modified script.sql file and the project. 2. Open Perspective list 3. Select Resource perspective 4. Project Explorer of Resource perspective 5. Use Git menu to manage GitHub project","tags":"DBeaver","url":"https://techjogging.com/how-use-dbeaver-ce-git.html","loc":"https://techjogging.com/how-use-dbeaver-ce-git.html"},{"title":"Install DBeaver CE Git Extension","text":"Git version control can be used in DBeaver Community Edition (CE). As DBeaver is Eclipse based Integrated Development Environment (IDE), Eclipse extensions are installed on it. Eclipse Marketplace is a repository of different kinds of extensions. Installation of Git extension follows the same rules as in Eclipse IDE. DBeaver CE Git extension was developed specially for DBeaver but it utilizes Eclipse EGit. The article uses DBeaver 21.1.0 CE installed in Windows. 1. Go to DBeaver Git extension Eclipse Marketplace https://marketplace.eclipse.org/content/dbeaver-git 2. Click Update site url button and copy DBeaver Git extension URL 3. Open DBeaver Help menu and select Install New Software 4. Paste DBeaver Git extension URL to Work with bar and check DBeaver Git Support items 5. Click Next to follow installation wizard Wait for installing software and answer on questions. 6. Restart DBeaver on the final step 7. Validate that Git extension has been installed clicking Open Perspective button","tags":"DBeaver","url":"https://techjogging.com/install-dbeaver-ce-git-extension.html","loc":"https://techjogging.com/install-dbeaver-ce-git-extension.html"},{"title":"Add GitHub Repository to DBeaver CE Secured with SSH Protocol","text":"GitHub is a good addition to DBeaver Community Edition (CE). It allows to store scripts and other files in source control. Even with some limitations in DBeaver CE, GitHub full functionality can be utilized. There are varieties of ways connecting GitHub to DBeaver CE. SSH protocol provides with enhanced security and seamlessly integrated with DBeaver CE. In some cases, it can work when HTTPS one might be problematic. The article uses DBeaver 21.1.0 CE installed in Windows. 1. Prerequisite. Create SSH key Follow Create SSH Key in DBeaver CE article. 2. Prerequisite. Install DBeaver Git extension Follow Install DBeaver CE Git Extension article. 3. Open File menu and select Import menu item 4. Expand Git node and select any item, for example, Projects from Git 5. Select Clone URI 6. Go to your repository in GitHub and copy SSH URL 7. Return back to DBeaver and click Next 8. Paste SSH URL to URI field. The rest of fields will be filled out automatically Leave Password field empty. 9. Accept and store key 10. Confirm creating of know_hosts file 11. Key in passphrase 12. Select branch to import 13. Pick up a destination directory or accept default one 14. Select a wizard for importing your project A general project should work for many cases. 15. Final confirmation 16. Switch to Projects tab to see the imported project","tags":"DBeaver","url":"https://techjogging.com/add-github-repository-dbeaver-ce-ssh-protocol.html","loc":"https://techjogging.com/add-github-repository-dbeaver-ce-ssh-protocol.html"},{"title":"Create SSH Key in DBeaver CE","text":"SSH keys are widely used to secure access to sensitive information. They provide both relatively easy implementation and better security than passwords. Generating of SSH keys can be done with different tools. In some cases, the process is complicated and requests manual interaction. DBeaver Community Edition (CE) utilizes SSH keys, for example, access GitHub repositories. As DBeaver is Eclipse based Integrated Development Environment (IDE), it includes functionality to generate a pair of SSH keys and set it up for usage. It is a transparent and trivial process. The article uses DBeaver 21.1.0 CE installed in Windows. 1. Go to Window menu 2. Open Preferences menu item and find SSH2 3. Switch to Key Management tab and click Generate RSA Key 4. Enter Passphrase and Confirm passphrase 5. Click Save Private Key button 6. Store generated public key for further distribution, for example, create a file with the key. 7. Click Close and Apply to finish it","tags":"DBeaver","url":"https://techjogging.com/create-ssh-key-dbeaver-ce.html","loc":"https://techjogging.com/create-ssh-key-dbeaver-ce.html"},{"title":"Access MinIO Secured by SSL/TLS with AWS SDK for Python (Boto3)","text":"AWS SDK for Python can be used with many AWS services including Amazon Simple Service (Amazon S3). Also, the SDK is capable to access S3 compatible storages such as MinIO . While MinIO Python SDK is a native library for MinIO access, there are some cases when AWS SDK for Python can be used as alternative. If MinIO access is secured by SSL/TLS protocol, SSL certificate is requested or insecure connection can be made as a workaround. SSL certificate can be received from your company infrastructure team or it might be extracted from internet browser as per Export TLS/SSL Server Certificate from Internet Browser article. Discover Access MinIO Secured by SSL/TLS with MinIO Python SDK article if you decide to access to MinIO with MinIO Python SDK . The samples are developed with Python 3.6.4 and AWS SDK for Python (Boto3) 1.17.27 in Windows 10. Those samples show lists of objects in buckets in a MinIO cluster. Secure connection import boto3 from botocore.exceptions import ClientError if __name__ == '__main__' : certificate_path = 'sample.pem' clientArgs = { 'aws_access_key_id' : '<access key>' , 'aws_secret_access_key' : 'secret key' , 'endpoint_url' : 'https://minio.sample.com:9000' , 'verify' : certificate_path } client = boto3 . resource ( \"s3\" , ** clientArgs ) try : print ( 'Retrieving buckets...' ) print () for bucket in client . buckets . all (): bucket_name = bucket . name print ( 'Bucket name: {} ' . format ( bucket_name )) objects = client . Bucket ( bucket_name ) . objects . all () for obj in objects : object_name = obj . key print ( 'Object name: {} ' . format ( object_name )) print () except ClientError as err : print ( \"Error: {} \" . format ( err )) Insecure connection import boto3 from botocore.exceptions import ClientError import urllib3 urllib3 . disable_warnings () if __name__ == '__main__' : clientArgs = { 'aws_access_key_id' : '<access key>' , 'aws_secret_access_key' : 'secret key' , 'endpoint_url' : 'https://minio.sample.com:9000' , 'verify' : False } client = boto3 . resource ( \"s3\" , ** clientArgs ) try : print ( 'Retrieving buckets...' ) print () for bucket in client . buckets . all (): bucket_name = bucket . name print ( 'Bucket name: {} ' . format ( bucket_name )) objects = client . Bucket ( bucket_name ) . objects . all () for obj in objects : object_name = obj . key print ( 'Object name: {} ' . format ( object_name )) print () except ClientError as err : print ( \"Error: {} \" . format ( err )) urllib3 library is aimed to eliminate C:\\Program Files\\Python36\\lib\\site-packages\\urllib3\\connectionpool.py:1020: InsecureRequestWarning: Unverified HTTPS request is being made to host 'minio.sample.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings InsecureRequestWarning message. Resources Boto3 documentation Boto3 - The AWS SDK for Python GitHub repository","tags":"MinIO","url":"https://techjogging.com/access-minio-secured-ssl-aws-python-sdk.html","loc":"https://techjogging.com/access-minio-secured-ssl-aws-python-sdk.html"},{"title":"View Yoosee SD-M5 Doorbell Stream in VLC Media Player","text":"Yoosee SD-M5 doorbell camera is generic one. It supports Onvif 2.5 Profile S and Real Time Streaming Protocol (RTSP). The doorbell exposes 2 RTSP stream profiles. The first one is high resolution with rtsp://<IP Address>:554/onvif1 address and the resolution is 1920x1080 5-15 fps. The second one is low resolution with rtsp://<IP Address>:554/onvif2 address and the resolution is 320x240 5-15 fps. Prerequisite Enable NVR connection and set up the NVR/RTSP password. Open Yoosee application settings. It is tested with version 00.46.00.69 of Yoosee application. Go to NVR connection Enable to connect. Change password. Write down the entered password as it will be used in VLC media player. Windows 10 VLC The sample has been created in Windows 10 with VLC version 3.0.12. Select Open Network Stream menu. Type in rtsp://admin:<NVR password>@192.168.0.102/onvif1 replacing < NVR password > with the password entered on the step 2 in Prerequisite and your doorbell local IP address. Linux VLC The sample has been created in CentOS Linux release 7.9.2009 (Core) with VLC version 3.0.11.1. Select Open Network Stream menu. Type in rtsp://admin:<NVR password>@192.168.0.102/onvif1 replacing < NVR password > with the password entered on the step 2 in Prerequisite and your doorbell local IP address. Android VLC The sample has been created in Android version 10 with VLC version 3.3.4. Click New stream button Type in rtsp://admin:<NVR password>@192.168.0.102/onvif1 replacing < NVR password > with the password entered on the step 2 in Prerequisite and your doorbell local IP address. Access to doorbell outside of your local network Open port 554 on your router Replace your local IP address with your public IP address.","tags":"Surveillance","url":"https://techjogging.com/view-yoosee-sd-m5-doorbell-stream-vlc-media-player.html","loc":"https://techjogging.com/view-yoosee-sd-m5-doorbell-stream-vlc-media-player.html"},{"title":"Access MinIO Secured by SSL/TLS with MinIO Python SDK","text":"Enterprise level products are secured with SSL/TLS protocol on top of HTTP. It enforces encrypted communications between a Web server and a client. To access those data, a client is supposed to obtain a SSL certificate. MinIO supports both secured and unsecured access to object storage. MinIO Python SDK has been developed as a native library to MinIO and it works with Amazon S3 compatible Cloud Storage as well. There are some modifications to access MinIO storage secured by HTTPS. Prerequisite for each method is a SSL certificate. It can received from your company infrastructure team or it might be extracted from internet browser as per Export TLS/SSL Server Certificate from Internet Browser article. The samples are developed with Python 3.6.4 and MinIO SDK 7.0.2 in Windows 10. They show a list of buckets in a MinIO cluster. Option 1 import os from minio import Minio if __name__ == '__main__' : os . environ [ 'SSL_CERT_FILE' ] = r 'C:\\temp\\sample.crt' client = Minio ( 'minio.sample.com:9000' , access_key = '<accress keyu>' , secret_key = 'secret key' , secure = True ) bucket_list = client . list_buckets () for bucket in bucket_list : print ( bucket . name ) Option 2 from minio import Minio import urllib3 from datetime import timedelta if __name__ == '__main__' : timeout = timedelta ( minutes = 5 ) . seconds http_client = urllib3 . PoolManager ( timeout = urllib3 . util . Timeout ( connect = timeout , read = timeout ), maxsize = 10 , cert_reqs = 'CERT_REQUIRED' , ca_certs = r 'C:\\temp\\sample.crt' , retries = urllib3 . Retry ( total = 5 , backoff_factor = 0.2 , status_forcelist = [ 500 , 502 , 503 , 504 ] ) ) client = Minio ( 'minio.sample.com:9000' , access_key = '<access key>' , secret_key = 'secret key' , secure = True , http_client = http_client ) bucket_list = client . list_buckets () for bucket in bucket_list : print ( bucket . name ) Option 3 Add SSL_CERT_FILE environment variable. Press Win+R combination, and then type in SystemPropertiesAdvanced to open System Properties Click Environment Variable It can be a User or System variable Restart your Integrated Development Environment (IDE) or command line window to pick up the new variable. If it does not work, restart your computer. from minio import Minio if __name__ == '__main__' : client = Minio ( 'minio.sample.com:9000' , access_key = '<secret key>' , secret_key = 'secret key' , secure = True ) bucket_list = client . list_buckets () for bucket in bucket_list : print ( bucket . name ) Resources MinIO Python SDK for Amazon S3 Compatible Cloud Storage documentation MinIO Python SDK GitHub repository","tags":"MinIO","url":"https://techjogging.com/access-minio-secured-ssl-minio-python-sdk.html","loc":"https://techjogging.com/access-minio-secured-ssl-minio-python-sdk.html"},{"title":"Built-in System Access Control to Catalogs in Presto","text":"Presto secures access to catalogs with built-in system. This article is not aimed to replace the Presto documentation which provides high level of overview with following details on the implementation of build-in system access control. As any documentation, it does not cover all topics and there are some missing pieces and features. The samples are based on Trino open-source distribution former known as PrestoSQL. The default global access for mostly types of catalogs is to create tables and insert data. It is applicable, for example, to MySql, PostgreSQL, SQL Server, and other connectors. PostgreSQL connector with default access. connector.name = postgresql connection-url = jdbc:postgresql://sample.com:5432/database_name connection-user = user_name connection-password = user_password Adding delete table permission is done with allow-drop-table property. connector.name = postgresql connection-url = jdbc:postgresql://sample.com:5432/database_name connection-user = user_name connection-password = user_password allow-drop-table = true If a connector is supposed to be read-only, a set of files are requested to implement it. The first file is access-control.properties and the location is /etc/presto folder. This file turns on the file-based system access control. The second file specifies access control rules and the file format is JSON. The file name and location are customizable. access-control.properties access-control.name = file security.config-file = /etc/presto/global_rules.json global_rules.json { \"catalogs\" : [ { \"catalog\" : \"(finance|warehouse|system)\" , \"allow\" : \"read-only\" }, { \"allow\" : \"all\" } ] } The read-only permission is applied only to finance, warehouse, and system catalogs. The rest of catalogs are not affected. The beneficial feature of built-in system access control is that a cluster restart is not required if settings are changed. Moreover, Presto can be instructed how often those security settings should be refreshed with security.refresh-period property in access-control.properties file. access-control.name = file security.config-file = /etc/presto/global_rules.json ecurity.refresh-period Some connectors have its own security settings, for example, Hive. hive.allow-add-column = true hive.allow-drop-column = true hive.allow-drop-table = true hive.allow-rename-column = true hive.allow-rename-table = true Resources Built-in system access control File based system access control","tags":"Trino(Presto)","url":"https://techjogging.com/builtin-system-access-control-catalogs-presto.html","loc":"https://techjogging.com/builtin-system-access-control-catalogs-presto.html"},{"title":"Connect Yoosee SD-M5 Doorbell to Synology DSM","text":"Yoosee SD-M5 doorbell camera is not included in the list of compatible cameras for Synology DSM. Hopefully, this camera exposes Real Time Streaming Protocol (RTSP) and supports Onvif 2.5 Profile S. It allows connect the doorbell camera to Synology DSM as an Onvif device. A feature of this camera is that it has 5000 fixed port. The sample has been tested with DSM 6.2.3-25426 Update 3 Synology DSM version and 13.1.1.36 Yoosee camera firmware with Android application. 1. Open Yoosee application settings 2. Go to NVR connection Enable to connect. Change password. Write down the entered password as it will be used in Synology DSM. 3. Add a new camera Select Complete Setup. 4. Enter connection information Camera name. Camera IP address. Port: 5000. Username: admin. Password: NVR password changed on step 2. Click Test Connection to validate the information. 5. Click Load Capacity 6. Change Stream 1 setting Resolution: 1920x1080. Frame rate (FPS): 25. 7. Leave Recording settings as is It can be adjusted later on as per your setup. 8. Leave Schedule setting as is It can be adjusted later on as per your setup. 9. Final screen 10. Activating process 11. Camera ready Troubleshooting In case of receiving disconnected status, try to replace admin password entered on step 4 with NVR one.","tags":"Surveillance","url":"https://techjogging.com/connect-yoosee-sd-m5-doorbell-synology-dsm.html","loc":"https://techjogging.com/connect-yoosee-sd-m5-doorbell-synology-dsm.html"},{"title":"Obtain Identity Value for SQL Server Insert in Python","text":"Extracting of an identity value when data inserted into a SQL Server table in Python is done in the same way as in Microsoft Transact-SQL. The difference is that SQL Server statements are wrapped up in Python commands. pypyodbc is a good candidate to communicate with SQL Server in Python. There are 2 major methods in Transact-SQL to accomplish it. The first method is based on OUTPUT clause in INSERT statements. It is the most reliable way and it works in varieties of scenarios. The second method is simple and faster one but it might not work properly in triggers and stored procedures. It retrieves a value from @@IDENTITY or other variables. The samples are run with Python 3.6, pypyodbc 1.3.4, and SQL Server 2016. Prerequisites Create the test table in SQL Server CREATE TABLE [ dbo ].[ book ]( [ title ] [ varchar ]( 100 ) NOT NULL , [ author ] [ varchar ]( 50 ) NOT NULL , [ book_id ] [ int ] IDENTITY ( 1 , 1 ) NOT NULL ); Option #1 import pypyodbc connection_string = 'Driver={{SQL Server Native Client 11.0}};Server= {server_name} ;Database= {database_name} ;Trusted_Connection=yes;' . format ( server_name = 'SAMPLE' , database_name = 'BookStore' ) with pypyodbc . connect ( connection_string ) as con : cur = con . cursor () sql_text = 'SET NOCOUNT ON;' + \\ 'DECLARE @table_identity TABLE(book_id int);' + \\ 'INSERT INTO {table_name} ' . format ( table_name = 'book' ) + \\ '(title, author) ' + \\ 'OUTPUT inserted.book_id INTO @table_identity(book_id) ' + \\ 'VALUES(?,?);' + \\ 'SELECT book_id FROM @table_identity;' cur . execute ( sql_text , ( 'Don Quixote' , 'Miguel de Cervantes' )) book_id = cur . fetchone ()[ 0 ] print ( book_id ) Option #2 import pypyodbc connection_string = 'Driver={{SQL Server Native Client 11.0}};Server= {server_name} ;Database= {database_name} ;Trusted_Connection=yes;' . format ( server_name = 'SAMPLE' , database_name = 'BookStore' ) with pypyodbc . connect ( connection_string ) as con : cur = con . cursor () sql_text = 'SET NOCOUNT ON;' + \\ 'INSERT INTO {table_name} ' . format ( table_name = 'book' ) + \\ '(title, author) ' + \\ 'VALUES(?,?);' + \\ 'SELECT @@IDENTITY AS book_id;' cur . execute ( sql_text , ( 'War and Peace' , 'Leo Tolstoy' )) book_id = cur . fetchone ()[ 0 ] print ( book_id ) @@IDENTITY variable can be replaced with @@SCOPE_IDENTITY and @@IDENT_CURRENT('table_or_view_name') . It depends on a case. The description of each variable is. @@IDENTITY : Returns the last identity value inserted into an identity column in the same session. @@SCOPE_IDENTITY : Returns the last identity value inserted into an identity column in the same session and scope. A scope is a module: a stored procedure, trigger, function, or batch. @@IDENT_CURRENT('table_or_view_name') : Returns the last identity value generated for a specified table or view. The last identity value generated can be for any session and any scope. A Python connection is equal to a SQL Server session. Resources Transact-SQL Reference","tags":"Python","url":"https://techjogging.com/obtain-identity-value-sqlserver-insert-python.html","loc":"https://techjogging.com/obtain-identity-value-sqlserver-insert-python.html"},{"title":"Export TLS/SSL Server Certificate from Internet Browser","text":"To securely browse the Web, HTTPS protocol is established. The secure protocol requests public certificate which is freely transmitted from a Web site to a client. The public certificate can be exported from Internet browser in Privacy-Enhanced Mail (PEM) format. If a certificate is exported from PKCS #12 or other formats, it includes Bag Attributes. Bag Attributes can be recreated from an Internet browser certificate information as well and they might be requested in some tools or software libraries to access a Web site in safely manner. A sample of a PEM certificate. -----BEGIN CERTIFICATE----- MIIFOTCCBCGgAwIBAgISBIAlOcXvyAyEBqj0ULzsgUPOMA0GCSqGSIb3DQEBCwUA MDIxCzAJBgNVBAYTAlVTMRYwFAYDVQQKEw1MZXQncyBFbmNyeXB0MQswCQYDVQQD EwJSMzAeFw0yMDEyMjYwMjUwNDFaFw0yMTAzMjYwMjUwNDFaMBoxGDAWBgNVBAMT D3RlY2hqb2dnaW5nLmNvbTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEB AMxMmigFj121UY1CWskO/3BB/xYaGYKWVjdcBeyRGTacnq7pXxngC8O3Q7hCIhFF zyWFQH+XWTobFtbkMi1xW+igHXZ4bPkI84PsfxjsXH8jTam2n7naJ1NW2XBuw4Bj WTvSdPti5LJsK7sbb0VpHbsH2q+WhPtjxxep0e+ZZCWl6BGK1/wRH588Y2ECH1RQ uLPgyTsuXcqlm3Wygfr7ikyfMu00aVuhPXlkob65VknYR+AvktYe4UEr0+QazGP4 9J9gvqYBLK2SrgA8FlWjnarE0/ouyPdzeSwA9iyhNKiffAN2HkLig+Sy8/K6L2zh QZcGr4Z7XUKBmBWbqXHyY7MCAwEAAaOCAl8wggJbMA4GA1UdDwEB/wQEAwIFoDAd BgNVHSUEFjAUBggrBgEFBQcDAQYIKwYBBQUHAwIwDAYDVR0TAQH/BAIwADAdBgNV HQ4EFgQUT5d3Rw4gZGWvSw5zb21jRv40gSkwHwYDVR0jBBgwFoAUFC6zF7dYVsuu UAlA5h+vnYsUwsYwVQYIKwYBBQUHAQEESTBHMCEGCCsGAQUFBzABhhVodHRwOi8v cjMuby5sZW5jci5vcmcwIgYIKwYBBQUHMAKGFmh0dHA6Ly9yMy5pLmxlbmNyLm9y Zy8wLwYDVR0RBCgwJoIPdGVjaGpvZ2dpbmcuY29tghN3d3cudGVjaGpvZ2dpbmcu Y29tMEwGA1UdIARFMEMwCAYGZ4EMAQIBMDcGCysGAQQBgt8TAQEBMCgwJgYIKwYB BQUHAgEWGmh0dHA6Ly9jcHMubGV0c2VuY3J5cHQub3JnMIIBBAYKKwYBBAHWeQIE AgSB9QSB8gDwAHYAXNxDkv7mq0VEsV6a1FbmEDf71fpH3KFzlLJe5vbHDsoAAAF2 nSt67wAABAMARzBFAiEAi+1Q1m/Oxyp5cB2POMz9utvZGq9KSg4/OarZA6vtM1kC IGdvKThenxh7v2LRfgmlUcWcxRq16q4ZB78OSF8WSbz2AHYAfT7y+I//iFVoJMLA yp5SiXkrxQ54CX8uapdomX4i8NcAAAF2nSt7CwAABAMARzBFAiBbaSk5tCZgx7bC UIXJVayOIA9Nsif654YSGtWa+DdwoQIhAPo3YId+rVlUHgf6/ucJsZd45Hj3mxb1 DQOVHM0hdIT+MA0GCSqGSIb3DQEBCwUAA4IBAQB5XTUi0NQhPPBXdakL/YPoH/50 BmC6iadIx1g6bqW+QcPWmqOqLm1ZyCgYpliJdCX9Q6uHXVjDJNep4w31+v5Sh3YQ hiwAzZk4m5hJ/ZbWOHatgqew7jApi9cfJ++lHXPC8C0oY4bNOvbU9x5ygSbpzsCn nqQH74SSvn49UtZZ/a2gSt+854daHPRqVHwnV7FhTttpeuSyEmkc3ocpZ2uxTgFC CAOcGrAzYIwfxzsTv7l3+L+a921vjei6KpLLw0z+DvzWAeMNRbEUL2IqD7EN+5Zm SNEXHPbcvGKXBueNoAzCQFmHoh9wUy2mPN2/wmHxfGgt+U6ykDGU5FIiKVtO -----END CERTIFICATE----- A sample of a PEM certificate with Bag Attributes. Bag Attributes friendlyName: techjogging.com localKeyID: 53 64 6A 61 20 33 36 30 39 36 35 38 38 33 31 31 31 32 subject=/CN=techjogging.com issuer=/C=US/O=Let's Encrypt/CN=R3 -----BEGIN CERTIFICATE----- MIIFOTCCBCGgAwIBAgISBIAlOcXvyAyEBqj0ULzsgUPOMA0GCSqGSIb3DQEBCwUA ... SNEXHPbcvGKXBueNoAzCQFmHoh9wUy2mPN2/wmHxfGgt+U6ykDGU5FIiKVtO -----END CERTIFICATE----- FireFox browser Click the lock (padlock) icon in the address bar. Click Show connection details button. Select More Information menu item. Click View Certificate on Security tab. Finally, click PEM (cert) link in Miscellaneous section. Google Chrome browser Click the lock (padlock) icon in the address bar. Select Certificate menu item. Click Copy to file... on Details tab. Follow Certificate Export Wizard steps. Select Base-64 encoded X.509 (.CER) format. Microsoft Edge browser Export steps are the same as for Google Chrome browser. Microsoft Internet Explorer browser Export steps are the same as for Google Chrome browser. The padlock icon is located on the right side of the address bar.","tags":"Security","url":"https://techjogging.com/export-tls-ssl-server-certificate-from-internet-browser.html","loc":"https://techjogging.com/export-tls-ssl-server-certificate-from-internet-browser.html"},{"title":"Custom Built Surveillance System","text":"Success of a custom built surveillance system mostly depends on initial stage when you design it. A diagram of your surveillance system would be the starting point of your design. it helps outline the future functionality and performance of the system. When a list of the surveillance system components is defined in a drawing, it is easy to work on each part of the design. The diagram represents POE IP camera custom built surveillance system. It can be used as a boilerplate for designing of any surveillance system. Your cameras are driving the design of the entire system. IP cameras are chosen as they have better quality comparing with analog ones. Also, those cameras are generic ones and they can be plugged in different Network Video Recorders (NVR). Moreover, cameras are wired with Power Over Ethernet (POE). It helps reduce number of wires coming to cameras combining connectivity and power cables together. Wired cameras are more reliable in terms of connection and volume of transmitted data than WiFi ones. Getting higher resolution data stream is not a problem for wired cameras whereas WiFi cameras can reduce video quality because of weak signal. TrenNet cameras are generic commercial grade ones. Network Video Recorder (NVR) is another critical component. Cameras and NVR have to compatible with each other. NVR collects video from cameras, converts video into compressed format and stores digital data. It is better to set up one device which serves as NVR and storage, for example, Synology NAS or QNAP NAS . They include requested hardware along with Surveillance Station software. POE switch connects cameras to a home network and supplies power to cameras. Also, home users can have wired connection to home network as an extra feature. TrendNet POE switch can do good job. WiFi router manages home network, provides access of wired and WiFi users to cameras, and exposes cameras to outside world. The last important component is Uninterruptible Power Supply (UPS). Power outages can break down your surveillance system.","tags":"Surveillance","url":"https://techjogging.com/custom-built-surveillance-system.html","loc":"https://techjogging.com/custom-built-surveillance-system.html"},{"title":"Python Access to Presto Cluster with Presto Client","text":"Presto access is represented by many Python libraries among those are Dropbox/PyHive , prestosql/presto-python-client , prestodb/presto-python-client , and easydatawarehousing/prestoclient . Mostly of libraries use Python DB-API interface to access Presto which uniforms commands. Python Access to Presto Cluster with PyHive article describes Dropbox/PyHive library usage. prestosql/presto-python-client library is actively supported by Presto developers. The sample is run with Python 3 in Windows. 1. Install Presto client library Linux. sudo pip3 install presto-client Windows. pip install presto-client 2. Include requested libraries import presto 3. Establish connection Access to Presto cluster without password. conn = presto . dbapi . connect ( host = 'localhost' , port = 8080 , catalog = 'system' , schema = 'runtime' ) Presto cluster is secured by password but skip SSL verification. This case might be used during development stage. conn = presto . dbapi . connect ( host = 'localhost' , port = 443 , http_scheme = 'https' , catalog = 'system' , schema = 'runtime' , auth = presto . auth . BasicAuthentication ( '<user name>' , '<password>' ), verify = False ) Presto cluster is secured by password. Option #1. Follow instructions in Convert Java Keystore to PEM File Format article to create presto.crt file. The file contains Presto SSL public certificate converted from Java keystore file. Option #2. Extract presto.crt certificate from Internet Browser. Follow Export TLS/SSL Server Certificate from Internet Browser article. conn = presto . dbapi . connect ( host = 'localhost' , port = 443 , http_scheme = 'https' , catalog = 'system' , schema = 'runtime' , auth = presto . auth . BasicAuthentication ( '<user name>' , '<password>' ), verify = 'presto.crt' ) 4. Create cursor cur = conn . cursor () 5. Retrieve data cur . execute ( 'SELECT * FROM nodes' ) for row in cur . fetchall (): print ( row ) 6. Improvements To disable insecure warnings during https requests if verify=False , add the code in import section. import urllib3 urllib3 . disable_warnings () 7. Troubleshooting In case of getting ssl.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:777) error, check your certificate expiration date. The date has to be valid. Resources SSL Cert Verification","tags":"Python","url":"https://techjogging.com/python-access-presto-cluster-presto-client.html","loc":"https://techjogging.com/python-access-presto-cluster-presto-client.html"},{"title":"Convert Java Keystore to PEM File Format","text":"Java keystore (JKS) file includes public certificates and cryptography keys. It is secured by a password and used in java applications. Other consumers of public certificates and cryptography keys, for example, tools or software libraries might not accept JKS format. In that case, Java keystore file can be converted into different formats. PEM is widely used format which can contain certificates and private keys as well. Conversion between those formats is done with multistep process. keytool is one tool to convert formats. The tool is a part of JDK or JRE. The second tool is openssl . List certificates in a source JKS repository keytool -list -v -keystore source_keystore.jks Alias identifies certificates and keys. Convert JKS format into intermediate PKCS #12 one. keytool -importkeystore -alias < alias from previous step> -srckeystore source_keystore.jks -destkeystore intermediate.p12 -srcstoretype jks -deststoretype pkcs12 -srcstorepass < source keystore password> -deststorepass 123456 -destkeypass 123456 alias parameter can be omitted if there is only 1 entry. deststorepass and destkeypass can be have any values but they have to match to password value on the next step. Output Importing keystore source_keystore.jks to intermediate.p12... Create final not encrypted PEM file. Option #1. One file containing both a certificate and a cryptography key. openssl pkcs12 -in intermediate.p12 -nodes -out output.pem -password pass:123456 Output MAC verified OK Option #2. Two files with a certificate and a cryptography key. Certificate openssl pkcs12 -in intermediate.p12 -nokeys -out output.crt -password pass:123456 Key openssl pkcs12 -in coordinator_keystore.p12 -nocerts -nodes -out output.key -password pass:123456 Output MAC verified OK","tags":"Security","url":"https://techjogging.com/convert-java-keystore-pem-file-format.html","loc":"https://techjogging.com/convert-java-keystore-pem-file-format.html"},{"title":"Python Access to Presto Cluster with PyHive","text":"Presto access is represented by many Python libraries among those are Dropbox/PyHive , prestosql/presto-python-client , prestodb/presto-python-client , and easydatawarehousing/prestoclient . Mostly of libraries use Python DB-API interface to access Presto which uniforms commands. Python Access to Presto Cluster with Presto Client article describes PrestoSQL client library usage. Dropbox/PyHive library is universal one as it can be used to access Hive or Presto. The sample is run with Python 3 in Windows. 1. Install PyHive library Linux. sudo pip3 install 'pyhive[presto]' Windows. Run as administrator. pip install 'pyhive[presto]' It installs only Presto interface. 2. Include requested libraries Access to Presto cluster without password. from pyhive import presto Presto cluster is secured by password. from pyhive import presto from requests.auth import HTTPBasicAuth 3. Establish connection Access to Presto cluster without password. conn = presto . connect ( host = 'localhost' , port = 8080 , catalog = 'system' , schema = 'runtime' ) Presto cluster is secured by password but skip SSL verification. This case might be used during development stage. conn = presto . connect ( host = 'localhost' , port = 443 , protocol = 'https' , catalog = 'system' , schema = 'runtime' , requests_kwargs = { 'auth' : HTTPBasicAuth ( '<user name>' , '<password>' ), 'verify' : False }) Presto cluster is secured by password. Option #1. Follow instructions in Convert Java Keystore to PEM File Format article to create presto.crt file. The file contains Presto SSL public certificate converted from Java keystore file. Option #2. Extract presto.crt certificate from Internet Browser. Follow Export TLS/SSL Server Certificate from Internet Browser article. conn = presto . connect ( host = 'localhost' , port = 443 , protocol = 'https' , catalog = 'system' , schema = 'runtime' , requests_kwargs = { 'auth' : HTTPBasicAuth ( '<user name>' , '<password>' ), 'verify' : 'presto.crt' }) 4. Create cursor cur = conn . cursor () 5. Retrieve data cur . execute ( 'SELECT * FROM nodes' ) for row in cur . fetchall (): print ( row ) 6. Improvements To disable insecure warnings during https requests if verify=False , add the code in import section. import urllib3 urllib3 . disable_warnings () 7. Troubleshooting In case of getting ssl.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:777) error, check your certificate expiration date. The date has to be valid. Resources SSL Cert Verification","tags":"Python","url":"https://techjogging.com/python-access-presto-cluster.html","loc":"https://techjogging.com/python-access-presto-cluster.html"},{"title":"Access MS SQL Server in Presto with Kerberos Keytab Authentication","text":"Presto SQL Server connector accesses SQL Server databases using SQL Server credentials. The connector properties contain SQL Server user name and password. This is only an option supported by Presto. Because of using SQL Server jdbc driver in SQL Server connector, it opens more ways to access MS SQL Server. Kerberos keytab is a part of SQL Server jdbc driver setup with Java Authentication and Authorization Service (JAAS). The sample is based on Starburst 339-e open source distribution with RPM installation and RHEL 7 Linux distribution. 1. Download SQL Server jdbc driver The download page is here . Pick up either zip or tar.gz package. Extract mssql-jdbc-8.4.1.jre8.jar, mssql-jdbc-8.4.1.jre11.jar, and mssql-jdbc-8.4.1.jre14.jar files. Copy a file corresponding your java version to /etc/presto folder on a coordinator and workers, for example, if Presto cluster is run in java 11, take mssql-jdbc-8.4.1.jre11.jar file. 2. Generate keytab file See Create keytab File for Kerberos Authentication in Windows article. The file location is /etc/presto folder on a coordinator and workers, for example, /etc/presto/sqlserver.keytab . 3. Create jaas file The file has to be deployed to a coordinator and workers in /etc/presto folder. the name might be conf.jaas . SQLJDBCDriver { com.sun.security.auth.module.Krb5LoginModule required debug = true doNotPrompt = true useKeyTab = true keyTab = \"/etc/presto/sqlserver.keytab\" useTicketCache = false renewTGT = false principal = \"mywindowsname@SAMPLE.COM\"; }; After successful completion of the setup, remove debug=true line. 3. Modify jvm.config file Add -Djava.security.auth.login.config=/etc/presto/conf.jaas line on a coordinator and workers to /etc/presto/jvm.config file. 4. Create Kerberos configuration file The file is krb5.conf and the location is /etc folder on a coordinator and workers. 5. Create a SQL Server connector file connector.name = sqlserver connection-url = jdbc:sqlserver://sqlserverserver.sample.com;databaseName=yourdatabasename;integratedSecurity=true;authenticationScheme=JavaKerberos;jaasConfigurationName=SQLJDBCDriver Replace sqlserverserver.sample.com with your SQL server name. It has to be Fully Qualified Domain Name (FQDN). Replace yourdatabasename with SQL Server database name. 6. Deploy the SQL Server connector file to each Presto node in a cluster The location of the SQL Server connector file is /etc/presto/catalog/ folder. The name might be sqlserver.properties . properties is the extension. Presto Admin tool can automate deployment of the connector file to a cluster. presto-admin catalog add sqlserver -I -u <user with sudo permissions> 7. Restart the Presto cluster Presto Admin tool can be handy as well. presto-admin server restart -I -u <user with sudo permissions> Resources Using Kerberos integrated authentication to connect to SQL Server Setting the connection properties","tags":"Trino(Presto)","url":"https://techjogging.com/access-sql-server-presto-kerberos-authentication.html","loc":"https://techjogging.com/access-sql-server-presto-kerberos-authentication.html"},{"title":"Access MinIO S3 Storage in Presto with Hive Metastore","text":"Presto Hive connector is aimed to access HDFS or S3 compatible storages. One of the key components of the connector is metastore which maps data files with schemas and tables. Two production metastore services are Hive and AWS Glue Data Catalog. Hive metastore works transparently with MinIO S3 compatible system. One more non official metastore is file. The information about file metastore can be find in Access MinIO S3 Storage in Presto with File Metastore article. The sample is based on Starburst 343-e open source distribution with RPM installation and RHEL 7 Linux distribution. 1. Set up Hive metastore for MinIO It might be followed Hive Standalone Metastore for Presto in Docker manual. 2. Create a connector file A set of mandatory parameters are. connector.name = hive-hadoop2 hive.metastore.uri = thrift://URL:9083 hive.metastore.username = metastore hive.s3.aws-access-key = access key hive.s3.aws-secret-key = secret key hive.s3.endpoint = http://URL:9000 hive.s3.path-style-access = true Replace Hive metastore URL in hive.metastore.uri property. The Hive metastore default port is 9083 . Replace credentials to access MinIO in hive.s3.aws-access-key and hive.s3.aws-secret-key properties. Replace MinIO URL in hive.s3.endpoint property. The default port is 9000 . In case if used SSL connection to MinIO server, replace http protocol with https in hive.s3.endpoint property, for example, hive.s3.endpoint=https://<URL>:9000 and add hive.s3.ssl.enabled=true property. 3. Deploy the connector file to each Presto node in a cluster The location of the Hive connector file is /etc/presto/catalog/ folder. The name might be mini.properties . properties is the extension. Presto Admin tool can automate deployment of the connector file to a cluster. presto-admin catalog add minio -I -u <user with sudo permissions> 4. Restart the Presto cluster Presto Admin tool can be handy as well. presto-admin server restart -I -u <user with sudo permissions> 5. Tests to access MinIO data 5.1 Managed table Presto is responsible for deleting table definition and MinIO data when a table is deleted. Create a schema CREATE SCHEMA minio . sample_schema WITH ( location = 's3a://sample_bucket/' ); Use dash ( - ) in the MinIO bucket name and underscore ( _ ) in the schema name. Create a table CREATE TABLE minio . sample_schema . sample_table ( col1 varchar , col2 varchar ); Insert values to the test table INSERT INTO minio . sample_schema . sample_table SELECT 'value1.1' , 'value1.2' ; 5.2 Non-managed table This type of tables is different from the managed one that when a table is deleted, MinIO data is not deleted. Create a schema CREATE SCHEMA minio . sample_schema ; Create a table CREATE TABLE minio . sample_schema . sample_table ( col1 varchar , col2 varchar ) WITH ( external_location = 's3a://sample_bucket/sample_table/' , format = 'TEXTFILE' ); Insert values to the test table INSERT INTO minio . sample_schema . sample_table SELECT 'value1.1' , 'value1.2' ; 5.3 Non-managed table with already existing data in MinIO It can be a case when data has been added already and a table schema is applied to access data as a table. To make sample simple, one column is defined which incorporates all columns and column delimiters. Each text file has a header line. Create a schema CREATE SCHEMA minio . sample_schema ; Create a table CREATE TABLE minio . sample_schema . sample_table ( all_columns varchar ) WITH ( external_location = 's3a://sample_bucket/sample_table/' , format = 'TEXTFILE' , skip_header_line_count = 1 ); 6. Improvements Instruct Presto to submit filter and aggregation statements directly for MinIO execution. hive.s3select-pushdown.enabled=true Make communication between Presto and MinIo more reliable increasing timeout. hive.s3.socket-timeout=1m Resources Hive Connector","tags":"Trino(Presto)","url":"https://techjogging.com/access-minio-s3-storage-prestodb-cluster-hive-metastore.html","loc":"https://techjogging.com/access-minio-s3-storage-prestodb-cluster-hive-metastore.html"},{"title":"Hive Standalone Metastore for Presto in Docker","text":"Hive connector in Presto can retrieve data from both HDFS and S3 compatible storages. The connector requests Hive metastore as a link to those storages. There are two types of metastores. The first one is beefy one which includes multiple services. The second one is light weight and is called standalone metastore. It contains only Hive service. The standalone metastore is used to connect to S3 compatible storages. When there is one S3 endpoint, a coordination or another server can host Hive standalone metastore. In case of many S3 endpoints, it is requested to have a Hive metastore for each endpoint. It is possible to dedicate one metastore but it should be applied a special Apache Ranger setup to separate each S3 endpoint. To save resources, a coordinator might be used to set up Hive standalone meatostores. Docker containers host those Hive metastores. This configuration simplifies setup and maintenance of a Presto cluster. The sample is based on Starburst 343-e open source distribution with RPM installation, Hive standalone metastoere 3.1.2, MinIO S3 storage, and RHEL 7 Linux distribution. 1. Install Docker See Install Docker CE Edition in CentOS/RHEL 7 article. 2. Create Docker working folder Log in to a Docker host with sudo user. mkdir docker_hive cd docker_hive 3. Download and prepare requested software Hive standalone metastore 3.1.2 wget https://repo1.maven.org/maven2/org/apache/hive/hive-standalone-metastore/3.1.2/hive-standalone-metastore-3.1.2-bin.tar.gz tar -xvf hive-standalone-metastore-3.1.2-bin.tar.gz rm -f hive-standalone-metastore-3.1.2-bin.tar.gz mv apache-hive-metastore-3.1.2-bin metastore Hadoop 3.2.1 wget http://apache.mirrors.hoobly.com/hadoop/common/hadoop-3.2.1/hadoop-3.2.1.tar.gz tar -xvf hadoop-3.2.1.tar.gz rm -f hadoop-3.2.1.tar.gz JDBC connector Option #1. Postgres backend. Replace postgresql- .jar with the latest one. wget https://jdbc.postgresql.org/download/postgresql-42.2.16.jar Option #2. MySQL backend. Replace mysql-connector-java- .el7.noarch.rpm with the latest one. MySQL connector is included in the RPM package, so after installation, the connector can be found in /usr/share/java folder. wget https://cdn.mysql.com//Downloads/Connector-J/mysql-connector-java-8.0.21-1.el7.noarch.rpm yum install mysql-connector-java-8.0.21-1.el7.noarch.rpm cp /usr/share/java/mysql-connector-java.jar ./ There are more options for a backend but those two are tested. See Resources section below. 4. Install backend server and create a database A backend server can be installed on coordinator or another box. A database can be created, for example, Postgres Using pgAdmin GUI tool. MySQL Commndline mysql tool mysql -u root -p create database metastore ; exit The database name might be metastore . 5. Request corporate Certificate Authority PEM file Copy ca.pem file to docker_hive working folder. 6. Create Dockerfile FROM centos:7 ENV container docker LABEL maintainer = \"your name here\" # copy Certificate Authority file COPY ca.pem /etc/pki/ca-trust/source/anchors/ # copy Hive standalone package COPY metastore /opt/metastore/ # copy Hadoop package COPY hadoop-3.2.1 /opt/hadoop-3.2.1/ # copy Postgres or MySQL JDBC connector COPY postgresql-42.2.16.jar /opt/metastore/lib/ # add Certificate Authority to database RUN update-ca-trust WORKDIR /install # install Java 1.8 and clean cache RUN yum install -y java-1.8.0-openjdk-devel \\ && yum clean all # environment variables requested by Hive metastore ENV JAVA_HOME = /usr/lib/jvm/java-1.8.0-openjdk ENV HADOOP_HOME = /opt/hadoop-3.2.1 # replace a library and add missing libraries RUN rm -f /opt/metastore/lib/guava-19.0.jar \\ && cp ${ HADOOP_HOME } /share/hadoop/common/lib/guava-27.0-jre.jar /opt/metastore/lib \\ && cp ${ HADOOP_HOME } /share/hadoop/tools/lib/hadoop-aws-3.2.1.jar /opt/metastore/lib \\ && cp ${ HADOOP_HOME } /share/hadoop/tools/lib/aws-java-sdk-bundle-1.11.375.jar /opt/metastore/lib WORKDIR /opt/metastore # copy Hive metastore configuration file COPY metastore-site.xml /opt/metastore/conf/ # Hive metastore data folder VOLUME [ \"/user/hive/warehouse\" ] # create metastore backend tables and insert data. Replace postgres with mysql if MySQL backend used RUN bin/schematool -initSchema -dbType postgres CMD [ \"/opt/metastore/bin/start-metastore\" ] In case of creating more than one Hive metastore, replace VOLUME [\"/user/hive/warehouse\"] path. Data is not supposed to be stored in that folder as MinIO or S3 compatible storage is aimed as a data storage. 7. Create Hive metastore setup file <?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?> <?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?> <configuration> <property> <name> fs.s3a.access.key </name> <value> AccessKey </value> </property> <property> <name> fs.s3a.secret.key </name> <value> SecretKey </value> </property> <property> <name> fs.s3a.connection.ssl.enabled </name> <value> true </value> </property> <property> <name> fs.s3a.path.style.access </name> <value> true </value> </property> <property> <name> fs.s3a.endpoint </name> <value> MinIO URL:9000 </value> </property> <property> <name> javax.jdo.option.ConnectionURL </name> <value> jdbc:postgresql://Backend URL or name:30684/metadata?allowPublicKeyRetrieval=true &amp; useSSL=false &amp; serverTimezone=UTC </value> </property> <property> <name> javax.jdo.option.ConnectionDriverName </name> <value> org.postgresql.Driver </value> </property> <property> <name> javax.jdo.option.ConnectionUserName </name> <value> Backend user name </value> </property> <property> <name> javax.jdo.option.ConnectionPassword </name> <value> Backend user password </value> </property> <property> <name> hive.metastore.event.db.notification.api.auth </name> <value> false </value> </property> <property> <name> metastore.thrift.uris </name> <value> thrift://localhost:9083 </value> <description> Thrift URI for the remote metastore. Used by metastore client to connect to remote metastore. </description> </property> <property> <name> metastore.task.threads.always </name> <value> org.apache.hadoop.hive.metastore.events.EventCleanerTask </value> </property> <property> <name> metastore.expression.proxy </name> <value> org.apache.hadoop.hive.metastore.DefaultPartitionExpressionProxy </value> </property> <property> <name> metastore.warehouse.dir </name> <value> /user/hive/warehouse </value> </property> </configuration> Replace AccessKey with MinIO access key. Replace SecretKey with MinIO secret key. Make fs.s3a.connection.ssl.enabled property true if MinIO is secured with HTTPS protocol otherwise false . Replace MinIO URL:9000 with MinIO address and port. Replace Backend URL or name:30684 with backend address and port. If it is installed on a coordinator, use localhost . If MySQL is used as backend, replace javax.jdo.option.ConnectionURL property with jdbc:mysql://Backend URL or name:3306/metastorepresto?allowPublicKeyRetrieval=true&amp;useSSL=false&amp;serverTimezone=UTC . If MySQL is used as backend, replace javax.jdo.option.ConnectionDriverName property with com.mysql.cj.jdbc.Driver . Replace Backend user name with backend user name. Replace Backend user password with backend user password. Replace /user/hive/warehouse value metastore.warehouse.dir property in to match with VOLUME [\"/user/hive/warehouse\"] Dockerfile . If more than one Hive metastore, add metastore.warehouse.dir and metastore.thrift.port properties, for example, <property> <name> metastore.thrift.port </name> <value> 9084 </value> </property> 8. Create Docker image docker build -t minio_hiveimage . The command adds tables to backend database as well. 9. Run Hive metastore docker run -d -p 9083 :9083/tcp --name mini_hive minio_hiveimage if more than one metastore, replace port for next metastore. docker run -d -p 9084 :9084/tcp --name mini_hive2 minio_hiveimage2 10. Test metastore Add Hive connector for MinIO storage, for example, minio_connector . See Access MinIO S3 Storage in Presto with Hive Metastore article. It is a catalog level in Presto hierarchy. Create a backet to store your schema, for example, finance-department . Create a schema CREATE SCHEMA minio_connector . finance_department WITH ( location = 's3a://finance-department/' ); Use dash ( - ) in the MinIO bucket name and underscore ( _ ) in the schema name. Create a table CREATE TABLE minio_connector . finance_department . test_table ( col1 varchar , col2 varchar ); Insert values to the test table INSERT INTO minio_connector . finance_department . test_table SELECT 'value1.1' , 'value1.2' ; Everything has to be completed without any error messages and finance_department bucket will conatain test_table folder with a file. Troubleshooting Troubleshooting Access to HTTP/HTTPS Resources in Docker Resources AdminManual Metastore 3.0 Administration Hive Schema Tool","tags":"Trino(Presto)","url":"https://techjogging.com/standalone-hive-metastore-presto-docker.html","loc":"https://techjogging.com/standalone-hive-metastore-presto-docker.html"},{"title":"Install Docker CE in CentOS/RHEL 7","text":"Installing Docker CE in CentOS/RHEL 7 is a trivial process but it is not true in all cases. Specially, when we deal with open source products, maintaining and patching might be an issue for doing it in timely manner. Community forums and other sources are our life savers to find workarounds for any encountering roadblocks. 1. Remove old version yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine 2. Set up Docker repository Install package manager. yum install -y yum-utils Add the Docker repository to the repository database. yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo If an error has been received, it has to be applied a workaround. As per version 7.9 on Oct 6, 2020, the repository path is broken and the path needs to be adjusted manually with the command. yum-config-manager --setopt = \"docker-ce-stable.baseurl=https://download.docker.com/linux/centos/7/x86_64/stable\" --save 3. Install the latest version of Docker CE yum install docker-ce docker-ce-cli containerd.io 4. Enable and run docker daemon systemctl enable docker systemctl start docker 5. Validate installation Docker service status systemctl status docker Output ● docker.service - Docker Application Container Engine Loaded: loaded (/usr/lib/systemd/system/docker.service; enabled; vendor preset: disabled) Active: active (running) since Tue 2020-10-06 10:08:03 EDT; 1 weeks 5 days ago Run the sample image docker run -it centos echo Hello-World Output Hello-World Resources Install Docker Engine on CentOS Docker CE Stable - x86_64 Repo not available : HTTPS Error 404 - Not Found CentOS 7/RHEL 7 installations broken where $releasever is '7Server'","tags":"Docker","url":"https://techjogging.com/install-docker-ce-centosrhel-7.html","loc":"https://techjogging.com/install-docker-ce-centosrhel-7.html"},{"title":"Hide Passwords and Sensitive Data in Presto Configuration Files","text":"Presto configuration files contain passwords and sensitive data in plain text. Corporation security policies are not tolerant with it. That kind of information has to be hidden from users who are not authorized to have access to it. As a rule of thumb, DevOps and security teams are responsible to set up and maintain the part of configuration with sensitive information. It can be achieved by implementing environment variables to supply passwords and sensitive data in Presto configuration files. A Presto property value can be replaced with a name of an environment variable. Populating of environment variables are done during starting of Presto service. Sensitive information can be loaded from different sources, for example, a file located in a secure place outside of Presto. The sample is based on Starburst 343-e open source distribution with RPM installation and RHEL 7 Linux distribution. Sensitive information is stored in a file. 1. Create ini file with secrets The file contains entries in format name=value , for example, MYSQL_SERVER_PASSWORD=password123 . nano /root/presto_secrets.ini 2. Load sensitive information from ini file Presto runs /etc/presto/env.sh file to initiate additional setup. Add an environment variable to the file. export MYSQL_SERVER_PASSWORD = $( awk -F \"=\" '/MYSQL_SERVER_PASSWORD/ {print $2}' /root/presto_secrets.ini ) 3. Deploy setup files to a coordinator and/or workers /root/presto_secrets.ini /etc/presto/env.sh 4. Limit access to ini file on each Presto node Make root as an owner of the file and remove everybody else from accessing the file. chown root:root /root/presto_secrets.ini chmod g+rw,u+rw,o-rwx /root/presto_secrets.ini 5. Adjust Presto service file The file location is /etc/rc.d/init.d/presto . Add --preserve-env after sudo -u $SERVICE_USER . start () { echo \"Starting ${ SERVICE_NAME } \" if [ -z \" ${ JAVA_HOME } \" ] then echo \"Warning: No value found for \\$JAVA_HOME. Default Java will be used.\" > & 2 sudo -u $SERVICE_USER --preserve-env /usr/lib/presto/bin/launcher start \" ${ CONFIGURATION [@] } \" else sudo -u $SERVICE_USER --preserve-env PATH = ${ JAVA_HOME } /bin: $PATH /usr/lib/presto/bin/launcher start \" ${ CONFIGURATION [@] } \" fi return $? } 6. Restart Presto cluster 7. Usage samples MySQL Server connector file. connector.name = mysql connection-url = jdbc:mysql://example.net:3306 connection-user = root connection-password = ${ENV:MYSQL_SERVER_PASSWORD} Resources Presto Secrets Presto Slack channels","tags":"Trino(Presto)","url":"https://techjogging.com/hide-passwords-sensitive-data-presto-config-files.html","loc":"https://techjogging.com/hide-passwords-sensitive-data-presto-config-files.html"},{"title":"Connect Presto to Cloudera Hive with Kerberos Authentication","text":"Presto includes Hive connector to access Hive data warehouse. Warehouse data is stored in the Hadoop Distributed File System (HDFS) or in S3 compatible storages. Data files located in Hive warehouse are in varieties of formats and data size can be enormous. Presto uses Hive metastore to discover schemas and tables in undelaying data files and runs its own query engine. Kerberos authentication with keytab is applied to access HDFS and Hive metastore. The sample is based on Starburst 343-e open source distribution, Cloudera CDH 5.7.0 and RHEL 7 Linux distribution. 1. Extract setup files from Cloudera CDH and copy to Presto nodes Open Cloudera Manager . Click on Hive service and then select Download Client Configuration in Actions button. Extract core-site.xml and hdfs-site.xml files from hive-clientconfig.zip downloaded archive. Copy files to /etc/presto folder in Presto nodes. 2. Copy Kerberos setup file from Cloudera CDH host to Presto nodes The file name is krb5.conf and it is located in /etc folder on Cloudera CDH host and it has to be copied to /etc folder in Presto nodes. 3. Create keytab file The file is supposed to be provided by the security team of your company. Follow the information in Create keytab File for Kerberos Authentication in Windows or Create keytab File for Kerberos Authentication in Linux article. Distribute principal_name.keytab created file to /etc/presto folder in Presto nodes. 4. Create connector file connector.name = hive-hadoop2 hive.metastore.uri = thrift://hive_meta_store_host.sample.com:9083 hive.metastore.authentication.type = KERBEROS hive.metastore.service.principal = hive/_HOST@SAMPLE.COM hive.metastore.client.principal = principal_name@SAMPLE.COM hive.metastore.client.keytab = /etc/presto/principal_name.keytab hive.hdfs.authentication.type = KERBEROS hive.hdfs.presto.principal = principal_name@SAMPLE.COM hive.hdfs.presto.keytab = /etc/presto/principal_name.keytab hive.config.resources = /etc/presto/core-site.xml,/etc/presto/hdfs-site.xml Hive metastore host: hive_meta_store_host.sample.com Hive metastore default port: 9083 Company domain: SAMPLE.COM Account name in keytab: principal_name Keytab location and name: /etc/presto/principal_name.keytab The same account and keytab are used to access HDFS. 5. Deploy connector file to Presto cluster The connector goes to /etc/presto/catalog folder in each Presto node and after Presto cluster has to be restarted. Presto Admin can accomplish it. presto-admin catalog add hive_connector -I -u <user name with sudo access> presto-admin server restart -I -u <user name with sudo access>","tags":"Trino(Presto)","url":"https://techjogging.com/connect-presto-cloudera-hive-with-kerberos-authentication.html","loc":"https://techjogging.com/connect-presto-cloudera-hive-with-kerberos-authentication.html"},{"title":"NVMe RAID Performance on Z390 Chipset Motherboard","text":"NVMe SSD RAIDs are getting more popular in our days. It is an interesting question - what hardware to use to build more efficient RAID in terms of performance. Z390 chipset motherboard might include 2 or 3 NVMe slots for building of Raid 0 or 1 based on Intel® Rapid Storage Technology. Also, if a motherboard supports bifurcation, it can be used NVMe PCIe expansion cards. Some NVMe card don't request bifurcation but they are more expensive. Performance tests have been done on ASUS Prime Z390-A motherboard with WD Blue SN550 1TB and Intel 660P 1TB NVMe SSDs used in RAID 1. WD Blue SN550 NVMes have been installed in onboard slots whereas Intel 660P has been set up in ASUS HYPER M.2 X16 CARD V2 card. AS SSD Benchmark 1GB data Intel 660P WD Blue SN550 AS SSD Benchmark 10GB data Intel 660P WD Blue SN550 AS SSD Benchmark 1GB data with BitLocker encrypted Intel 660P WD Blue SN550 AS SSD Benchmark 10GB data with BitLocker encrypted Intel 660P WD Blue SN550 Conclusion In spite of better performance in specification, Western Digital NVMe RAID is behind of Intel one in many tests. HYPER M.2 X16 card boosts performance of Intel RAID because of directly interfacing with the CPU's existing PCIe lanes. Also, Intel RAID is better than Western Digital one in all writing tests. Western Digital RAID overperforms Intel one in 10GB data chunks because Intel NVMe buffer is out of game. Copying a big data from or to Intel RAID is an issue as it starts throttling. The speed is dropping significantly and it gets comparable with existing hard drives.","tags":"Hardware","url":"https://techjogging.com/nvme-raid-performance-z390-chipset-motherboards.html","loc":"https://techjogging.com/nvme-raid-performance-z390-chipset-motherboards.html"},{"title":"Remove Private Key from Java Keystore","text":"Java keystore (JKS) is a file secured by a password. Java keystore repository can contain public key certificates with corresponding private keys. The keys are used to encrypt communication over network. If JKS file is distributed, private keys should be removed from the repository. keytool tool is aimed to manipulate JKS repository. The tool is a part of JDK or JRE. List certificates in a source JKS repository keytool -list -v -keystore source_keystore.jks Export a public key certificate from a JKS file. Public key certificate is exported in binary format. keytool -export -alias <your certificate name> -keystore source_keystore.jks -file < export file name, for example, public.der> -storepass < source JKS password> Import the public key certificate into a new JKS file. If JKS file doesn't exist, it will be created. keytool -import -trustcacerts -alias <your certificate name> -file < export file name, for example, public.der> -keystore destination_keystore.jks -storepass <destination JKS password> List certificates in a destination JKS repository. keytool -list -v -keystore destination_keystore.jks","tags":"Security","url":"https://techjogging.com/remove-private-key-java-keystore.html","loc":"https://techjogging.com/remove-private-key-java-keystore.html"},{"title":"Connect DBeaver to Presto with HTTPS Protocol","text":"Communication between Presto and a client can be secured with HTTPS protocol . config.properties Presto setup file located in /etc/presto folder includes the information necessary for establishing a connection to DBeaver. Java Keystore file contains a public key which is required to connect DBeaver to Presto cluster secured with SSL/TLS. Also, it is possible to disable SSL verification. The sample uses encrypted with SSL/TLS Starburst Presto cluster. Setup This step can be skipped if SSL verification is disabled in a connection. Find a file defined in http-server.https.keystore.path variable. The file can be in two formats: Java keystore ( .jks ) or certificate ( .pem ). Java keystore works for DBeaver. Certificate can be converted into Java keystore as well. It is recommended to remove private key from Java keystore . Place the file in any location where DBeaver installed. It might be DBeaver root or C:\\Users\\[Windows user name]\\AppData\\Roaming\\DBeaverData folder. Create PrestoSQL connection. Replace Host with your Presto coordinator FQDN and port with your Presto HTTPS one, and add user name on Main tab. Add properties on Driver properties tab. Option 1 Properties SSL : true SSLKeyStorePath : path to Java Keystore file, for example, C:/Users/sample/AppData/Roaming/DBeaverData. Make sure to use forward slash . SSLKeyStorePassword : Java Keystore file password Option 2 Properties SSL : true SSLVerification : NONE Test connection. Success Failure Resources JDBC driver","tags":"Trino(Presto)","url":"https://techjogging.com/connect-dbeaver-presto-https-protocol.html","loc":"https://techjogging.com/connect-dbeaver-presto-https-protocol.html"},{"title":"Access Windows 10 Shared Folder from RHEL/CentOS 7","text":"Heterogeneous networks are common now. Those networks include computers with Windows and Unix Operation Systems (OS). Exchange files between those Operation Systems is a trivial task. It can be done with command line interface (CLI) or Graphic User Interface (GUI). Both methods give the same results but it depends on usage, user technical level, and so on which method is suitable. Access with CLI Install Samba client and other related libraries on your Linux computer. sudo yum -y install samba-client samba-common cifs-utils Create mount point. sudo mkdir -p /mnt/F_drive Check connection to Windows server. smbclient -L //window_server -U user_name Access Windows shared folder. sudo mount.cifs //window_server/F_shared_drive /mnt/F_drive -o rw,username = user,file_mode = 0777 ,dir_mode = 0777 Parameters rw - read write access ro - read only access file_mode=0777,dir_mode=0777 - Linux access to shared folder. The sample grants full access to everybody. Troubleshooting Invalid argument error mount error(22): Invalid argument Refer to the mount.cifs(8) manual page (e.g. man mount.cifs) Possible issue is missing shared folder in UNC: //window_server rather than //window_server/F_shared_drive . Device or resource busy error mount error(16): Device or resource busy Refer to the mount.cifs(8) manual page (e.g. man mount.cifs) Possible issue is that your mount is used. You need to unmount your mount point, for example, sudo umount /mnt/F_drive Could not resolve address error mount error: could not resolve address for window_server: Unknown error Possible issue is shared folder computer name. You need to replace name with IP address, for example, replace //window_server with 192.168.0.2 . Access with GUI Files is a default file manager in GNOME desktop and it has embedded Samba client. Open Files , go to Other Locations , and type in your shared folder path, for example, smb://window_server/F_shared_drive . Fill out connection form. Troubleshooting. Unable to access location. Failed to retrieve share list from server: No such file or directory error. There are 2 possible reasons. Address can not be resolved and you need to use IP address rather than computer name, for example, 192.168.0.2 . It is used the SMB version 1.0 protocol in Files file manager. You might need to try one of the options below. a) Update your Linux computer. sudo yum update b) Downgrade SMB protocol to version 1.0 in your Windows server. Open Windows Explorer and key in Control Panel\\Programs in address bar Go to Turn Windows features on or off Check SMB 1.0/CIFS Server setting Unable to access location. Failed to mount Windows share: Connection time out error. Enable file in printer sharing in Windows 10. Open Advanced sharing settings form typing Control Panel\\Network and Internet\\Network and Sharing Center\\Advanced sharing settings in Windows Explorer address bar. Resources Simple file manager for GNOME","tags":"Windows","url":"https://techjogging.com/access-windows-shared-folder-from-centos.html","loc":"https://techjogging.com/access-windows-shared-folder-from-centos.html"},{"title":"Set Up NVMe Raid on Z370/Z390 Motherboards","text":"Z370 and Z390 Intel chipsets was released back in 2017 and 2018 years and they still are mainstream now. There are many motherboards based on Z370 and Z390 chipsets containing 2 or more M.2 sockets. Those motherboards might accommodate NVMe SSD storages with RAID 0 or 1 built on Intel® Rapid Storage Technology. ASRock Z370 Pro4 and ASUS Prime Z390-A motherboards are used in the article to build RAID 1 on WD Blue SN550 1TB and Intel 660P 1TB NVMe SSDs. The performnce of NVMe SSDs can be observed in NVMe RAID Performance on Z390 Chipset Motherboard article. Those two motherboards and NVMe SSDs work unstable in bootable RAID 1 configuration. It seems the chipsets are not aimed to be used in that role. After installing Windows 10 on RAID 1, both motherboards are constantly doing two steps: rebuilding RAID and crashing Windows. There are 2 ways to fix it. The first way is to use ASUS HYPER M.2 X16 CARD V2 adapter. It is only applicable to ASUS motherboard as ASRock motherboard is lack of bifurcation. The second way is to make your motherboard cooler running CPU and motherboard fans on 80% or more of their capacity. Also, it can be used non-bootable RAID. Intel 660P NVMe is only supported by ASUS HYPER M.2 X16 CARD V2 adapter as the adapter accepts Intel NVMe SSDs for Z370 and Z390 chipsets. WD Blue SN550 is recognized but RAID can't be built and it works unstable as well. Installing RAID on ASRock Z370 Pro4 motherboard Enable RAID in BIOS. Enter Advanced menu. Click Storage Configuration menu. Switch SATA Operation Mode option to Raid. Enable remapping of installed PCIE SSD M.2 slots. Save BIOS setting. Exit BIOS setup. Reboot computer. Create a RAID volume. Enter Advanced menu. Click Intel Rapid Storage Technology. This item is available only if Raid mode is enabled. Create RAID. After the RAID setup completed, Windows 10 can be installed. Windows 10 does not request any additional drivers as BIOS handles everything transparently. Installing Raid on ASUS Prime Z390-A motherboard The easiest way to set up Raid is EZ Tuning Wizard. It takes care of all settings. Run EZ Tuning Wizard from BIOS landing screen. Select PCIE mode Follow wizard. After the RAID setup completed, Windows 10 can be installed. Windows 10 does not request any additional drivers as BIOS handles everything transparently. Installing RAID with ASUS HYPER M.2 X16 CARD V2 adapter Enable the adapter in BIOS Pay attention to information how many NVMe SSDs can be used in the adapter and what slots can be occupied. In the sample below, M.2_1, M.2_3, and M.2_4 are usable slots and it can be maximum 3 NVMe SSDs. Switch to RAID mode in SATA Mode Selection Reboot computer Create a RAID volume. Enter Advanced menu. Click Intel Rapid Storage Technology. This item is available only if RAID mode is enabled. Create RAID. Switch Launch CSM to Disabled in Boot menu After the RAID setup completed, Windows 10 can be installed. Windows 10 requests drivers and they can downloaded from the motherboard support page and located in SATA section. Extract drivers and write to USB with Windows 10 installation.","tags":"Hardware","url":"https://techjogging.com/set-up-nvme-raid-z370-z390-chipset-motherboards.html","loc":"https://techjogging.com/set-up-nvme-raid-z370-z390-chipset-motherboards.html"},{"title":"Memory Configuration in Presto Cluster","text":"Presto cluster is sensitive to memory setup. As Presto is developed in Java, Java is foundation to configure it. In many cases, Presto server is not started because of memory configuration. During Presto server launch, the validation rules are applied to make sure that major memory settings are consistent. It does not guarantee of cluster stability and performance so spending time on initial memory setup can contribute to success of your cluster. The article is based on CentOS 7 environment and Starburst version 332-e.1. Disable Linux swap Presto assumes that memory swap is disabled and is not mounted. The current swappiness setting can be received. cat /proc/sys/vm/swappiness Turn off swappiness temporary. sudo sysctl vm.swappiness = 0 Turn off swappiness permanently changing vm.swappiness=0 setting in the file below. sudo nano /etc/sysctl.conf Swap memory information. free -h Output total used free shared buff/cache available Mem: 503G 182G 297G 10M 23G 319G Swap: 0B 0B 0B JVM configuration The setting is defined in jvm.config file. It should be set up 70-80% of a server physical memory. If you are tough on resources, it can be at least 12GB less than physical memory. The more setting value is set up, the less stable system you get. -Xmx480G Default Presto configuration The list of memory settings are below. If any value is skipped, it is taken as the default one. query.max-memory-per-node Default value : JVM max memory * 0.1 Description : Max amount of user memory a query can use on a worker. query.max-total-memory-per-node Default value : JVM max memory * 0.3 Description : Max amount of user and system memory a query can use on a worker. query.max-memory Default value : 20GB Description : Max amount of user memory a query can use across the entire cluster. query.max-total-memory Default value : query.max-memory * 2 Description: Max amount of user and system memory a query can use across the entire cluster. memory.heap-headroom-per-node Default value : JVM max memory * 0.3 Description : Amount of memory set aside as headroom/buffer in the JVM heap for allocations that are not tracked by Presto. Basic memory setup Physical memory: 512GB Workers: 10 JVM Xmx: physical memory * 70% = 358GB query.max-memory-per-node: JVM Xmx * 0.5 = 179GB query.max-total-memory-per-node: query.max-memory-per-node * 1.2 = 214GB memory.heap-headroom-per-node: 50GB query.max-memory: workers * query.max-memory-per-node = 1,790GB query.max-total-memory: workers * query.max-total-memory-per-node = 2,140GB Highly concurrent memory setup Physical memory: 512GB Workers: 10 JVM Xmx: physical memory * 70% = 358GB query.max-memory-per-node: JVM Xmx * 0.1 = 36GB query.max-total-memory-per-node: query.max-memory-per-node * 1.2 = 43GB memory.heap-headroom-per-node = 50GB query.max-memory: workers * query.max-memory-per-node = 360GB query.max-total-memory: workers * query.max-total-memory-per-node = 430GB Large data skew memory setup Physical memory: 512GB Workers: 10 JVM Xmx: physical memory * 80% = 410GB query.max-memory-per-node: JVM Xmx * 0.7 = 287GB query.max-total-memory-per-node: query.max-memory-per-node * 1.2 = 344GB memory.heap-headroom-per-node = 30GB query.max-memory: workers * query.max-memory-per-node = 2,870GB query.max-total-memory: workers * query.max-total-memory-per-node = 3,440GB Validation rule JVM Xmx > query.max-total-memory-per-node + memory.heap-headroom-per-node Killer policy in case of out of memory Out of memory (OOM) is customizable. The setting is query.low-memory-killer.policy . Spill to disk OOM can be mitigated if spilling memory to disk is enabled. It does not cover all possible cases. The configuration file is config.properties . For example, experimental.max-spill-per-node = 500GB experimental.query-max-spill-per-node = 200GB experimental.spill-enabled = true experimental.spiller-spill-path = /mnt/presto/data/spill Resources Presto Memory Management Properties Starburst Configuring Presto Presto The Definitive Guide by O'Reilly","tags":"Trino(Presto)","url":"https://techjogging.com/memory-setup-prestodb-cluster.html","loc":"https://techjogging.com/memory-setup-prestodb-cluster.html"},{"title":"Connect Microsoft Access to Hive with ODBC in Windows","text":"Microsoft Access can be used to connect to Big Data. Cloudera Hadoop cluster is one of Big Data platforms. Those two systems can easily interact with each other by means of an ODBC connection. Microsoft Access natively supports ODBC drivers and Cloudera develops ODBC drivers for many Operation Systems. Hive data size might impact on a way how to work with data in Microsoft Access. 1. Check Microsoft Access version, if it is 32 or 64-bit. Open Microsoft Access . Go to File menu. Click Account item. Click About Access button. The image shows Microsoft Access 64-bit. 2. Download Cloudera Hive ODBC driver. The latest driver can be found on Cloudera web site. Let's download version 2.6.4 64-bit driver . The file name is ClouderaHiveODBC64.msi . 3. Create new ODBC Data Source Click on Windows Start button and type in ODBC in Search bar, then run ODBC Data Sources (64-bit) . Go to System DSN tab. After installing Cloudera Hive driver, it will be 2 new entries: Sample Cloudera Hive DSN 32-bit and 64-bit. Create a new data source Fill out all fields and click Test button to make sure it works. The sample reveals one of the possible configurations. Hive Server Type: Hive Server 2 Service Discovery Mode: No Service Discovery Host(s): samplehive Port: 10000 Database: default Mechanism: Kerberos Host FQDN: samplehive.com Service: Hive Thrift Transport: SASL 4. Link Hive table to Microsoft Access Open Microsoft Access . Go to External Data menu, then New Data Source , after From Other Sources , and finally ODBC Database . Select Link to the data source by creating a linked table option. Pick up Sample Hive Machine Data Source created on step #3. Be patient. It might take a few minutes to show a list of all tables in all Hive databases. Select your table, and then a unique column or columns to finish it up. 5. Change Microsoft Access timeout when opening a Hive linked table In some cases, the timeout error can come up. It depends on how busy is Hive or how much data is in a linked table. Open Registry Editor in Windows. Find HKEY_LOCAL_MACHINE\\SOFTWARE\\Wow6432Node\\Microsoft\\Office\\15.0\\Access Connectivity Engine\\Engines\\ODBC entry and change QueryTimeout value. 6. Features to access Hive data The size is the main limitation to use Hive data so you might need to follow those guidelines. Before a table usage, figure out the record count in your table. If size is up to 100,000 records, the table can be used as a linked one with all available operations in Microsoft Access. If size is larger than 100,000 records or so, create a query to filter data to the acceptable size and export data to a Microsoft Access table.","tags":"Microsoft Access","url":"https://techjogging.com/connect-microsoft-access-hive-odbc-windows.html","loc":"https://techjogging.com/connect-microsoft-access-hive-odbc-windows.html"},{"title":"Trino(Presto) Installation and Setup Pitfalls","text":"Like other installations and setups, Trino formerly PrestoSQL can contain steps which cause difficulties. How many times you were stuck with something? In mostly cases, it was a trivial issue but you spent countless time to solve it. It's better to have a cheat sheet for discovering those issues before encountering them. The list of pitfalls is based on Starburst open-source distribution. History Trino formerly PrestoSQL was originated in 2012 year as PrestoDB open-source project in Facebook. PrestoSQL was started in 2019 by the PrestoDB founders. Facebook forced to rebrand PrestoSQL into Trino in 2020. One of the successful commercial distribution based on Trino is Starburst . Starburst includes both open-source and commercial products. Disable swap on each node Trino assumes that swap is not used. Swap can dramatically impact on performance and stability of a Trino cluster. If swap is on, memory consumption will be close to 100% and, as a result, Trino cluster will be slow and many queries will fail. The typical error messages are. Error type 1. io.prestosql.spi.PrestoException: Query 20200720_132906_00038_4smph has not been accessed since 2020-07-20T09:42:25.080-04:00: currentTime 2020-07-20T09:52:25.447-04:00 Error type 2. io.prestosql.spi.PrestoTransportException: Encountered too many errors talking to a worker node. The node may have crashed or be under too much load. This is probably a transient issue, so please retry your query in a few minutes. The current swappiness setting can be received. cat /proc/sys/vm/swappiness Turn off swappiness temporary. sudo sysctl vm.swappiness = 0 Turn off swappiness permanently changing vm.swappiness=0 setting in the file below. sudo nano /etc/sysctl.conf Swap memory information. free -m Java 11 installation OpenJDK 11 can be used. Java 11 does not have JRE dedicated folder. sudo yum install java-11-openjdk-devel OpenJDK JRE folder is /usr/lib/jvm/jre-11 . It points to the same location as JDK one. SQL Server connector overwhelms SQL Server When data is written to SQL Server, Trino tries to do it as fast as possible. It will utilize all workers to push data to SQL Server. As a result, it opens a lot of connections at least one per worker and SQL Server can crash. Wideness of an exported table impacts on it as well. The more columns is in your table, the more chances to encounter the issue can be. Also, the number of records in a destination table contributes to the issue. The error message is io.prestosql.spi.PrestoException: There is insufficient system memory in resource pool 'default' to run this query. at io.prestosql.plugin.jdbc.JdbcPageSink.appendPage(JdbcPageSink.java:117) at io.prestosql.operator.TableWriterOperator.addInput(TableWriterOperator.java:257) at io.prestosql.operator.Driver.processInternal(Driver.java:384) at io.prestosql.operator.Driver.lambda$processFor$8(Driver.java:283) at io.prestosql.operator.Driver.tryWithLock(Driver.java:675) at io.prestosql.operator.Driver.processFor(Driver.java:276) at io.prestosql.execution.SqlTaskExecution$DriverSplitRunner.processFor(SqlTaskExecution.java:1076) at io.prestosql.execution.executor.PrioritizedSplitRunner.process(PrioritizedSplitRunner.java:163) at io.prestosql.execution.executor.TaskExecutor$TaskRunner.run(TaskExecutor.java:484) at io.prestosql.$gen.Presto_348_e____20210219_123137_2.run(Unknown Source) at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) at java.base/java.lang.Thread.run(Thread.java:834) Caused by: com.microsoft.sqlserver.jdbc.SQLServerException: There is insufficient system memory in resource pool 'default' to run this query. at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:254) at com.microsoft.sqlserver.jdbc.SQLServerStatement.getNextResult(SQLServerStatement.java:1608) at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.doExecutePreparedStatementBatch(SQLServerPreparedStatement.java:2766) at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement$PrepStmtBatchExecCmd.doExecute(SQLServerPreparedStatement.java:2641) at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7240) at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:2869) at com.microsoft.sqlserver.jdbc.SQLServerStatement.executeCommand(SQLServerStatement.java:243) at com.microsoft.sqlserver.jdbc.SQLServerStatement.executeStatement(SQLServerStatement.java:218) at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeBatch(SQLServerPreparedStatement.java:2056) at io.prestosql.plugin.jdbc.JdbcPageSink.appendPage(JdbcPageSink.java:109) ... 12 more To solve the issue, RAM of SQL Server should be pumped up. You can try to increase SQL Server memory until the issue is gone. For example, if you export a table with 100 columns and your record count is some hundred million records, RAM can be set up to 96GB with 90GB dedicated to SQL Server. Permissions for /tmp folder if Hive connector used /tmp folder has to have the permissions in case of using Hive connector. ls -ld /tmp drwxrwxrwx. 13 root root 4096 Jul 30 15 :08 /tmp Trino copies Hive connector files in /tmp folder during Trino server starup. The location of the temporary folder can be changes with -Djava.io.tmpdir property in jvm.config file. If /tmp folder is not granted emough permissions, Trino server will not start. server.log error message when Hive connector is being loaded. 2021-07-30T14:09:22.395-0400 INFO main io.trino.metadata.StaticCatalogStore -- Loading catalog /etc/starburst/catalog/hive_connector.properties -- ... 2021-07-30T14:09:23.815-0400 ERROR main io.trino.server.Server null java.lang.ExceptionInInitializerError at io.trino.plugin.hive.HdfsEnvironment$$FastClassByGuice$$e99ee3bd.newInstance(<generated>) at com.google.inject.internal.DefaultConstructionProxyFactory$FastClassProxy.newInstance(DefaultConstructionProxyFactory.java:89) at com.google.inject.internal.ConstructorInjector.provision(ConstructorInjector.java:114) at com.google.inject.internal.ConstructorInjector.access$000(ConstructorInjector.java:32) at com.google.inject.internal.ConstructorInjector$1.call(ConstructorInjector.java:98) at com.google.inject.internal.ProvisionListenerStackCallback$Provision.provision(ProvisionListenerStackCallback.java:112) at io.airlift.bootstrap.LifeCycleModule.provision(LifeCycleModule.java:54) at com.google.inject.internal.ProvisionListenerStackCallback$Provision.provision(ProvisionListenerStackCallback.java:120) at com.google.inject.internal.ProvisionListenerStackCallback.provision(ProvisionListenerStackCallback.java:66) at com.google.inject.internal.ConstructorInjector.construct(ConstructorInjector.java:93) at com.google.inject.internal.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:306) at com.google.inject.internal.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:40) at com.google.inject.internal.SingletonScope$1.get(SingletonScope.java:168) at com.google.inject.internal.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:39) at com.google.inject.internal.SingleParameterInjector.inject(SingleParameterInjector.java:42) at com.google.inject.internal.SingleParameterInjector.getAll(SingleParameterInjector.java:65) at com.google.inject.internal.ConstructorInjector.provision(ConstructorInjector.java:113) at com.google.inject.internal.ConstructorInjector.access$000(ConstructorInjector.java:32) at com.google.inject.internal.ConstructorInjector$1.call(ConstructorInjector.java:98) at com.google.inject.internal.ProvisionListenerStackCallback$Provision.provision(ProvisionListenerStackCallback.java:112) at io.airlift.bootstrap.LifeCycleModule.provision(LifeCycleModule.java:54) at com.google.inject.internal.ProvisionListenerStackCallback$Provision.provision(ProvisionListenerStackCallback.java:120) at com.google.inject.internal.ProvisionListenerStackCallback.provision(ProvisionListenerStackCallback.java:66) at com.google.inject.internal.ConstructorInjector.construct(ConstructorInjector.java:93) at com.google.inject.internal.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:306) at com.google.inject.internal.FactoryProxy.get(FactoryProxy.java:62) at com.google.inject.internal.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:40) at com.google.inject.internal.SingletonScope$1.get(SingletonScope.java:168) at com.google.inject.internal.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:39) at com.google.inject.internal.InternalInjectorCreator.loadEagerSingletons(InternalInjectorCreator.java:213) at com.google.inject.internal.InternalInjectorCreator.injectDynamically(InternalInjectorCreator.java:184) at com.google.inject.internal.InternalInjectorCreator.build(InternalInjectorCreator.java:111) at com.google.inject.Guice.createInjector(Guice.java:87) at io.airlift.bootstrap.Bootstrap.initialize(Bootstrap.java:276) at io.trino.plugin.hive.InternalHiveConnectorFactory.createConnector(InternalHiveConnectorFactory.java:117) at io.trino.plugin.hive.InternalHiveConnectorFactory.createConnector(InternalHiveConnectorFactory.java:77) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:566) at io.trino.plugin.hive.HiveConnectorFactory.create(HiveConnectorFactory.java:63) at io.trino.connector.ConnectorManager.createConnector(ConnectorManager.java:359) at io.trino.connector.ConnectorManager.createCatalog(ConnectorManager.java:216) at io.trino.connector.ConnectorManager.createCatalog(ConnectorManager.java:208) at io.trino.connector.ConnectorManager.createCatalog(ConnectorManager.java:194) at io.trino.metadata.StaticCatalogStore.loadCatalog(StaticCatalogStore.java:88) at io.trino.metadata.StaticCatalogStore.loadCatalogs(StaticCatalogStore.java:68) at io.trino.server.Server.doStart(Server.java:119) at io.trino.server.Server.lambda$start$0(Server.java:73) at io.trino.$gen.Trino_354_e____20210730_180904_1.run(Unknown Source) at io.trino.server.Server.start(Server.java:73) at com.starburstdata.presto.StarburstTrinoServer.main(StarburstTrinoServer.java:50) Caused by: java.lang.RuntimeException: failed to load Hadoop native library at io.trino.hadoop.HadoopNative.requireHadoopNative(HadoopNative.java:59) at io.trino.plugin.hive.HdfsEnvironment.<clinit>(HdfsEnvironment.java:39) ... 52 more Caused by: java.io.IOException: Permission denied at java.base/java.io.UnixFileSystem.createFileExclusively(Native Method) at java.base/java.io.File.createTempFile(File.java:2129) at java.base/java.io.File.createTempFile(File.java:2175) at io.trino.hadoop.HadoopNative.loadLibrary(HadoopNative.java:92) at io.trino.hadoop.HadoopNative.requireHadoopNative(HadoopNative.java:47) ... 53 more","tags":"Trino(Presto)","url":"https://techjogging.com/prestodb-installation-setup-pitfalls.html","loc":"https://techjogging.com/prestodb-installation-setup-pitfalls.html"},{"title":"Access MinIO S3 Storage in Presto with File Metastore","text":"Presto accesses a variety of data sources by means of connectors. Hive connector is used to access files stored in Hadoop Distributed File System (HDFS) or S3 compatible systems. Metastore can be configured with two options: Hive or AWS Glue Data Catalog. Hive metastore information can be find in Access MinIO S3 Storage in Presto with Hive Metastore article There is another undocumented option, it is the file metastore. It was developed by Dain Sundstrom in a weekend. Metadata and data are stored in file system. As a result, the setup is very simple. It is a couple of lines in a configuration file. This setup is not aimed for production usage. The main use cases might be demo or PoC projects. S3 compatible storages are very good alternatives to store big data. They are lightweight, easy to set up, and support. Many of those storages are open source. When building an enterprise level system, it is important to set up and tune up Presto to work with a coordinator and one or more workers. The setup is different from single node one. MinIO S3 compatible storage along with file metadata configuration is used in the sample below. Internal tables are stored in a shared folder. Hive connector property file is created in /etc/presto/catalog folder or it can be deployed by presto-admin tool or other tools. The name might be minio.properties . It has to have .properties extension name. A set of mandatory parameters are. connector.name = hive-hadoop2 hive.s3.path-style-access = true hive.metastore = file hive.metastore.catalog.dir = file:///mnt/presto/data/minio hive.s3.endpoint = http://minio.sample.com:9000 hive.s3.aws-access-key = YourAccessKey hive.s3.aws-secret-key = YourSercetKey hive.temporary-staging-directory-path = file:///mnt/presto/data/tmp hive.s3.socket-timeout = 1m hive.metastore.catalog.dir - the folder is shared between all nodes: a coordinator and workers. Internal tables are stored in the folder. hive.temporary-staging-directory-path - the folder is shared between all nodes: a coordinator and workers. The location of temporary staging folder that is used for write operations. Each user has a separate sub folder with the name pattern: presto-UserName . If the parameter is missing, INSERT INTO or CREATE TABLE AS statements will write only a portion of data into destination tables and sporadically, the error message will come up. Error moving data files from file:/tmp/presto-root/6b5efc64-177e-409f-b34c-aeddbc942a92/20200612_155605_00395_stnes_45252f19-7244-46ec-86f0-88da4c300c3d to final location file:/mnt/presto/data/minio/schema_name/table_name/20200612_155605_00395_stnes_45252f19-7244-46ec-86f0-88da4c300c3d hive.s3.socket-timeout - default value is 5 seconds and if MinIO is busy, you get the error. Unable to execute HTTP request: Read time out. The sample to test access to MinIO data. CREATE SCHEMA minio . sample_schema ; CREATE TABLE sample_table ( combined_columns VARCHAR ) WITH ( external_location = 's3a://your_minio_bucket_name/' , format = 'TEXTFILE' , skip_header_line_count = 1 );","tags":"Trino(Presto)","url":"https://techjogging.com/access-minio-s3-storage-prestodb-cluster.html","loc":"https://techjogging.com/access-minio-s3-storage-prestodb-cluster.html"},{"title":"Create Ticket Cache File for Kerberos Authentication in Linux","text":"Kerberos credentials can be stored in Kerberos ticket cache. They are valid for relatively short period of time. The period can be a session or a specified timeframe. A Kerberos ticket cache contains a service and a client principal names, lifetime indicators, flags, and the credential itself. Kerberos 5 client is aimed to generate a ticket cache file. The article is based on CentOS / RHEL distribution. 1. Validate that Kerberos 5 client is installed Kerberos 5 client is installed as default. There are two components. yum list installed | grep 'krb5-workstation\\|krb5-libs' Output krb5-libs.x86_64 1.15.1-46.el7 @base krb5-workstation.x86_64 1.15.1-46.el7 @base Kerberos 5 client installation sudo yum install krb5-workstation krb5-libs 2. Create a folder to store ticket cache file mkdir ~/kerberos 3. Add KRB5CCNAME variable The variable defines the location of a Kerberos ticket cache file. Open .bashrc file. nano ~/.bashrc Add the variable export command. export KRB5CCNAME = /home/username/kerberos/krb5cc_username Reboot your computer to make it effective. Validate KRB5CCNAME variable. export | grep KRB5CCNAME 4. Create ticket cache file kinit -c /home/username/kerberos/krb5cc_username username@SAMPLE.COM -l 10h -c means the location of the ticket cache -l states lifetime of the ticket cache 4. Validate ticket cache file klist -c /home/username/kerberos/krb5cc_username 5. Configuration file krb5.conf is a configuration file to tune up Kerberos ticket cache creation. The default location is /etc but KRB5_CONFIG environmental variable can overwrite the location of the configuration file. Our interest is mainly 2 sections: [libdefaults] and [realms] . [libdefaults] default_realm = SAMPLE.COM ticket_lifetime = 24h renew_lifetime = 7d forwardable = true [realms] SAMPLE.COM = { kdc = server1.sample.com }","tags":"Kerberos","url":"https://techjogging.com/create-ticket-cache-kerberos-authentication-linux.html","loc":"https://techjogging.com/create-ticket-cache-kerberos-authentication-linux.html"},{"title":"Add Google Analytics Pageviews in Static Web Site","text":"Google Analytics requests authentication to access Google Analytics data/reports. Implementation of Google Analytics can be done in both places client and server. Server APIs support a wide variety of languages and the authentication process is transparent for users. Javascript is a language to access Google Analytics APIs in client implementation. Also, to view Google Analytics in javascript, users have to be authenticated. The process of authentication includes a Google form to enter user credentials. It works perfectly for a set of scenarios but it doesn't work if we want to see pageviews in static web sites. To solve an issue with authentication, we can use a Google Analytics token. A token allows bypass entering of credentials. It will add an extra step to generate a token and place it in a location accessible by your static web site. Google Analytics token has limited life; it is only 1 hour, so we need to renew our token every hour. It seems complicated but in fact not. It can be used a tool to generate tokens and push it in a static web site. A sample is located in github repository. Static web site implementation is Pelican blog . 1. Create Google Analytics service account and extract json key file Go to Google Analytics Platform APIs and Services Credentials to create a Service account Create Key 2. Develop Generate Google Analytics token tool The tool will use oauth2client library to run from_json_keyfile_name function from ServiceAccountCredentials service object. After getting a token, the token file will be updated on the static web site. from oauth2client.service_account import ServiceAccountCredentials # The scope for the OAuth2 request. SCOPE = 'https://www.googleapis.com/auth/analytics.readonly' # Defines a method to get an access token from the ServiceAccount object. def access_token ( key_file_path ): return ServiceAccountCredentials . from_json_keyfile_name ( key_file_path , SCOPE ) . get_access_token () if __name__ == \"__main__\" : key_file_path = 'tech-jogging-blog-98stj21aac52.json' token_file_path = 'report_access.js' with open ( token_file_path , \"w\" ) as f_out : token = access_token ( key_file_path ) . access_token token_code = 'var ANALYTICS_TOKEN = \\' {} \\' ;' . format ( token ) f_out . write ( token_code ) Generated token javascript file looks like. It contains a variable with token value. var ANALYTICS_TOKEN = 'ya29.c.Ko4BzQdIsUMOFvVHh5O90tnJZvW3but0Ym-C9C1NhKEt3ihBKEk6AOQFp-Mm-MUzZiyLJsfNSD90vqeUm078fSeXl0NXUhWZKvY79BJhg33UB_crRmwDY3Xn98KaPTgi22y4_QFdRA0l3GiQeISkQcnEmb0P1Y_eCquWR-qtDWVy-IBDZRJph2j6otc64oxoqQ' ; 3. Develop javascript code to access Google Analytics The code runs when Google Analytics APIs is loaded, then it is used the generated token to authorize access to Google Analytics, after a pageviews query is sent, and finally the received pageview value is assigned to an HTML control. api . analytics . ready ( function () { /** * Authorize the user with an access token. */ gapi . analytics . auth . authorize ({ 'serverAuth' : { 'access_token' : ANALYTICS_TOKEN } }); var pagePathFilter = 'ga:pagePath==' + window . location . pathname ; var report = new gapi . analytics . report . Data ({ query : { ids : 'ga:209816969' , 'start-date' : 'today' , 'end-date' : 'today' , metrics : 'ga:pageviews' , filters : pagePathFilter } }); report . on ( 'success' , function ( response ) { document . getElementById ( 'query-output' ). value = response . totalsForAllResults [ 'ga:pageviews' ]; }); report . execute (); }); 4. Create a HTML file to show pageviews <!DOCTYPE html> < html > < head > < meta charset = \"utf-8\" > < title > Google Analytics Pageviews Sample </ title > < script > ( function ( w , d , s , g , js , fs ){ g = w . gapi || ( w . gapi = {}); g . analytics = { q : [], ready : function ( f ){ this . q . push ( f );}}; js = d . createElement ( s ); fs = d . getElementsByTagName ( s )[ 0 ]; js . src = 'https://apis.google.com/js/platform.js' ; fs . parentNode . insertBefore ( js , fs ); js . onload = function (){ g . load ( 'analytics' );}; }( window , document , 'script' )); </ script > </ head > < body > < h1 > Google Analytics </ h1 > < h3 > Pageviews Counter </ h3 > < textarea cols = \"10\" rows = \"1\" id = \"query-output\" ></ textarea > < script src = 'report_access.js' ></ script > < script src = 'pageviews.js' ></ script > </ body > </ html > 5. Schedule Generate Google Analytics token tool to be run every hour It can be done in many different ways, for example, Linux - cron, Windows - Task Scheduler, Synology DSM - Task Scheduler.","tags":"Google Analytics","url":"https://techjogging.com/add-google-analytics-pageviews-static-web-site.html","loc":"https://techjogging.com/add-google-analytics-pageviews-static-web-site.html"},{"title":"Create Keytab for Kerberos Authentication in Linux","text":"Keytab stands for key table. It is a file which stores one or more Kerberos principals with corresponding encrypted keys. Encrypted keys are generated based on user passwords. It allows to secure storing of passwords and authenticate users without entering of passwords. The current version of the Kerberos protocol is 5. The article is sampled in CentOS / RHEL distribution. 1. Validate that Kerberos 5 client is installed Kerberos 5 client is installed as default. There are two components. yum list installed | grep 'krb5-workstation\\|krb5-libs' Output krb5-libs.x86_64 1.15.1-46.el7 @base krb5-workstation.x86_64 1.15.1-46.el7 @base Kerberos 5 client installation sudo yum install krb5-workstation krb5-libs 2. Create a folder to store keytab file mkdir ~/kerberos 3. Create keytab file The tool to generate keytab file is interactive one and you need to type in the commands. ktutil ktutil: addent -password -p username@SAMPLE.COM -k 1 -e RC4-HMAC Password for username@SAMPLE.COM: ktutil: wkt /home/username/kerberos/username.keytab ktutil: l slot KVNO Principal ---- ---- --------------------------------------------------------------------- 1 1 username@SAMPLE.COM ktutil: exit 4. Validate keytab file klist -e -k -t ~/kerberos/username.keytab","tags":"Kerberos","url":"https://techjogging.com/create-keytab-file-kerberos-authentication-linux.html","loc":"https://techjogging.com/create-keytab-file-kerberos-authentication-linux.html"},{"title":"Configure Talend Job to Connect to SQL Server with Windows Integrated Authentication","text":"The main way to access MS SQL Server in Talend is SQL Server authentication when user name and password must be supplied. SQL Server credentials are compromised as password is in plain text. In some cases, this is only a way to connect to SQL Server. There is another option to use Windows Integrated authentication. It's safe way because Talend jobs don't contain either user name or password. Credentials are provided by Windows Operation System or another service in background. Windows Integrated authentication is common in Windows environment as it's used Active Directory. 1. Download jTDS SQL Server and Sybase JDBC driver 2. Extract ntlmauth.dll library from the driver package. As Talend is 64 bit now, it has to be 64 bit library as well. jtds-1.3.1-dist │ └───x64 └───SSO ntlmauth.dll 3. Place the extracted library to Talend root folder 4. Set Up tMSSqlInput Talend Component There are 2 options. The first one is to set up tMSSqlInput component directly and another one uses tMSSqlInput component in conjunction with database connection. Option #1. tMSSqlInput component settings. Leave Username and password empty with double quotes. The advanced settings has to include IntegratedSecurity=true in double quotes. Option #2 Create a new database connection. Leave Username and password blank. Add IntegratedSecurity=true to Additional parameters. MSSqlInput component settings with the database connection. 5. Testing Create 2 jobs. Each job contains 2 components: (1) tMSSqlInput and (2) tJavaRow. Change settings of tMSSqlInput component as per Set Up tMSSqlInput Talend Component accordingly. Change settings of tJavaRow component as below.","tags":"Talend","url":"https://techjogging.com/configure-talend-job-connect-sqlserver-windows-integrated-authentication.html","loc":"https://techjogging.com/configure-talend-job-connect-sqlserver-windows-integrated-authentication.html"},{"title":"Installing and Using PIP on Synology DSM","text":"Python3 can be easily installed on Synology DSM through Synology Installation Center but pip installation is skipped. The first method to install pip is to bootstrap the pip installer into an existing Python installation. The ensurepip package is aimed for it. It's available starting from Python version 3.4. As all pip components are a part of Python package, the internet connection is not required to install pip. The second method installs pip manually and it does request internet connection. The method #1 is based on Synology DSM 6.2.2-24922 Update 4 and Python version is 3.5.1. Python 3.5.1 bin folder is /volume1/@appstore/py3k/usr/local/bin . Python 3.5.1 lib is located in /volume1/@appstore/py3k/usr/local/lib/python3.5 folder. The method #2 is tried with Synology DSM 6.2.3-25426 Update 3 and Python version is 3.8.2. Python 3.8.2 bin folder is /volume1/@appstore/py3k/usr/local/bin . Python 3.8.2 lib is located in /volume1/@appstore/py3k/usr/local/lib/python3.8 folder. 1. Validate Python3 installation and version. python3 -V 2. Install pip Run pip installation with admin privilege. A running Synology user has to belong to the administrator group. There are 2 options to proceed: (1) run commands as sudo or (2) switch to root with sudo -i . sudo python3 -m ensurepip If you receive the message: /usr/local/bin/python3: No module named ensurepip , it means the ensurepip package is not available and you have to go with method #2. Download get-pip.py package. wget https://bootstrap.pypa.io/get-pip.py -O /tmp/get-pip.py Install get-pip.py package. sudo python3 /tmp/get-pip.py 3. Upgrade pip to the latest version. Skip this step if it has been used the method #2. You might need to add an option to limit pip installation version if your Python version is old one and reached end-of-life. For example, Python 3.5 is not supported anymore and the added option is \"pip < 21.0\" . sudo python3 -m pip install --upgrade pip if your Python version is reached end-of-life. sudo python3 -m pip install --upgrade \"pip < 21.0\" 4. Validate pip installation and version. python3 -m pip -V 5. Install a package, for example, requests. sudo python3 -m pip install requests Resources Bootstrapping the pip installer","tags":"Synology DSM","url":"https://techjogging.com/installing-and-using-pip-on-synology-dsm.html","loc":"https://techjogging.com/installing-and-using-pip-on-synology-dsm.html"},{"title":"Create Ticket Cache File for Kerberos Authentication in Windows","text":"Kerberos ticket cache is one of the options to utilize Kerberos authentication in Windows. Another option is to use Kerberos keytab file . Kerberos ticket cache can be transparently consumed by many tools, whereas Kerberos keytab requests additional setup to plug in to tools. Kerberos ticket cache file default location and name are C:\\Users\\windowsuser\\krb5cc_windowsuser and mostly tools recognizes it. There are some tools and techniques to generate a ticket cache file. 1. Kinit Java tool Make sure that Java JRE or SDK or open source equivalent, for example, OpenJDK is installed. Run kinit tool located in C:\\Program Files\\Java\\jre[version]\\bin folder. The folder name depends on JRE or SDK or 32 or 64 bit edition. It's assumed java 8 is installed in C:\\Program Files\\Java\\jre1.8.0_192 folder. If Kerberos ticket cache is created for a user currently logged in to a Windows computer \"C:\\Program Files\\Java\\jre1.8.0_192\\kinit\" Output Password for windowsuser@SAMPLE.COM: New ticket is stored in cache file C:\\Users\\windowsuser\\krb5cc_windowsuser If Kerberos ticket cache is created for a different user from currently logged in to a Windows computer \"C:\\Program Files\\Java\\jre1.8.0_192\\kinit\" windowsuser@SAMPLE.COM Output Password for windowsuser@SAMPLE.COM: New ticket is stored in cache file C:\\Users\\windowsuser\\krb5cc_windowsuser Utilize Kerberos keytab file with kerberized Windows service account provided by your administrator. \"C:\\Program Files\\Java\\jre1.8.0_192\\kinit\" servicewindowsaccount@SAMPLE.COM -k -t C: \\k eytabfolder \\k eytabname.keytab Output New ticket is stored in cache file C:\\Users\\windowsuser\\krb5cc_windowsuser The created cache file can be validated with klist command \"C:\\Program Files\\Java\\jre1.8.0_192\\klist\" Output Credentials cache: C:\\Users\\windowsuser\\krb5cc_windowsuser Default principal: windowsuser@SAMPLE.COM, 1 entry found. [1] Service Principal: krbtgt/SAMPLE.COM@SAMPLE.COM Valid starting: Mar 26, 2020 21:35:00 Expires: Mar 27, 2020 07:35:00 2.MIT Kerberos software MIT Kerberos can be loaded from MIT Kerberos Distribution Page . It includes command line and GUI tools. Because of coming from Unix environment, it doesn't understand the default location and the location should be explicitly stated. If Kerberos ticket cache is created for a user currently logged in to a Windows computer \"C:\\Program Files\\MIT\\Kerberos\\bin\\kinit\" -c C: \\U sers \\w indowsuser \\k rb5cc_windowsuser No output. If Kerberos ticket cache is created for a different user from currently logged in to a Windows computer \"C:\\Program Files\\MIT\\Kerberos\\bin\\kinit\" -c C: \\U sers \\w indowsuser \\k rb5cc_windowsuser windowsuser@SAMPLE.COM No output. Utilize Kerberos keytab file with kerberized Windows service account provided by your administrator. \"C:\\Program Files\\MIT\\Kerberos\\bin\\kinit\" -k -t C: \\k eytabfolder \\k eytabname.keytab -c C: \\U sers \\w indowsuser \\k rb5cc_windowsuser servicewindowsaccount@SAMPLE.COM No output. The created cache file can be validated with klist command \"C:\\Program Files\\MIT\\Kerberos\\bin\\klist\" -c C: \\U sers \\w indowsuser \\k rb5cc_windowsuser Output Ticket cache: FILE:C:\\Users\\windowsuser\\krb5cc_windowsuser Default principal: windowsuser@SAMPLE.COM Valid starting Expires Service principal 05/09/20 22:39:22 05/10/20 08:39:22 krbtgt/krbtgt/SAMPLE.COM@SAMPLE.COM renew until 05/10/20 22:39:22 It can be applied some options to customize ticket cache, for example, -r renewable_life . MIT Kerberos Ticket Manager is GUI tool. It can be run from Windows Start menu or from desktop or C:\\Program Files\\MIT\\Kerberos\\bin\\MIT Kerberos.exe . Set up 'KRB5CCNAME' environment variable Open System Properties entering sysdm.cpl in Windows Start Go to Advanced tab and click Environment Variables... Add a new System Variable . Name: KRB5CCNAME and value: C:\\Users\\windowsuser\\krb5cc_windowsuser . Reboot computer to make it in effect. Run MIT Kerberos Ticket Manager Click Get Ticket and enter Principal and Password . Also, you can customize ticket properties. Validate ticket location in Credential Cache column or C:\\Users\\windowsuser\\krb5cc_windowsuser file.","tags":"Kerberos","url":"https://techjogging.com/create-ticket-cache-file-for-kerberos-authentication-in-windows.html","loc":"https://techjogging.com/create-ticket-cache-file-for-kerberos-authentication-in-windows.html"},{"title":"Multi-Character Field Delimiter in Apache Hive Table","text":"Multi-character field delimiter is not implemented in LazySimpleSerDe and OpenCSVSerde text file SerDe classes. There are two options to use multi-character field delimiter in Hive. The first option is MultiDelimitSerDe class specially developed to handle multi-character field delimiters. The second one is to use RegexSerDe class as a workaround. Those two options don't work as expected because of limitations. MultiDelimitSerDe implementation MultiDelimitSerDe SerDe is considered as experimental one until Hive release 4.0.0. It's included in hive-contrib-<version>.jar library and you have to add the library to the class path. If hive-contrib-<version>.jar library is not included in the class path, the functionality is limited to run only SELECT * FROM table_name; queries. The limitation is caused by map/reduce jobs which don't have access to the library. The issue should be fixed in Hive 4.0.0 when MultiDelimitSerDe class is supposed to be included in org.apache.hadoop.hive.serde2 library. Currently, MultiDelimitSerDe class is a part of org.apache.hadoop.hive.contrib.serde2 library. One more limitation is that skip header lines functionality ( TBLPROPERTIES (\"skip.header.line.count\"=\"1\") ) doesn't work. Sample of experimental version with ~| field delimiter. The code is run on Hive 1.1.0. CREATE EXTERNAL TABLE ` sample_table ` ( ` column1 ` string , ` column2 ` int , ` column3 ` decimal ( 10 , 2 ), ` column4 ` timestamp ) ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.MultiDelimitSerDe' WITH SERDEPROPERTIES ( \"field.delim\" = \"~|\" ); LOCATION '/folder/folder2' Sample of final version with ~| field delimiter. The code can't be validated. CREATE EXTERNAL TABLE ` sample_table ` ( ` column1 ` string , ` column2 ` int , ` column3 ` decimal ( 10 , 2 ), ` column4 ` timestamp ) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.MultiDelimitSerDe' WITH SERDEPROPERTIES ( \"field.delim\" = \"~|\" ); LOCATION '/folder/folder2' RegexSerDe implementation RegexSerDe SerDe limitation is to support only string data type in Hive tables. Also, It's expected performance overhead. Sample with ~| field delimiter. CREATE EXTERNAL TABLE ` sample_table ` ( ` column1 ` string , ` column2 ` string , ` column3 ` string , ` column4 ` string ) ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.RegexSerDe' WITH SERDEPROPERTIES ( \"input.regex\" = \"(.*)[~][|](.*)[~][|](.*)[~][|](.*)\" ) LOCATION '/folder/folder2' TBLPROPERTIES ( \"skip.header.line.count\" = \"1\" ); Resources MultiDelimitSerDe Use multiple-characters as field delimiter Include MultiDelimitSerDe in HIveServer2 By Default Hive contrib jar should not be in lib","tags":"Hive","url":"https://techjogging.com/multi-character-field-delimiter-in-apache-hive-table.html","loc":"https://techjogging.com/multi-character-field-delimiter-in-apache-hive-table.html"},{"title":"Field Delimiter in Apache Hive Table","text":"Not too much official documentation can be found on how to define a field delimiter in a create or an alter Apache Hive statement. This setting is requested for delimited text files placed as source of Hive tables. When a field delimiter is not assigned properly, Hive can't split data into columns, and as a result, the first column will contain all data and the rest of columns will have NULL values. Also, it's critical to know a default field delimiter if field delimiter setting is missed in a create statement. There are 2 major SerDe (Serializer/Deserializer) classes for text data. SerDe defines input/output (IO) interface which handles: (1) read data from a Hive table and (2) write it back out to HDFS. org.apache.hadoop.hive.serde2 is the Hive SerDe library including TEXTFILE formats. org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe . The default field delimiter value is '\\001' . org.apache.hadoop.hive.serde2.OpenCSVSerde The default field delimiter value is ',' . LazySimpleSerDe is more efficient in terms of performance. OpenCSVSerde has a limitation to handle only string data type in Hive tables. The default format is LazySimpleSerDe . The main issue with field delimiter is that Java char data type is used as an argument to assign a field delimiter. It can hold only 2 bytes. Java char data type can understand both ASCII and Unicode characters but it can handle Unicode characters which belong to ASCII table. Characters of the first part of ASCII table with codes from 0 to 127 are only accepted as field delimiters. If you need to use the extended ASCII character from 128 to 255 codes, it should be used other SerDe classes, for example, org.apache.hadoop.hive.contrib.serde2.RegexSerDe . The rules to assign a filed delimiter are. Any visible ASCII character can be assigned directly, for example, '1' , 'a' , or '!' . It can be used special predefined characters, for example. '\\t' , '\\r' , and '\\n' . If a character belongs to ASCII set and invisible, it can be used octal or Unicode notations. Octal starts from back slash and contains 3 digits, for example, '\\001' . Character 'a' is '\\040' . Hex has '\\u' prefix and includes 4 digits. It represents a Unicode code but you have to use decimal ASCII code, for example, '\\u0010' definition is converted to '\\000a' Hive table field delimiter. Another sample is visible ASCII character 'a' , '\\u0032' field delimiter definition is converted to '\\0020' in Hive table. Those commands can be used to retrieve field delimiter for a table from Hive meta data. Show statement. SHOW CREATE TABLE sample_table_name ; Describe statement #1. DESCRIBE FORMATTED sample_table_name ; Describe statement #2. DESCRIBE EXTENDED sample_table_name ; Field delimiter can be assigned or changed in those Hive statements. CREATE statement with LazySimpleSerDe interface. CREATE TABLE sample ( column1 string , column2 string , column3 string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\n' STORED AS TEXTFILE LOCATION '/folder1/folder2' CREATE statement with OpenCSVSerde interface. CREATE TABLE sample ( column1 string , column2 string , column3 string ) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' WITH SERDEPROPERTIES ( 'separatorChar' = ',' ) STORED AS TEXTFILE LOCATION '/folder1/folder2' ALTER statement with LazySimpleSerDe interface. ALTER TABLE multi_char_field_delim SET SERDEPROPERTIES ( 'field.delim' = ',' ) ALTER statement with OpenCSVSerde interface. ALTER TABLE multi_char_field_delim SET SERDEPROPERTIES ( 'separatorChar' = ',' ) or ALTER TABLE multi_char_field_delim SET SERDEPROPERTIES ( 'field.delim' = ',' )","tags":"Hive","url":"https://techjogging.com/field-delimiter-in-apache-hive-table.html","loc":"https://techjogging.com/field-delimiter-in-apache-hive-table.html"},{"title":"Create OAuth Credentials for Google Analytics APIs","text":"One of the ways to implement Google Analytics in your tools or Web sites is to utilize Google Analytics APIs. This way is very flexible as Google Analytics APIs can be used with wide range of the programming languages. Moreover, APIs are mature product which on the market for many years. The latest v4 contains rich set of functionalities which was forged from earliest versions. It's back compatible with previous v3. The first step to start using Google Analytics APIs includes creating of an OAuth credentials. There are many options to proceed with it but we follow a route to generate credentials for a Web site with javascript implementation. It's applicable to other scenarios as well. The article is based on API v4 and includes a sample of code to test your setup. The sample is extracted from Google JavaScript quickstart for web applications . The original code was modified to show error messages in case of encountering any issues and logout functionality. 1. Create a Google account All Google tools request a Google account. If you already have a Google email, it can be used as your Google account otherwise follow link . 2. Open Google Developer Console Google Developer Console is used to create an OAuth credentials. To open the tool, sign in with your Google account, and then agree to Terms of Service. 3. Create a new API project Type in your project name. If you create a new project for your personal usage, leave Location with No organization default value. Your project name is the current one. 4. Enable Google Analytics APIs Click ENABLE APIS AND SERVICES button. Search for Google Analytics Reporting APIs from the list of available APIs. Confirm your intention. 5. Configure Consent Select Credentials menu item from the dashboard. Initiate configuration of consent clicking CONFIGURE CONSENT SCREEN . If you left Location with No organization value, you have only External User Type option enabled. Enter information about your Web site. Final screen of the configuration. 6. Create an OAuth credentials Go back to Credentials screen and click CREATE CREDENTIALS button, and then select OAuth client ID. Select Web application type, type in your application name, and enter your website. If you test it, you can enter http://localhost.com. Confirmation screen. Your OAuth credentials created. OAuth credentials information can be retrieved if you click on the name of OAuth 2.0 Client ID entry. 7. Grant Account Explorer access to your Google Account Open Account Explorer . Allow Account Explorer access your Google Account. Select your Account, Property, and View. View field contain VIEW_ID requested by Google Analytics API. 8. API v4 Sample The sample shows a number of sessions for the last 8 days. Create an HTML file on your Web server replacing (1) client_id with yours in <meta name=\"google-signin-client_id\" content=\"109743573222-tu7960r1m6kam5acmigfumlqebf016cf.apps.googleusercontent.com\"> line and (2) VIEW_ID in var VIEW_ID = '209816969'; line, and then open it in browser. It will ask for your Google account credentials associated with your Google Analytics. <!DOCTYPE html> < html > < head > < meta charset = \"utf-8\" > < title > Analytics Reporting API V4 Sample </ title > < meta name = \"google-signin-client_id\" content = \"109743573222-tu7960r1m6kam5acmigfumlqebf016cf.apps.googleusercontent.com\" > < meta name = \"google-signin-scope\" content = \"https://www.googleapis.com/auth/analytics.readonly\" > </ head > < body > < h1 > Analytics Reporting API V4 Sample </ h1 > <!-- The Sign-in button. This will run `queryReports()` on success. --> < p class = \"g-signin2\" data-onsuccess = \"queryReports\" ></ p > <!-- The Sign-out button. --> < button onclick = \"signOut()\" > Logout </ button > <!-- The API response will be printed here. --> < h3 > API response </ h3 > < textarea cols = \"80\" rows = \"10\" id = \"query-output\" ></ textarea > <!-- The errors will be printed here. --> < h3 > Error </ h3 > < textarea cols = \"80\" rows = \"5\" id = \"query-error\" ></ textarea > < script > // Replace with your view ID. var VIEW_ID = '209816969' ; // Query the API and print the results to the page. function queryReports () { try { document . getElementById ( 'query-error' ). value = '' ; gapi . client . request ({ path : '/v4/reports:batchGet' , root : 'https://analyticsreporting.googleapis.com/' , method : 'POST' , body : { reportRequests : [ { viewId : VIEW_ID , dateRanges : [ { startDate : '7daysAgo' , endDate : 'today-1' } ], metrics : [ { expression : 'ga:sessions' } ] } ] } }). then ( displayResults , displayErrors ); } catch ( err ) { document . getElementById ( 'query-error' ). value = err ; } } function displayResults ( response ) { var formattedJson = JSON . stringify ( response . result , null , 2 ); document . getElementById ( 'query-output' ). value = formattedJson ; } function displayErrors ( reason ) { document . getElementById ( 'query-error' ). value = reason . result . error . message ; } function signOut () { gapi . auth2 . getAuthInstance (). disconnect () document . getElementById ( 'query-output' ). value = '' ; document . getElementById ( 'query-error' ). value = '' ; } </ script > <!-- Load the JavaScript API client and Sign-in library. --> < script src = \"https://apis.google.com/js/client:platform.js\" ></ script > </ body > </ html > Response is { \"reports\" : [ { \"columnHeader\" : { \"metricHeader\" : { \"metricHeaderEntries\" : [ { \"name\" : \"ga:sessions\" , \"type\" : \"INTEGER\" } ] } }, \"data\" : { \"rows\" : [ { \"metrics\" : [ { \"values\" : [ \"43\" ] } ] } ], \"totals\" : [ { \"values\" : [ \"43\" ] } ], \"rowCount\" : 1 , \"minimums\" : [ { \"values\" : [ \"43\" ] } ], \"maximums\" : [ { \"values\" : [ \"43\" ] } ] } } ] }","tags":"Google Analytics","url":"https://techjogging.com/create-oauth-credentials-for-google-analytics-apis.html","loc":"https://techjogging.com/create-oauth-credentials-for-google-analytics-apis.html"},{"title":"Fast Record Count in Table or Tables in Database in SQL Server","text":"As data size is growing each year, count record in a table takes more and more time if you use old fashion methods, for example, SELECT COUNT(*) FROM table . It's the most reliable method to get record count but if you deal with multi-billion record table, it might take hours to get your result. Also, you need to consider that COUNT(*) operator is resource consuming as MS SQL Server has to scan every record in worst scenario. Moreover, COUNT(*) calculation locks a table which impacts on overall performance. Meta data is a good source of information without workloading our SQL Server and getting results in no time. One drawback is that we might get \"dirty\" numbers because meta data is not aware of transactions which might occur during retrieving of record count. If we look at \"dirty\" numbers from another point of view, we can accept it. Let's imagine that we need to check our progress on inserting record in a table, we don't need exact number, we need to get a number to evaluate our current progress. Another case is when we have completed data transformation on a big table, we need to validate our result with record count. We know exactly that we don't do any data modification on the table and we are safe to use meta data. To get record count fast, we need to consider a case when meta data might be locked and getting record count quickly will be problematic. It can be solved by reading uncommitted (\"dirty\") data with NOLOCK or READUNCOMMITTED hint. An exception is when TABLOCK hint is used on your table. It will lock access to the table completely. 1. Count Records in a Table SELECT SCHEMA_NAME ( schema_id ) + '.' + t . name AS [ Table Name ], FORMAT ( SUM ( p .[ rows ]), '#,#' ) AS [ Row Count ] FROM sys . tables t WITH ( NOLOCK ) JOIN sys . partitions p WITH ( NOLOCK ) ON t .[ object_id ] = p .[ object_id ] AND p . index_id IN ( 0 , 1 ) WHERE t . name = '$(Table name)' GROUP BY t .[ schema_id ] , t .[ name ]; Replace $(Table name) place holder with your table name. 2. Count Record in User Tables in a Database DECLARE @ SqlText NVARCHAR ( MAX ); SELECT @ SqlText = COALESCE ( @ SqlText + CHAR ( 13 ) + 'UNION ALL' + CHAR ( 13 ), '' ) + 'SELECT SCHEMA_NAME(schema_id) + ''.'' + t.name AS [Table Name],' + 'FORMAT(SUM(p.[rows]), ''#,#'') AS [Row Count] ' + 'FROM sys.tables t WITH (NOLOCK) JOIN sys.partitions p WITH (NOLOCK) ON t.[object_id] = p.[object_id] ' + 'AND p.index_id IN (0, 1) ' + 'WHERE t.name=''' + l .[ name ] + ''' ' + 'AND t.[schema_id]=' + CAST ( l .[ schema_id ] AS VARCHAR ( 10 )) + ' ' + 'GROUP BY [schema_id]' + ',t.[name]' FROM sys . tables l WITH ( NOLOCK ) WHERE [ type ] = 'U' ORDER BY SCHEMA_NAME ([ schema_id ]) ,[ name ]; EXEC sp_executesql @ SqlText","tags":"MS SQL Server","url":"https://techjogging.com/fast-record-count-in-table-or-tables-in-database-in-sql-server.html","loc":"https://techjogging.com/fast-record-count-in-table-or-tables-in-database-in-sql-server.html"},{"title":"Redirect 404 Error to Specified URL in Synology DSM","text":"Synology DiskStation Manager (DSM) restricts flexibility of Web Station customization. Access to many settings in Apache or Nginx Web servers are hidden or not available. There are some open doors to accomplish your customization without breaking Synology DSM. If you want to address 404 error with your way, it's still possible. The sample below is based on Synology DSM 6.2.2. and it shows 404 error page, and then load a Web page of your choice. You can customize the logic eliminating showing of 404 error page at all and go straight to your page. 1. Create a file with missing name. Don't use any extensions in file name. It has to be the bare name. The file has to be place in root of your Web site. 2. Add HTML content in the missing file. The code is extracted from Synology DSM 404 error page. <!DOCTYPE html> < html > < head > < meta charset = \"utf-8\" > < meta http-equiv = \"refresh\" content = \"2; url = https://techjogging.com\" /> < style > html { height : 100 % } body { margin : 0 auto ; min-height : 600 px ; min-width : 800 px ; height : 100 % }. top { height : 100 px ; height : calc ( 40 % - 140 px )}. bottom { height : 150 px ; height : calc ( 60 % - 210 px )}. center { height : 350 px ; text-align : center ; vertical-align : middle ; font- family : Verdana }. circle { margin : auto ; width : 260 px ; height : 260 px ; border-radius : 50 % ; background : #c0c6cc }. circle_text { line-height : 260 px ; font-size : 100 px ; color : #ffffff ; font-weight : bold }. text { line-height : 40 px ; font-size : 26 px ; color : #505a64 } </ style > </ head > < body > < div class = \"top\" ></ div > < div class = \"center\" > < div class = \"circle\" > < div class = \"circle_text\" > 404 </ div > </ div > < div > < p class = \"text\" id = \"a\" > The page you are looking for cannot be found </ p > </ div > < div class = \"bottom\" ></ div > </ body > </ html > 3. Replace parameters. < meta http-equiv = \"refresh\" content = \"2; url = https://techjogging.com\" /> Delay before redirecting to your URL. Your URL.","tags":"Synology DSM","url":"https://techjogging.com/redirect-404-error-to-specified-url-in-synology-dsm.html","loc":"https://techjogging.com/redirect-404-error-to-specified-url-in-synology-dsm.html"},{"title":"Redirect WWW to HTTPS in Synology DSM Nginx","text":"WWW website prefix has long history and it was used to classify information what was exposed to users of websites. Another usage of WWW might be for load balancing when web traffic is redirected to a cluster of web servers. Also, some sources tell that WWW was introduced accidently as a mistake but everybody considered it as the new standard and started using it. WWW is not required to be used in URLs. In any case, we need to address this prefix to keep back compatibility with the old rules. It means that www.sample.com, https://www.sample.com, and https://www.sample.com URLs should be valid and converted to https://www.sample.com one to make a Web connection secured. 1. Add WWW CNAME record to DSN. The sample is based on noip.com DSN service. 2. Create SSL Certificate in Synology DiskStation Manager (DSM) The easiest way to create a SSL certificate is Synology DSM which supports Let's Encrypt natively. Make sure to add WWW to Subject Alternative Name . 3. Enable SSH service. 4. Install Web Station. 5. Make Nginx Web Server as Default. 6. Modify Moustache Template Sample is based on DSM 6.2.2 operation system and the original moustache template is. server { listen 80 default_server{{#reuseport}} reuseport{{/reuseport}}; listen [::]:80 default_server{{#reuseport}} reuseport{{/reuseport}}; gzip on; {{> /usr/syno/share/nginx/WWW_Main}} location ~ &#94;/$ { rewrite / http://$host:{{DSM.port}}/ redirect; } } server { listen 443 default_server ssl{{#reuseport}} reuseport{{/reuseport}}; listen [::]:443 default_server ssl{{#reuseport}} reuseport{{/reuseport}}; {{#DSM.https.compression}} gzip on; {{/DSM.https.compression}} {{> /usr/syno/share/nginx/WWW_Main}} location ~ &#94;/$ { rewrite / https://$host:{{DSM.ssl.port}}/ redirect; } } Use ssh client to access your Synology NAS with a user which has administrative permission. It can be PuTTY tool in Windows or terminal with ssh command in Unix. Switch to root user sudo su - Back up the current moustache template cp /usr/syno/share/nginx/WWWService.mustache /usr/syno/share/nginx/WWWService.mustache.bak Open the moustache template for editing vi /usr/syno/share/nginx/WWWService.mustache Replace 4 lines in port 80 section {{> /usr/syno/share/nginx/WWW_Main}} location ~ &#94;/$ { rewrite / http://$host:{{DSM.port}}/ redirect; } with those 2 lines server_name _; return 301 https://$host$request_uri; The final content should be server { listen 80 default_server{{#reuseport}} reuseport{{/reuseport}}; listen [::]:80 default_server{{#reuseport}} reuseport{{/reuseport}}; gzip on; server_name _; return 301 https://$host$request_uri; } server { listen 443 default_server ssl{{#reuseport}} reuseport{{/reuseport}}; listen [::]:443 default_server ssl{{#reuseport}} reuseport{{/reuseport}}; {{#DSM.https.compression}} gzip on; {{/DSM.https.compression}} {{> /usr/syno/share/nginx/WWW_Main}} location ~ &#94;/$ { rewrite / https://$host:{{DSM.ssl.port}}/ redirect; } } Restart the Nginx web server to apply the changes synoservicecfg --restart nginx The last important step is to refresh your browser. When you open your web site with http , it's still showing as http and don't redirect to https . Just click Ctrl-F5 .","tags":"Synology DSM","url":"https://techjogging.com/redirect-www-to-https-in-synology-nas-nginx.html","loc":"https://techjogging.com/redirect-www-to-https-in-synology-nas-nginx.html"},{"title":"Connect DBeaver to MS SQL Server with JAAS Using Kerberos Keytab in Windows","text":"Kerberos authentication can be established by applying Kerberos ticket cache or keytab file. Kerberos ticket cache method has a disadvantage, ticket cache should be renewed on regular basis. If it is not automatic process, it will request our attention. Another option is to use Kerberos keytab file. First of all, it has to be regenerated only in case of changing password for kerberized account. It's beneficial when a service account is involved. In that scenario, an administrator might maintain keytab file and distribute it to users. Also, keytab file be created by users. 1. Create Kerberos keytab File Keytab file can be created following Create keytab File for Kerberos Authentication in Windows article. 2. Create JAAS Configuration File JAAS file stores Kerberos setup used by Microsoft JDBC Driver for SQL Server driver. Krb5LoginModule module authenticates users using Kerberos protocols. SQLJDBCDriver { com.sun.security.auth.module.Krb5LoginModule required doNotPrompt = true useKeyTab = true keyTab = \"keytabname.keytab\" useTicketCache = false renewTGT = false principal = \"windowsserviceaccount@SAMPLE.COM\"; }; Make your changes in the sample: Replace keyTab=\"keytabname.keytab\" setting with your keytab file name. Keytab file name without path means that the file is stored in root folder of your DBeaver installation, for example, C:\\dbeaver-ce-7.0.0-win32.win32.x86_64 . Make sure that you use double back slash (\\) or forward slash (/) in file path, for example, C:\\\\kerberos\\\\keytabname.keytab or C:/kerberos/keytabname.keytab Replace principal=\"windowsserviceaccount@SAMPLE.COM\" setting with your service account name which kerberized in keytab file and your default realm. 3. DBeaver Kerberos Setup Add JAAS file name and location to dbeaver.ini DBeaver configuration file -Djava.security.auth.login.config = jaas.conf The file name is jaas.conf and the location is a root folder of DBeaver installation, for example, C:\\dbeaver-ce-7.0.0-win32.win32.x86_64 . Optionally, it can be added a command to relax the usual restriction of requiring a GSS mechanism . Your network security configuration might make it as mandatory. -Djavax.security.auth.useSubjectCredsOnly = false -Djava.security.auth.login.config = jaas.conf Optionally, krb5.ini Kerberos configuration file can be added. The file can be obtained from your administrator. -Djavax.security.auth.useSubjectCredsOnly = false -Djava.security.krb5.conf = krb5.ini -Djava.security.auth.login.config = jaas.conf The file location is a root folder of DBeaver installation, for example, C:\\dbeaver-ce-7.0.0-win32.win32.x86_64 . This is a sample of the final dbeaver.ini file for DBeaver Community edition version 7.0.0. -startup plugins/org.eclipse.equinox.launcher_1.5.600.v20191014-2022.jar --launcher.library plugins/org.eclipse.equinox.launcher.win32.win32.x86_64_1.1.1100.v20190907-0426 -vmargs -XX:+IgnoreUnrecognizedVMOptions --add-modules = ALL-SYSTEM -Xms64m -Xmx1024m -Djavax.security.auth.useSubjectCredsOnly = false -Djava.security.krb5.conf = krb5.ini -Djava.security.auth.login.config = jaas.conf 4. Add New MS SQL Connection Open New Database Connection wizard from Database menu and select MS SQL Server driver. On the next step, replace Host with your server name and choose Kerberos from Authentication list. Click Finish button to complete wizard. Next screen will request to download drivers. Just click Download button. Be patient, it takes time to retrieve SQL server meta data. 5. Upgrade MS SQL Drivers It's an optional step if you are able to connect to MS SQL Server. Download Microsoft JDBC Driver for SQL Server Unzip driver and store in C:\\Users\\windowsuser\\AppData\\Roaming\\DBeaverData\\drivers DBeaver settings folder, for example, C:\\Users\\windowsuser\\AppData\\Roaming\\DBeaverData\\drivers\\sqljdbc_8.2 Edit created connection Click Edit Driver Settings button Delete all files in Libraries tab Click Add File button and select downloaded files a) JDBC driver corresponding your java version, for example, java 1.8 - mssql-jdbc-8.2.2.jre8.jar. b) Authentication library. It should be 64 bit - mssql-jdbc_auth-8.2.2.x64.dll. Click OK button twice to complete setup. Restart DBeaver to apply new drivers.","tags":"DBeaver","url":"https://techjogging.com/connect-dbeaver-to-ms-sql-server-with-jaas-using-kerberos-keytab-file-in-windows.html","loc":"https://techjogging.com/connect-dbeaver-to-ms-sql-server-with-jaas-using-kerberos-keytab-file-in-windows.html"},{"title":"Connect DBeaver to MS SQL Server with Kerberos Ticket Cache in Windows","text":"The easiest way to connect DBeaver to MS SQL Server with Kerberos authentication is Kerberos ticket cache. It requests only 2 steps. The first step is to create Kerberos ticket cache and the second one is to add a new connection to DBeaver with default settings. There are variety of tool in Windows to create Kerberos ticket cache. Also, you can apply different techniques to generate ticket cache. 1. Create Kerberos Ticket Cache File See Create Ticket Cache File for Kerberos Authentication in Windows article. 2. Add New MS SQL Connection Open New Database Connection wizard from Database menu and select MS SQL Server driver. On the next step, replace Host with your server name and choose Kerberos from Authentication list. Click Finish button to complete wizard. Next screen will request to download drivers. Just click Download button. Be patient, it takes time to retrieve SQL server meta data. 3. Upgrade MS SQL Drivers It's an optional step if you are not able to connect to MS SQL Server. Download Microsoft JDBC Driver for SQL Server Unzip driver and store in C:\\Users\\windowsuser\\AppData\\Roaming\\DBeaverData\\drivers DBeaver settings folder, for example, C:\\Users\\windowsuser\\AppData\\Roaming\\DBeaverData\\drivers\\sqljdbc_8.2 Edit created connection Click Edit Driver Settings button Delete all files in Libraries tab Click Add File button and select downloaded files a) JDBC driver corresponding your java version, for example, java 1.8 - mssql-jdbc-8.2.2.jre8.jar. b) Authentication library. It should be 64 bit - mssql-jdbc_auth-8.2.2.x64.dll. Click OK button twice to complete setup. Restart DBeaver to apply new drivers.","tags":"DBeaver","url":"https://techjogging.com/connect-dbeaver-to-ms-sql-server-with-kerberos-ticket-cache-in-windows.html","loc":"https://techjogging.com/connect-dbeaver-to-ms-sql-server-with-kerberos-ticket-cache-in-windows.html"},{"title":"Enable gzip Compression in Synology DSM in Nginx","text":"Enabling gzip compression for your website can be done in Synology DiskStation Manager (DSM). HTTP traffic is already compressed with gzip as default. HTTPS protocol needs to be enabled explicitly. Synology DSM is doing it in an easy step without rebooting Synology NAS. Also, Synology DSM includes text/javascript and text/css MIME types additional to text/html one. 1. Validate Compression 1.1 Firefox Web browser Open your website in Firefox browser Click Shift-F12 to open Developer Tools panels. Go to Network panel Click Reload button in the panel or Ctrl-F5 buttons to initiate your website data load. Filter resources which you are interested in. Those resources are: HTML , CSS , and JS , for example, HTML website root page. Click on a resource to see compression and other resource properties, for example, HTML resource. Clear cache. Gzip compression is set up. 1.2 Chrome Web browser The steps to get information are very similar to Firefox. To open Chrome DevTools, press Command+Option+C (Mac) or Ctrl+Shift+C (Windows, Linux, Chrome OS). 2. Enable gzip Compression in DSM Open Control Panel in Synology DiskStation Manager. Go to Security settings. Select Advanced tab and flag Enable HTTP Compression . Apply the setting pressing Save button. 3. Include Different MIME Types Compression is applied to HTML MIME type as default. Also, compression is applied to CSS and JS in Synology DSM. If compression is missing for CSS and JS resources, it can be set up. The sample is based on DSM 6.2.2 operation system. Use ssh client to access your Synology NAS with a user which has administrative permission. It can be PuTTY tool in Windows or terminal with ssh command in Unix. Switch to root user. sudo su - Back up the current moustache template. cp /usr/syno/share/nginx/WWWService.mustache /usr/syno/share/nginx/WWWService.mustache.bak Open the moustache template for editing. vi /usr/syno/share/nginx/WWWService.mustache Add a line after gzip on; command to both HTTP and HTTPS server sections. gzip_types text/javascript text/css; The final content should be. server { listen 80 default_server{{#reuseport}} reuseport{{/reuseport}}; listen [::]:80 default_server{{#reuseport}} reuseport{{/reuseport}}; gzip on; gzip_types text/javascript text/css; {{> /usr/syno/share/nginx/WWW_Main}} location ~ &#94;/$ { rewrite / http://$host:{{DSM.port}}/ redirect; } } server { listen 443 default_server ssl{{#reuseport}} reuseport{{/reuseport}}; listen [::]:443 default_server ssl{{#reuseport}} reuseport{{/reuseport}}; {{#DSM.https.compression}} gzip on; gzip_types text/javascript text/css; {{/DSM.https.compression}} {{> /usr/syno/share/nginx/WWW_Main}} location ~ &#94;/$ { rewrite / https://$host:{{DSM.ssl.port}}/ redirect; } } Restart the Nginx web server to apply the changes. synoservicecfg --restart nginx","tags":"Synology DSM","url":"https://techjogging.com/enable-gzip-compression-in-synology-nas-in-nginx.html","loc":"https://techjogging.com/enable-gzip-compression-in-synology-nas-in-nginx.html"},{"title":"Create Keytab for Kerberos Authentication in Windows","text":"There are two ways to utilize Kerberos authentication: Kerberos ticket cache and Kerberos keytab. Windows has a limited set of tools to create a keytab file. There are a couple of tools for this purpose. One tool is the Windows Server built-in utility ktpass . It can be only run on a Windows Server. Another tool is ktab which can be used on any Windows computer. ktab tool is a part of Java installation. 1. ktpass There are some restrict requirements to run the tool. It must be run on either a member server or a domain controller of the Active Directory domain. Windows Server operating system such as Windows Server 2008, 2012, or 2016 are supported. When running ktpass.exe, Windows Command Prompt must be run with Run as administrator option. ktpass -princ [ Windows user name ] @ [ Realm name ] -pass [ Password ] -crypto [ Encryption type ] -ptype [ Principle type ] -kvno [ Key version number ] -out [ Keytab file path ] [Windows user name] - mywindowsname. [Real name] - SAMPLE.COM. [Password] - mywindowsname user password. [Encryption type] - RC4-HMAC-NT. See RFC 3961, section 8 . [Principle type] - KRB5_NT_PRINCIPAL which is Kerberos protocol 5. [Key version number] - 0. [Keytab file path] - c:\\kerberos\\keytabname.keytab. 2. ktab It requests to install Java JRE or SDK or open source equivalent, for example, OpenJDK. The tool has a limited set of options. It can't be defined encryption and principle types. It will be used Kerberos protocol 5 and it will be created multiple encryption types. ktab -a [ Windows user name ] @ [ Realm name ] [ Password ] -n [ Key version number ] -k [ Keytab file path ] List all encryption types stored in a keytab file ktab -l -e -k [ Keytab file path ] If multiple encryption types are not accepted in authentication process, it can be left one encryption type and the rest can be deleted. ktab -d [ Windows user name ] @ [ Realm name ] -f -e [ Number of encryption type ] -k [ Keytab file path ] [Number of encryption type] - 16. As per RFC 3961, section 8. 3. Usage Samples 3.1. DBeaver connection to Hive with Kerberos Authentication It can be created multiple encryption types in a keytab file. Create a keytab file ktab -a mywindowsname@SAMPLE.COM mypassword -n 0 -k c: \\k erberos \\m ywindowsname.keytab Done! Service key for mywindowsname@SAMPLE.COM is saved in c: \\k erberos \\m ywindowsname.keytab List content of the keytab file ktab -l -e -k c: \\k erberos \\m ywindowsname.keytab Keytab name: c: \\k erberos \\m ywindowsname.keytab KVNO Principal ---- --------------------------------------------------------------- 0 mywindowsname@SAMPLE.COM ( 18 :AES256 CTS mode with HMAC SHA1-96 ) 0 mywindowsname@SAMPLE.COM ( 17 :AES128 CTS mode with HMAC SHA1-96 ) 0 mywindowsname@SAMPLE.COM ( 16 :DES3 CBC mode with SHA1-KD ) 0 mywindowsname@SAMPLE.COM ( 23 :RC4 with HMAC ) 3.2. Talend tHDFSConnection Component with Kerberos Authentication It should be one encryption type in a keytab file, for example, 23. Create a keytab file ktab -a mywindowsname@SAMPLE.COM mypassword -n 0 -k c: \\k erberos \\m ywindowsname.keytab Done! Service key for mywindowsname@SAMPLE.COM is saved in c: \\k erberos \\m ywindowsname.keytab List content of the keytab file ktab -l -e -k c: \\k erberos \\m ywindowsname.keytab Keytab name: c: \\k erberos \\m ywindowsname.keytab KVNO Principal ---- --------------------------------------------------------------- 0 mywindowsname@SAMPLE.COM ( 18 :AES256 CTS mode with HMAC SHA1-96 ) 0 mywindowsname@SAMPLE.COM ( 17 :AES128 CTS mode with HMAC SHA1-96 ) 0 mywindowsname@SAMPLE.COM ( 16 :DES3 CBC mode with SHA1-KD ) 0 mywindowsname@SAMPLE.COM ( 23 :RC4 with HMAC ) Delete unused encryption types ## 16-18 ktab -d mywindowsname@SAMPLE.COM -f -e 16 -k c: \\k erberos \\m ywindowsname.keytab Done! 1 entries removed. ktab -d mywindowsname@SAMPLE.COM -f -e 17 -k c: \\k erberos \\m ywindowsname.keytab Done! 1 entries removed. ktab -d mywindowsname@SAMPLE.COM -f -e 18 -k c: \\k erberos \\m ywindowsname.keytab Done! 1 entries removed. List content of the keytab file again ktab -l -e -k c: \\k erberos \\m ywindowsname.keytab Keytab name: c: \\k erberos \\m ywindowsname.keytab KVNO Principal ---- --------------------------------------------------------------- 0 mywindowsname@SAMPLE.COM ( 23 :RC4 with HMAC ) 3.3. Windows It depends on Windows account settings how many encryption types and what types can be used. Windows account properties dialog contains the next options for Kerberos authentication. 4. Encryption types As per RFC 3961, section 8 . Encryption Type Code Section or Comment des-cbc-crc 1 6.2.3 des-cbc-md4 2 6.2.2 des-cbc-md5 3 6.2.1 [reserved] 4 des3-cbc-md5 5 [reserved] 6 des3-cbc-sha1 7 dsaWithSHA1-CmsOID 9 (pkinit) md5WithRSAEncryption-CmsOID 10 (pkinit) sha1WithRSAEncryption-CmsOID 11 (pkinit) rc2CBC-EnvOID 12 (pkinit) rsaEncryption-EnvOID 13 (pkinit from PKCS#1 v1.5) rsaES-OAEP-ENV-OID 14 (pkinit from PKCS#1 v2.0) des-ede3-cbc-Env-OID 15 (pkinit) des3-cbc-sha1-kd 16 6.3 aes128-cts-hmac-sha1-96 17 [KRB5-AES] aes256-cts-hmac-sha1-96 18 [KRB5-AES] rc4-hmac 23 (Microsoft) rc4-hmac-exp 24 (Microsoft) subkey-keymaterial 65 (opaque; PacketCable)","tags":"Kerberos","url":"https://techjogging.com/create-keytab-file-for-kerberos-authentication-in-windows.html","loc":"https://techjogging.com/create-keytab-file-for-kerberos-authentication-in-windows.html"},{"title":"Build DBeaver Installation Package to Access Hive with JAAS Using Kerberos Authentication in Windows","text":"When your organization has decided to start using DBeaver, you need to plan how to deploy and setup DBeaver for users. It might not be an issue if you are going to utilize databases/drivers which included in the standard configuration. But let's imagine case when you need to connect to Hive with Kerberos authentication. This task request advanced skills and time for troubleshooting in case of encountering any issues. Setup The following sample based on Cloudera Hive JDBC driver v. 2.6.5.1007 and DBeaver EE v. 6.3.0 64 bit or DBeaver CE v. 7.0.0 64 bit edition. Download Windows 64 bit (zip) DBeaver installation and unzip it, for example, to C: drive. Your root folder is C:\\dbeaver-ce-7.0.0-win32.win32.x86_64 . Download and unzip Cloudera Hive JDBC driver. Place it in C:\\dbeaver-ce-7.0.0-win32.win32.x86_64\\DBeaverData\\drivers folder. C:\\dbeaver-ce-7.0.0-win32.win32.x86_64\\DBeaverData └───drivers └───hive_jdbc_2.6.5.1007 ├───ClouderaHiveJDBC41-2.6.5.1007 │ HiveJDBC41.jar │ └───docs Cloudera-JDBC-Driver-for-Apache-Hive-Install-Guide.pdf Cloudera-JDBC-Driver-for-Apache-Hive-Release-Notes.txt third-party-licenses.txt Copy or modify the following configuration files in C:\\dbeaver-ce-7.0.0-win32.win32.x86_64 DBeaver root folder. You can figure out how make them from Connect DBeaver to Cloudera Hive with JAAS Configuration using Kerberos Authentication in Windows article. 1) dbeaver.ini 2) jaas.conf 3) krb5.ini 4) organization-service-account.keytab Create dbeaver.vbs file and placed in C:\\dbeaver-ce-7.0.0-win32.win32.x86_64 DBeaver root folder as well. This file is a DBeaver launcher and our goal is to redirect DBeaver workspace from Windows User Home folder to DBeaver root one. We could create Windows batch file but it would show Command Prompt window during DBeaver start. Set WshShell = CreateObject ( \"WScript.Shell\" ) WshShell . Run \"dbeaver.exe -data ./DBeaverData/workspace6\" , 0 Set WshShell = Nothing Run DBeaver with dbeaver.vbs launcher and create a Hive driver and a new Hive connection as per Connect DBeaver to Cloudera Hive with JAAS Configuration using Kerberos Authentication in Windows article. After this step, it has to be created workspace folders by DBeaver. C:\\dbeaver-ce-7.0.0-win32.win32.x86_64\\DBeaverData └───workspace6 ├───.metadata │ ├───.plugins │ │ ├───org.eclipse.core.resources │ │ │ ├───.history │ │ │ ├───.projects │ │ │ │ └───General │ │ │ ├───.root │ │ │ │ └───.indexes │ │ │ └───.safetable │ │ ├───org.eclipse.core.runtime │ │ │ └───.settings │ │ ├───org.eclipse.e4.ui.workbench.swt │ │ ├───org.eclipse.e4.workbench │ │ ├───org.eclipse.ui.workbench │ │ ├───org.jkiss.dbeaver.core │ │ │ └───security │ │ ├───org.jkiss.dbeaver.model │ │ └───org.jkiss.dbeaver.ui │ └───qmdb └───General ├───.dbeaver └───Scripts The final step is to zip C:\\dbeaver-ce-7.0.0-win32.win32.x86_64 DBeaver root folder. Your zipped file name is dbeaver-ce-7.0.0-win32.win32.x86_64.zip . Deployment and Setup Users get dbeaver-ce-7.0.0-win32.win32.x86_64.zip file and they need to unzip it in a desired folder. If DBeaver has been set up in a network folder, it will be one limitation. You need to use mapped driver to run DBeaver, for example, Y:\\myuser\\desktop\\dbeaver-ce-7.0.0-win32.win32.x86_64 . It will not work if you run from \\\\networkserver\\UserHome\\myuser\\desktop\\dbeaver-ce-7.0.0-win32.win32.x86_64 folder. This limitation is caused by a reference to HiveJDBC41.jar library. You can run it from a network folder with UNC path but you need to make \\\\networkserver\\UserHome\\myuser\\desktop\\dbeaver-ce-7.0.0-win32.win32.x86_64 folder as your DBeaver root one and complete steps ## 1-6 in that folder.","tags":"DBeaver","url":"https://techjogging.com/build-dbeaver-installation-package-to-access-hive-with-jaas-configuration-using-kerberos-authentication-in-windows.html","loc":"https://techjogging.com/build-dbeaver-installation-package-to-access-hive-with-jaas-configuration-using-kerberos-authentication-in-windows.html"},{"title":"Connect DBeaver to Cloudera Hive with JAAS using Kerberos Authentication in Windows","text":"DBeaver allows connecting to a wide range of databases including Cloudera Hive. Hive driver is part of DBeaver installation but it uses basic authentication with user name and password. Kerberos authentication is another option to connect to Hive. It can be accomplished by adding a new driver to DBeaver. The DBeaver driver is based on Cloudera JDBC Driver for Hive and JAAS configuration file. Legend Before going forward, let's get agreed with the initial information used in configuration files. You can easily replace it with configurations applicable to your cases. It will be a kind of a template. Windows user name: mywindowsuser. Hive name host: hivehost. Kerberos realm if your organization supports multiple regions: REGION.SAMPLE.COM. Hive port: 10000. Prerequisites Kerberos configuration file: krb5.conf. It can be obtained from your Kerberos administrator or from the /etc/krb5.conf folder on the machine that is hosting the Hive Server 2 instance. One of the files below. It depends on which method is chosen. a) Kerberos credential cache: krb5cc_mywindowsuser. This file contains your Windows kerberized credentials. Using this file will request to renew it. b) Kerberos keytab file: mywindowsuser.keytab. This file stores your Windows kerberized credentials. It doesn't request renewal. Setup The following sample based on DBeaver EE v. 6.3.0 64 bit and Cloudera Hive JDBC driver v. 2.6.5.1007 . Download and unzip Cloudera Hive JDBC driver. Place the unzipped folder to the permanent location. DBeaver reads it from that location every time when the driver is used. It's advisable to read the driver documentation from ./hive_jdbc_2.6.5.1007/docs folder. Registering the Driver Class section tells the class name used in DBeaver driver setup and Using Kerberos section explains each parameter utilized in the driver setup. Append those 4 lines to dbeaver.ini file. The location of the ini file can be different. If you have installed DBeaver, it will be in C:\\Program Files\\DBeaverEE folder. If you use zipped installation, it will be a root folder where it has been unzipped. -Djavax.security.auth.useSubjectCredsOnly = false -Dsun.security.krb5.debug = true -Djava.security.krb5.conf = C:/Users/mywindowsuser/DBeaver/krb5.ini -Djava.security.auth.login.config = C:/Users/mywindowsuser/DBeaver/jaas.conf It's very critical to use correct path separator. It has to be \"/\" forward slash character or you have to use \"\\\\\" double back slash characters. After successful completion of the setup, you need to remove the line. -Dsun.security.krb5.debug = true The final beaver.ini file is -startup plugins/org.eclipse.equinox.launcher_1.5.600.v20191014-2022.jar --launcher.library plugins/org.eclipse.equinox.launcher.win32.win32.x86_64_1.1.1100.v20190907-0426 -vmargs -XX:+IgnoreUnrecognizedVMOptions --add-modules = ALL-SYSTEM -Xms128m -Xmx2048m -Djavax.security.auth.useSubjectCredsOnly = false -Dsun.security.krb5.debug = true -Djava.security.krb5.conf = C:/Users/mywindowsuser/DBeaver/krb5.ini -Djava.security.auth.login.config = C:/Users/mywindowsuser/DBeaver/jaas.conf Move krb5.conf file with new krb5.ini name to C:/Users/mywindowsuser/DBeaver folder. You don't need to do any changes to the file. Create JAAS configuration file Option #1. Kerberos credential cache. Client { com.sun.security.auth.module.Krb5LoginModule required debug = true doNotPrompt = true useKeyTab = true keyTab = \"C:/Users/mywindowsuser/krb5cc_mywindowsuser\" useTicketCache = true renewTGT = true principal = \"mywindowsuser@REGION.SAMPLE.COM\"; }; Option #2. Kerberos keytab file. Client { com.sun.security.auth.module.Krb5LoginModule required debug = true doNotPrompt = true useKeyTab = true keyTab = \"mywindowsuser.keytab\" useTicketCache = false renewTGT = false principal = \"mywindowsuser@REGION.SAMPLE.COM\"; }; After successful completion of the setup, you need to remove the line. debug = true Create Kerberos authentication file Option #1. Kerberos credential cache. See Create Ticket Cache File for Kerberos Authentication in Windows article. Option #2. Kerberos keytab file. See Create keytab File for Kerberos Authentication in Windows article. Create a new driver in DBeaver Option #1. Kerberos credential cache. Driver Name: Hive-Cloudera Class Name: com.cloudera.hive.jdbc41.HS2Driver URL Template: jdbc:hive2://hivehost:10000/{database};AuthMech=1;KrbRealm=REGION.SAMPLE.COM;KrbServiceName=hive;KrbHostFQDN=hivehost.region.sample.com;KrbAuthType=2; Default Port: 10000 Category: Hadoop Add jar file to the driver: ./hive_jdbc_2.6.5.1007/ClouderaHiveJDBC41-2.6.5.1007/HiveJDBC41.jar Option #2. Kerberos keytab file. Driver Name: Hive-Cloudera Class Name: com.cloudera.hive.jdbc41.HS2Driver URL Template: jdbc:hive2://hivehost:10000/{database};AuthMech=1;KrbRealm=REGION.SAMPLE.COM;KrbServiceName=hive;KrbHostFQDN=hivehost.region.sample.com;KrbAuthType=1; Default Port: 10000 Category: Hadoop Add jar file to the driver: ./hive_jdbc_2.6.5.1007/ClouderaHiveJDBC41-2.6.5.1007/HiveJDBC41.jar Create a new connection in DBeaver Utilize the driver created on step #6. The name is Hive-Cloudera . Add default to Database/Schema field. Flag Save password locally . Test your new connection clicking Test Connection button. Troubleshooting Check the debug log in C:\\Users\\mywindowsuser\\AppData\\Roaming\\DBeaverData\\workspace6\\.metadata\\dbeaver-debug.log folder.","tags":"DBeaver","url":"https://techjogging.com/connect-dbeaver-to-cloudera-hive-with-jaas-configuration-using-kerberos-authentication-in-windows.html","loc":"https://techjogging.com/connect-dbeaver-to-cloudera-hive-with-jaas-configuration-using-kerberos-authentication-in-windows.html"},{"title":"Cron Scheduler with Docker Container in CentOS/RHEL 7","text":"Using cron with the official CentOS Docker image requests activating systemd, keeping container running, and opening a Docker container in privileged mode. CentOS Docker Hub image includes the description of the setup which should be done to activate systemd and keep a container going. The missing is information how to install and set up cron. This article provides a summary of steps and a functioning sample of Dockerfile. Some customization is needed to implement your cron project. Dockerfile The Dockerfile can be used as a template to design your file. After line #15, you can add your commands to install any packages. You need to replace line #20 with your time zone. Finally, line #22 shows how to add a scheduled job to crontab file. ROM centos:7 ENV container docker RUN ( cd /lib/systemd/system/sysinit.target.wants/ ; for i in * ; do [ $i == \\ systemd-tmpfiles-setup.service ] || rm -f $i ; done ) ; \\ rm -f /lib/systemd/system/multi-user.target.wants/* ; \\ rm -f /etc/systemd/system/*.wants/* ; \\ rm -f /lib/systemd/system/local-fs.target.wants/* ; \\ rm -f /lib/systemd/system/sockets.target.wants/*udev* ; \\ rm -f /lib/systemd/system/sockets.target.wants/*initctl* ; \\ rm -f /lib/systemd/system/basic.target.wants/* ; \\ rm -f /lib/systemd/system/anaconda.target.wants/* ; VOLUME [ \"/sys/fs/cgroup\" ] RUN yum install -y cronie && yum clean all RUN rm -rf /etc/localtime RUN ln -s /usr/share/zoneinfo/America/Toronto /etc/localtime RUN crontab -l | { cat ; echo \"25 04 * * sun,mon,tue python3 /app/do_maintenance.py\" ; } | crontab - CMD [ \"/usr/sbin/init\" ] Dockerfile logical parts Activating systemd. RUN ( cd /lib/systemd/system/sysinit.target.wants/ ; for i in * ; do [ $i == \\ systemd-tmpfiles-setup.service ] || rm -f $i ; done ) ; \\ rm -f /lib/systemd/system/multi-user.target.wants/* ; \\ rm -f /etc/systemd/system/*.wants/* ; \\ rm -f /lib/systemd/system/local-fs.target.wants/* ; \\ rm -f /lib/systemd/system/sockets.target.wants/*udev* ; \\ rm -f /lib/systemd/system/sockets.target.wants/*initctl* ; \\ rm -f /lib/systemd/system/basic.target.wants/* ; \\ rm -f /lib/systemd/system/anaconda.target.wants/* ; VOLUME [ \"/sys/fs/cgroup\" ] Install cron. RUN yum install -y cronie && yum clean all Set up your time zone. RUN rm -rf /etc/localtime RUN ln -s /usr/share/zoneinfo/America/Toronto /etc/localtime Add a job in crontab file. RUN crontab -l | { cat ; echo \"25 04 * * sun,mon,tue python3 /app/do_maintenance.py\" ; } | crontab - Keep container running. CMD [ \"/usr/sbin/init\" ] Build conatiner The image name is c7-cron and it's designated as local one. docker build --rm -t local/c7-cron . Run container To create and start a container, use the command. docker run --privileged --name = parking –v /sys/fs/cgroup:/sys/fs/cgroup:ro -d local/c7-cron To access the container in a terminal, run the command. docker exec -it parking /bin/bash","tags":"Docker","url":"https://techjogging.com/cron-scheduler-with-docker-container-in-centosrhel-7.html","loc":"https://techjogging.com/cron-scheduler-with-docker-container-in-centosrhel-7.html"},{"title":"Resize Disk in Oracle VM VirtualBox","text":"Virtualization is flexible in terms of using resources. A virtual machine can be built with minimal assigned resources and later, they can be added when it's needed. Oracle VM VirtualBox allows allocation more space to an existing disk. This procedure is common for all virtualized operation systems. It's similar to replacing your old hard drive with new one which is larger size. Run Oracle VM VirtualBox Manager and open settings of a virtual machine. It can be observed that the disk of the virtual machine is run out of space. Open Virtual Media Manager Select disk to add more space After applying the change, open settings of the virtual machine again to validate it Next steps depend on your operation system. You need to run your virtual machine and consume added space. Follow article to set it up for CentOS/RHEL.","tags":"Virtualization","url":"https://techjogging.com/resize-disk-in-oracle-vm-virtualbox.html","loc":"https://techjogging.com/resize-disk-in-oracle-vm-virtualbox.html"},{"title":"Expand Logical Volume in CentOS/RHEL 7","text":"As Linux systems have root file systems on a logical volume, it can be used Logical Volume Management (LVM) to resize the volume. The exercise of logical volume expanding is completed in case of adding an extra disk to a physical system or having a pool of storage in a virtual environment. Run fdisk or gdisk partition tool. gdisk is used if the partition layout is GPT otherwise fdisk has to be used. gdisk will make your system unbootable if you don't have GPT partition. Both tools work identically when a new partiton is created. How to figure out if GPT partition is present in your system? Assiming that sda is device with available space, run gdisk /dev/sda command. This screen shows that your partition is GPT. $ gdisk /dev/sda GPT fdisk (gdisk) version 0.8.10 Partition table scan: MBR: protective BSD: not present APM: not present GPT: present Found valid GPT with protective MBR; using GPT. In case of MBR partition your screen is. $ gdisk /dev/sda GPT fdisk (gdisk) version 0.8.10 Partition table scan: MBR: MBR only BSD: not present APM: not present GPT: not present *************************************************************** Found invalid GPT and valid MBR; converting MBR to GPT format in memory. THIS OPERATION IS POTENTIALLY DESTRUCTIVE! Exit by typing 'q' if you don't want to convert your MBR partitions to GPT format! *************************************************************** Create a new logical volume partition. Enter 8E00 partition code. Command (? for help): n Partition number (6-128, default 6): First sector (31457280-52428766, default = 31457280) or {+-}size{KMGTP}: Last sector (31457280-52428766, default = 52428766) or {+-}size{KMGTP}: Current type is 'Linux filesystem' Hex code or GUID (L to show codes, Enter = 8300): 8E00 Changed type of partition to 'Linux LVM' Apply changes. Command (? for help): w Final checks complete. About to write GPT data. THIS WILL OVERWRITE EXISTING PARTITIONS!! Do you want to proceed? (Y/N): Y OK; writing new GUID partition table (GPT) to /dev/sda. Warning: The kernel is still using the old partition table. The new table will be used at the next reboot. The operation has completed successfully. Notify the operation system about changes in the partition tables. $ partprobe Validate the new created partition. It can be used either fdisk or gdisk partition tool $ fdisk -l /dev/sda WARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion. Disk /dev/sda: 26.8 GB, 26843545600 bytes, 52428800 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: gpt Disk identifier: 71DD2E79-BD1C-4713-9880-22664C87E57B # Start End Size Type Name 1 2048 2099199 1G Linux filesyste Linux filesystem 2 2099200 16777215 7G Linux LVM Linux LVM 3 16777216 20971519 2G Linux LVM Linux LVM 4 20971520 31457279 5G Linux LVM Linux LVM 5 34 2047 1007K BIOS boot BIOS boot partition 6 31457280 52428766 10G Linux LVM Linux LVM Find out what logical groups/volumes are available. $ lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert root centos -wi-ao---- <13.19g swap centos -wi-ao---- 820.00m Our intertest is to add space to the root file system. The logical volume path is centos/root . In case of RHEL, it might be rhel/root . Create a physical volume. $ pvcreate /dev/sda6 WARNING: ext4 signature detected on /dev/sda6 at offset 1080. Wipe it? [y/n]: y Wiping ext4 signature on /dev/sda6. Physical volume \"/dev/sda6\" successfully created. Extend centos volume group. $ vgextend centos /dev/sda6 Volume group \"centos\" successfully extended Figure out exact free space in PE. The field name is Free PE / Size and the value in the sample is 2559 $ vgdisplay --- Volume group --- VG Name centos System ID Format lvm2 Metadata Areas 4 Metadata Sequence No 8 VG Access read/write VG Status resizable MAX LV 0 Cur LV 2 Open LV 2 Max PV 0 Cur PV 4 Act PV 4 VG Size 23.98 GiB PE Size 4.00 MiB Total PE 6140 Alloc PE / Size 3581 / <13.99 GiB Free PE / Size 2559 / <10.00 GiB VG UUID ZPaYGz-7hbZ-2H6y-RS9W-x13x-2K81-pXCsA3 Extend centos/root logical volume $ lvextend -l+2559 centos/root Size of logical volume centos/root changed from <13.19 GiB (3376 extents) to 23.18 GiB (5935 extents). Logical volume centos/root successfully resized. XFS file system may be grown while mounted using the xfs_growfs command. $ xfs_growfs /dev/centos/root meta-data=/dev/mapper/centos-root isize=512 agcount=9, agsize=406016 blks = sectsz=512 attr=2, projid32bit=1 = crc=1 finobt=0 spinodes=0 data = bsize=4096 blocks=3457024, imaxpct=25 = sunit=0 swidth=0 blks naming =version 2 bsize=4096 ascii-ci=0 ftype=1 log =internal bsize=4096 blocks=2560, version=2 = sectsz=512 sunit=0 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 data blocks changed from 3457024 to 6077440","tags":"Linux","url":"https://techjogging.com/expand-logical-volume-in-centosrhel-7.html","loc":"https://techjogging.com/expand-logical-volume-in-centosrhel-7.html"},{"title":"Convert MBR Partition into GPT in CentOS/RHEL 7","text":"Master Boot Record (MBR) partitioned disks are replaced with newer GUID Partition Table (GPT) standard but MBR is still used widely as a default format. GPT layout for the partition tables has a lot of benefits comparing with MBR one. Along with supporting significantly larger size of disks, it introduces faster and more stable booting. GPT requests to support Unified Extensible Firmware Interface (UEFI) boot. Switch to root user. $ sudo su - Run GPT partition tool. Assuming that sda disk is bootable and will be converted into GPT. $ gdisk /dev/sda GPT fdisk (gdisk) version 0.8.10 Partition table scan: MBR: MBR only BSD: not present APM: not present GPT: not present *************************************************************** Found invalid GPT and valid MBR; converting MBR to GPT format in memory. THIS OPERATION IS POTENTIALLY DESTRUCTIVE! Exit by typing 'q' if you don't want to convert your MBR partitions to GPT format! *************************************************************** Make sure that there is enough space before the first partition to support a boot partition. 2048 value for the first sector confirms that GPT can be applied to MBR. Command (? for help): p Disk /dev/sda: 52428800 sectors, 25.0 GiB Logical sector size: 512 bytes Disk identifier (GUID): 71DD2E79-BD1C-4713-9880-22664C87E57B Partition table holds up to 128 entries First usable sector is 34, last usable sector is 52428766 Partitions will be aligned on 2048-sector boundaries Total free space is 20973501 sectors (10.0 GiB) Number Start (sector) End (sector) Size Code Name 1 2048 2099199 1024.0 MiB 8300 Linux filesystem 2 2099200 16777215 7.0 GiB 8E00 Linux LVM 3 16777216 20971519 2.0 GiB 8E00 Linux LVM 4 20971520 31457279 5.0 GiB 8E00 Linux LVM Create a new bootable partition. As the first sector, enter 34 and the last sector is 2047 . Partition code is ef02 . Command (? for help): n Partition number (5-128, default 5): First sector (34-52428766, default = 31457280) or {+-}size{KMGTP}: 34 Last sector (34-2047, default = 2047) or {+-}size{KMGTP}: Current type is 'Linux filesystem' Hex code or GUID (L to show codes, Enter = 8300): ef02 Changed type of partition to 'BIOS boot partition' Save changes. Command (? for help): w Final checks complete. About to write GPT data. THIS WILL OVERWRITE EXISTING PARTITIONS!! Do you want to proceed? (Y/N): Y OK; writing new GUID partition table (GPT) to /dev/sda. Warning: The kernel is still using the old partition table. The new table will be used at the next reboot. The operation has completed successfully. Notify the operation system about changes. It eliminates rebooting of the system. $ partprobe Install GRUB on the new bootable partition. $ grub2-install /dev/sda Installing for i386-pc platform. Installation finished. No error reported. Validate the conversion. $ gdisk /dev/sda GPT fdisk (gdisk) version 0.8.10 Partition table scan: MBR: protective BSD: not present APM: not present GPT: present Found valid GPT with protective MBR; using GPT.","tags":"Linux","url":"https://techjogging.com/convert-mbr-partition-into-gpt-in-centosrhel-7.html","loc":"https://techjogging.com/convert-mbr-partition-into-gpt-in-centosrhel-7.html"},{"title":"Redirect HTTP to HTTPS in Synology DSM Nginx","text":"Synology DiskStation Manager (DSM) doesn't include GUI based functionality to set up a redirect HTTP web traffic to secured HTTPS version of your web site. The default web server in DSM 6 is Nginx and the configuration of the web server should be adjusted. It can be accomplished making manual changes to the Nginx web server moustache template. Prerequisites SSL certificate is added to Synology NAS. SSH service is enabled. Web Station is installed. Web server is Nginx. Environment Document is based on DSM 6.2.2 operation system Original moustache template server { listen 80 default_server{{#reuseport}} reuseport{{/reuseport}}; listen [::]:80 default_server{{#reuseport}} reuseport{{/reuseport}}; gzip on; {{> /usr/syno/share/nginx/WWW_Main}} location ~ &#94;/$ { rewrite / http://$host:{{DSM.port}}/ redirect; } } server { listen 443 default_server ssl{{#reuseport}} reuseport{{/reuseport}}; listen [::]:443 default_server ssl{{#reuseport}} reuseport{{/reuseport}}; {{#DSM.https.compression}} gzip on; {{/DSM.https.compression}} {{> /usr/syno/share/nginx/WWW_Main}} location ~ &#94;/$ { rewrite / https://$host:{{DSM.ssl.port}}/ redirect; } } Setup Use ssh client to access your Synology NAS with a user which has administrative permission. It can be PuTTY tool in Windows or terminal with ssh command in Unix. Switch to root user sudo su - Back up the current moustache template Don't skip it. First of all, in case of any issues with the setup, you can always roll it back. Also, if you decide to add another domain or subdomain, you need your backup copy to do it. cp /usr/syno/share/nginx/WWWService.mustache /usr/syno/share/nginx/WWWService.mustache.bak Open the moustache template for editing vi /usr/syno/share/nginx/WWWService.mustache Replace 4 lines in port 80 section {{> /usr/syno/share/nginx/WWW_Main}} location ~ &#94;/$ { rewrite / http://$host:{{DSM.port}}/ redirect; } with those 2 lines server_name _; return 301 https://$host$request_uri; The final content should be server { listen 80 default_server{{#reuseport}} reuseport{{/reuseport}}; listen [::]:80 default_server{{#reuseport}} reuseport{{/reuseport}}; gzip on; server_name _; return 301 https://$host$request_uri; } server { listen 443 default_server ssl{{#reuseport}} reuseport{{/reuseport}}; listen [::]:443 default_server ssl{{#reuseport}} reuseport{{/reuseport}}; {{#DSM.https.compression}} gzip on; {{/DSM.https.compression}} {{> /usr/syno/share/nginx/WWW_Main}} location ~ &#94;/$ { rewrite / https://$host:{{DSM.ssl.port}}/ redirect; } } Restart the Nginx web server to apply the changes synoservicecfg --restart nginx The last important step is to refresh your browser. When you open your web site with http , it's still showing as http and don't redirect to https . Just click Ctrl-F5 . Add another domain Adding another domain or subdomain with SSL/TLS encryption will request to do Let's Encrypt authorization again. The easiest way is to roll back WWWService.mustache.bak file created on step #3 of the setup. After creating of a SSL certificate, the modified mustache file is returned back.","tags":"Synology DSM","url":"https://techjogging.com/redirect-http-to-https-in-synology-nas-nginx.html","loc":"https://techjogging.com/redirect-http-to-https-in-synology-nas-nginx.html"},{"title":"Extract Number in Microsoft Access/Excel","text":"Microsoft Access and Excel don't include any string functions to extract a number from a string. It should be created a custom function to complete this task. Regular Expressions are a good option to deal with manipulation of text data. Microsoft Access and Excel are lacking of support of regular expressions but they allow to utilize third party libraries. A library of our interest is Microsoft VBScript Regular Expressions 5.5 one. Open your Microsoft Access database or Excel spreadsheet, then go to VBA Editor pressing Alt-F11 combination. Microsoft Access Microsoft Excel Create a new module calling context menu on the root node of the project tree. Microsoft Access Microsoft Excel Copy and paste the function below to the new created module. Function ExtractNumber ( textValue ) Dim re As Object Set re = CreateObject ( \"vbscript.RegExp\" ) re . Pattern = \"[&#94;\\d]\" re . Global = True ExtractNumber = re . Replace ( textValue , \"\" ) End Function This code uses late binding to the library. This method is not preferable but it reduces number of steps to implement the solution. Close VBA Editor. Create a table and a query in Microsoft Access and a column of values in Microsoft Excel to test the function. Microsoft Access Microsoft Excel","tags":"Microsoft Access","url":"https://techjogging.com/extract-number-in-microsoft-accessexcel.html","loc":"https://techjogging.com/extract-number-in-microsoft-accessexcel.html"},{"title":"Set Up Scanner in CentOS 7","text":"As a scanner software can be used Paperwork . It's open source software which is available in both Windows and Unix. CentOS 7 includes Paperwork scanner software as a part of the distribution repository. Make sure that your scanner driver installed in CentOS. This is a sample of Brother printer/scanner driver installation instructions from the official website. Install Scanner Access Now Easy (SANE) application programming interface that provides standardized access to any raster image scanner hardware. sudo yum install sane-backends Enable SANE connection required for scanning. sudo sh -c \"echo 127.0.0.1 >> /etc/sane.d/saned.conf\" Set up SANE service to start automatically and run the service. sudo systemctl enable saned.socket sudo systemctl start saned.socket Open Application Installer from System Tools menu and search for paperwork in search bar. Install Paperwork. After completion it will be available in Office menu. Run Paperwork and open Setting from menu Validate that Device , Default source , and Resolution populated properly as per your scanner.","tags":"Linux","url":"https://techjogging.com/set-up-scanner-in-centos-7.html","loc":"https://techjogging.com/set-up-scanner-in-centos-7.html"},{"title":"Clean Up USB Flash, SSD, Hard Drives","text":"When a drive has been used in Operation System (OS) different from Windows, for example, Linux, it can get unusable in Windows. Disk Management Windows GUI tool can't handle those disks but DiskPart Windows tool can help clean up and re-partition a drive. The tool is similar to fdisk Linux one with comprehensive functionality. 1. Run the tool typing diskpart from Windows Command Prompt or Windows Start Microsoft DiskPart version 10.0.19041.1 Copyright (C) Microsoft Corporation. On computer: MAINCOMPUTER DISKPART> 2. Identify an index of a drive DISKPART > LIST DISK Disk ### Status Size Free Dyn Gpt -------- ------------- ------- ------- --- --- Disk 0 Online 931 GB 1024 KB * Disk 1 Online 953 GB 1024 KB * Disk 2 Online 1863 GB 1863 GB 3. Select a drive Based on size, the drive index is 2. DISKPART > SELECT DISK 2 Disk 2 is now the selected disk. 4. Clean up drive Remove all partitions from the drive. DISKPART > CLEAN DiskPart succeeded in cleaning the disk. if additionally, the drive content has to be removed securely, apply ALL option. It will take time as it will write to every sector of the disk. DISKPART > CLEAN ALL DiskPart succeeded in cleaning the disk. 5. Create a new primary partition DISKPART > CREATE PARTITION PRIMARY DiskPart succeeded in creating the specified partition. 6. Format the drive A list of file systems is FAT, FAT32, NTFS, exFAT. quick option allows to complete it very fast DISKPART > FORMAT fs=exFAT quick 100 percent completed DiskPart successfully formatted the volume. 7. Assign a letter to access the drive in Windows DISKPART > ASSIGN DiskPart successfully assigned the drive letter or mount point. 8. Close DiskPart tool DISKPART > EXIT","tags":"Windows","url":"https://techjogging.com/clean-up-usb-flash-drive.html","loc":"https://techjogging.com/clean-up-usb-flash-drive.html"}]};